```{r echo=FALSE}
knitr::opts_chunk$set(fig.retina = 2.5, fig.align = "center",
                      fig.width = 6, fig.height = 3)
```

# Resampling Methods

>Resampling methods are an indispensable tool in modern statistics. They
involve repeatedly drawing samples from a training set and refitting a model
of interest on each sample in order to obtain additional information about
the fitted model. For example, in order to estimate the variability of a linear
regression fit, we can repeatedly draw different samples from the training
data, fit a linear regression to each new sample, and then examine the
extent to which the resulting fits differ. Such an approach may allow us to
obtain information that would not be available from fitting the model only
once using the original training sample.

>In this chapter, we discuss two of the most commonly
used resampling methods, *cross-validation* and the *bootstrap*.

Cross-validation is most often used to estimate test error associated with a statistical learning method, whereas the boostrap is most commonly used to provide a measure of accuracy for a given parameter/method.

> The process
of evaluating a model’s performance is known as *model assessment*, whereas 
the process of selecting the proper level of flexibility for a model is known as *model selection*.

## Cross Validation

Sometimes we want to estimate the test error rate using the available training data.
A number of approaches can be used for this.
In this section we consider methods which involve *holding out* a subset of the training data from the fitting process, then applying the model to that hold-out set for model assessment.

### The Validation Set Approach {#validation-set}

This simple strategy involves randomly dividing available observations into training and validation sets.
The model is fitted on the training set, and used to make predictions on the validation set.
The corresponding metric from the validation set predictions -- usually MSE in the case of a quantitative response -- provides an estimate of the test error rate.
To illustrate this, load the `Auto` data set and R packages:

```{r message=FALSE}
auto <- ISLR2::Auto

library(tidyverse)
library(tidymodels)
library(broom)
library(gt)
library(patchwork) # for composing plots
# Load my R package and set the ggplot theme
library(dunnr)
extrafont::loadfonts(device = "win", quiet = TRUE)
theme_set(theme_td())
set_geom_fonts()
set_palette()
```

Randomly split the data into 50% training and 50% validation, fit on the training set, and compute the MSE on the validation set.
Since I'll be repeating this 10 times to reproduce the figure, make a couple functions.
First, a function to set a random seed and split the data into training and validation set (called `assessment` set in the `rsample` package):

```{r}
auto_validation_split <- function(seed) {
  set.seed(seed)
  validation_split(auto, prop = 0.5)
}
auto_splits <- auto_validation_split(seed = 10)
auto_splits
```

Similar to the `initial_split` workflow, I can access the training and validation sets as follows:

```{r}
training(auto_splits$splits[[1]])
assessment(auto_splits$splits[[1]])
```

Second, a function to fit 10 models (1 to 10 polynomial degrees of freedom) on the training set, and evaluate on the validation set with `fit_resamples`.
^[In the text, they compute the MSE but here I am computing RMSE then squaring it. This is because `yardstick` has a `rmse()` function but not an `mse()` function. If I wanted to, I could define a custom metric [like this](https://yardstick.tidymodels.org/articles/custom-metrics.html).]

```{r}
auto_rec <- recipe(mpg ~ horsepower, data = auto)
lm_workflow <- workflow() %>% add_model(linear_reg())

evaluate_poly_fits <- function(auto_splits) {
  tibble(poly_df = 1:10) %>%
    mutate(
      lm_rec = map(
        poly_df, ~ auto_rec %>% step_poly(horsepower, degree = .x)
      ),
      lm_fit = map(
        lm_rec,
        ~ lm_workflow %>% add_recipe(.x) %>% fit_resamples(auto_splits)
      ),
      lm_metrics = map(lm_fit, collect_metrics)
    ) %>%
    unnest(lm_metrics) %>%
    filter(.metric == "rmse") %>%
    select(poly_df, rmse = mean)
}
auto_poly_fits_validation <- evaluate_poly_fits(auto_splits)
auto_poly_fits_validation
```

Now reproduce Figure 5.2:

```{r figure5.2}
auto_poly_fits_validation <- bind_rows(
  # 9 additional sets of fits
  map_dfr(
    1:9,
    function(seed) {
      auto_splits <- auto_validation_split(seed)
      evaluate_poly_fits(auto_splits)
    },
    .id = "rep"
  ),
  auto_poly_fits_validation %>% mutate(rep = "10")
)

# Use a 10 color palette from the MetBrewer package
pal <- MetBrewer::met.brewer("Veronese", 10, type = "continuous")

p2 <- auto_poly_fits_validation %>%
  ggplot(aes(x = poly_df, y = rmse^2, fill = rep)) +
  geom_line(aes(color = rep), size = 1) +
  expand_limits(y = c(15, 30)) +
  scale_x_continuous(breaks = seq(2, 10, 2)) +
  scale_color_manual(values = pal) +
  scale_fill_manual(values = pal) +
  labs(x = "Degree of polynomial", y = NULL) +
  theme(legend.position = "none")
p1 <- p2 %+% filter(auto_poly_fits_validation, rep == "1") +
  geom_point(aes(fill = rep), shape = 21,  color = "white", size = 4) +
  labs(y = "MSE")
p1 | p2
```

As is clear from the right-hand panel, this approach is highly variable depending on the testing/validation set split.
Another downside is that, because the training set used to fit the data has fewer observations, it tends to overestimate the test error rate on the entire data set.

### Leave-One-Out Cross Validation {#loocv}

*Leave-one-out cross validation* (LOOCV) attempts to address the shortcomings of the validation set approach.
It still involves splitting the $n$ observations into two parts, but it repeats it $n$ times, with a single observation $(x_i, y_i)$ as the hold-out "set" and the remaining $n-1$ observations as the training set.
The MSE for each iteration is simply $\text{MSE}_i = (y_i - \hat{y}_i)^2$.
Then the LOOCV estimate of the MSE is the average over all observations:

$$
\text{CV}_{(n)} = \frac{1}{n} \sum_{i=1}^n \text{MSE}_i.
$$

The LOOCV approach has a few advantages over the validation set approach:

>
* First, it has far less bias. In LOOCV, we repeatedly fit the statistical learning method using training sets that contain $n − 1$ observations, almost as many as are in the entire data set. This is in contrast to the validation set approach, in which the training set is typically around half the size of the original data set. Consequently, the LOOCV approach tends not to overestimate the test error rate as much as the validation set approach does.
* Second, in contrast to the validation approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always yield the same results: there is no randomness in the training/validation set splits.

Define a new function to split the data by LOOCV:
^[Since there is no random splitting with this approach, I don't need use a function that sets the random seed.]

```{r}
auto_splits <- loo_cv(auto)
auto_splits
```

`r nrow(auto_splits)` splits, which is the number of observations, as expected.
Fit:

```{r error=TRUE}
auto_poly_fits_loo_cv <- evaluate_poly_fits(auto_splits)
```

As the error says, LOOCV is not supported.
Here's an [explanation I found online](https://www.tmwr.org/resampling.html#leave-one-out-cross-validation):

>
Leave-one-out methods are deficient compared to almost any other method. For anything but pathologically small samples, LOO is computationally excessive and it may not have good statistical properties. Although rsample contains a `loo_cv()` function, these objects are not generally integrated into the broader tidymodels frameworks.

Fair enough.
For completeness, I will still get the LOOCV estimates using `boot::cv.glm()`:

```{r}
auto_poly_fits_loo_cv <- tibble(
  poly_df = 1:10,
  mse = map_dbl(poly_df,
    function(poly_df) {
      glm_fit <- glm(mpg ~ poly(horsepower, degree = poly_df), data = auto)
      boot::cv.glm(auto, glm_fit)$delta[1]
    }
  )
)
auto_poly_fits_loo_cv
```

### $k$-fold Cross-Validation {#kfoldcv}

*$k$-fold CV* involves randomly dividing the observations into $k$ groups/folds of approximately equal size.
The first fold is used as the validation/assessment set, and the remaining $k-1$ folds used to fit the model.
This is repeated $k$ times, with each fold being used as the assessment set once.
The $k$-fold CV estimate of the test error is then the average:

$$
\text{CV}_{(k)} = \frac{1}{k} \sum_{i=1}^k \text{MSE}_i.
$$

This should look familiar:

>
It is not hard to see that LOOCV is a special case of $k$-fold CV in which $k$
is set to equal $n$. In practice, one typically performs $k$-fold CV using $k$ = 5
or $k$ = 10. What is the advantage of using $k$ = 5 or $k$ = 10 rather than
$k$ = $n$? The most obvious advantage is computational. LOOCV requires
fitting the statistical learning method $n$ times. This has the potential to be
computationally expensive (except for linear models fit by least squares,
in which case formula (5.2) can be used). But cross-validation is a very
general approach that can be applied to almost any statistical learning
method. Some statistical learning methods have computationally intensive
fitting procedures, and so performing LOOCV may pose computational
problems, especially if $n$ is extremely large. In contrast, performing 10-fold
CV requires fitting the learning procedure only ten times, which may be
much more feasible. As we see in Section 5.1.4, there also can be other
non-computational advantages to performing 5-fold or 10-fold CV, which
involve the bias-variance trade-off.

Fit the polynomial models with 10-fold CV:

```{r}
auto_10_fold_cv <- function(seed) {
  set.seed(seed)
  vfold_cv(auto, v = 10)
}
auto_splits <- auto_10_fold_cv(seed = 10)
auto_splits
```

```{r}
auto_poly_fits_10_fold_cv <- evaluate_poly_fits(auto_splits)
auto_poly_fits_10_fold_cv
```

Now repeat this another 9 times.
Thought it won't take *too* long to run, I'll make use of `parallel` to speed it up a bit:

```{r}
n_cores <- parallel::detectCores(logical = FALSE)
library(doParallel)
cl <- makePSOCKcluster(n_cores - 1)
registerDoParallel(cl)

tic()
auto_poly_fits_10_fold_cv <- bind_rows(
  # 9 additional sets of fits
  map_dfr(
    1:9,
    function(seed) {
      auto_splits <- auto_10_fold_cv(seed)
      evaluate_poly_fits(auto_splits)
    },
    .id = "rep"
  ),
  auto_poly_fits_10_fold_cv %>% mutate(rep = "10")
)
toc()
```

Now reproduce Figure 5.4:

```{r figure5.4}
p2 <- auto_poly_fits_10_fold_cv %>%
  mutate(mse = rmse^2) %>%
  ggplot(aes(x = poly_df, y = mse, fill = rep)) +
  geom_line(aes(color = rep), size = 1) +
  expand_limits(y = c(15, 30)) +
  scale_x_continuous(breaks = seq(2, 10, 2)) +
  scale_color_manual(values = pal) +
  scale_fill_manual(values = pal) +
  labs(x = "Degree of polynomial", y = NULL) +
  theme(legend.position = "none")
p1 <- p2 %+% mutate(auto_poly_fits_loo_cv, rep = "1") +
  geom_point(aes(fill = rep), shape = 21,  color = "white", size = 4) +
  labs(y = "MSE")
p1 | p2
```

The $k$-fold CV approach (right panel) still has some variability due to random splitting, but much less than the validation set approach.

As a reminder, with cross-validation we are trying to approximate the true test MSE, which we cannot know for certain unless the data are simulated (like in Figure 5.6).
The true error value itself is important if we want to know how a model will perform on independent data.
However, if all we care about is parameter(s) that give the minimum error (like the degree of polynomial in these examples), then the CV estimate will usually come close to the true answer.

### Bias-Variance Trade-Off for $k$-Fold Cross-Validation

In addition to being computationally more efficient than LOOCV, $k$-fold CV with $k < n$ also often gives more accurate estimates of the test error rate due to the bias-variance tradeoff.

* $k$-fold CV has moderate bias in comparison to LOOCV, which is approximately unbiased.
* $k$-fold CV (average over $k$ fitted models) tends to have lower variance than LOOCV (average over $n$ fitted models, highly correlated with each other).

>To summarize, there is a bias-variance trade-off associated with the
choice of $k$ in $k$-fold cross-validation. Typically, given these considerations,
one performs $k$-fold cross-validation using $k$ = 5 or $k$ = 10, as these values
have been shown empirically to yield test error rate estimates that suffer
neither from excessively high bias nor from very high variance.

### Cross-Validation on Classification Problems

Rather than quantitative $Y$, cross-validation works just as well with qualitative $Y$.
Instead of MSE, we use the number of misclassified observations, $\text{Err}_i = I(y_i \neq \hat{y}_i)$.
The LOOCV error rate:

$$
\text{CV}_{(n)} = \frac{1}{n} \sum_{i=1}^n \text{Err}_i.
$$

The $k$-fold CV and validation set error rates are defined analogously.

## The Bootstrap {#bootstrap}

>
The *bootstrap* is a widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method. As a simple example, the bootstrap can be used to estimate the standard errors of the coefficients from a linear regression fit. In the specific case of linear regression, this is not particularly useful, since we saw in Chapter 3 that standard statistical software such as R outputs such standard errors automatically. However, the power of the bootstrap lies in the fact that it can be easily applied to a wide range of statistical learning methods, including some for which a measure of variability is otherwise difficult to obtain and is not automatically output by
statistical software.

The toy example in this section is about investment in two assets $X$ and $Y$.
We wish to choose a fraction $\alpha$ of investment into $X$ which minimizes the total variance (risk) of the investment.
In can be shown that the optimal value is given by:

$$
\alpha = \frac{\sigma_Y^2 - \sigma_{XY}}{\sigma_X^2 + \sigma_Y^2 - 2\sigma_{XY}},
$$

where $\sigma_X^2 = \text{Var}(X)$, $\sigma_Y^2 = \text{Var}(Y)$, and $\sigma_{XY} = \text{Cov}(X, Y)$.
In reality, we don't know these variances and covariance, so we estimate them (e.g. $\hat{\sigma}_X^2$) using observations of $X$ and $Y$ to get an estimated $\hat{\alpha}$.

To illustrate, we simulate 100 pairs of $X$ and $Y$, and compute the estimated $\alpha$.
The simulation parameters are $\sigma_X^2 = 1$, $\sigma_Y^2 = 1.25$, and $\sigma_{XY} = 0.5$.
Presumably, these are bivariate normally distributed, which I can simulate with the `mvtnorm` package:

```{r}
library(mvtnorm)
sigma_x <- 1
sigma_y <- sqrt(1.25)
sigma_xy <- 0.5
# The variance-covariance matrix
sigma <- matrix(c(sigma_x^2, sigma_xy, sigma_xy, sigma_y^2), nrow = 2)
# Generate 5 observations
rmvnorm(n = 5, mean = c(0, 0), sigma = sigma)
```

Write a function to compute $\alpha$ from the simulated $X$ and $Y$:

```{r}
sim_alpha <- function(xy = NULL) {
  if (is.null(xy)) {
    xy <- rmvnorm(n = 100, mean = c(0, 0), sigma = sigma)
  }
  x <- xy[,1]; y <- xy[,2]
  (var(y) -  cov(x, y)) / (var(x) + var(y) - 2 * cov(x, y))
}
sim_alpha()
```

Run four simulations and plot $X$ vs $Y$ for Figure 5.9

```{r figure5.9}
d4 <- tibble(sim = 1:4) %>%
  rowwise() %>%
  mutate(xy = list(rmvnorm(n = 100, mean = c(0, 0), sigma = sigma))) %>%
  ungroup() %>%
  mutate(alpha = map_dbl(xy, sim_alpha) %>% round(3),
         x = map(xy, ~ .x[,1]), y = map(xy, ~ .x[,2])) %>%
  unnest(cols = c(x, y))
d4 %>%
  ggplot(aes(x, y)) +
  geom_point(color = td_colors$nice$emerald, size = 2) +
  facet_wrap(~alpha, labeller = "label_both") +
  add_facet_borders()
```

For Figure 5.10, I first simulate 1000 data sets from the true population and estimate $\alpha$ for each:

```{r}
d <- bind_rows(
  d4 %>% distinct(sim, alpha),
  tibble(sim = 5:1000) %>%
    rowwise() %>%
    mutate(alpha = sim_alpha()) %>%
    ungroup()
)
d
```

And then, using just the first simulated data set, use the `rsample::boostraps()` function to generate 1000 bootstrap resamples:

```{r}
d1_boot <- d4 %>% filter(sim == 1) %>% select(x, y) %>%
  rsample::bootstraps(times = 1000)
d1_boot
```

Compute $\hat{\alpha}$ from each split:

```{r}
d1_boot_alpha <- map_dbl(
  d1_boot$splits,
  function(split) {
    xy <- as.data.frame(split)
    sim_alpha(as.matrix(xy))
  }
)
d <- bind_rows(
  d %>% mutate(population = "true"),
  tibble(alpha = d1_boot_alpha, population = "bootstrap resamples")
) %>%
  mutate(population = fct_rev(population))

true_alpha <- (sigma_y^2 - sigma_xy) / (sigma_x^2 + sigma_y^2 - 2 * sigma_xy)
p1 <- d %>%
  ggplot(aes(x = alpha, fill = population)) +
  geom_histogram(binwidth = 0.05, show.legend = FALSE, color = "black") +
  geom_vline(xintercept = true_alpha,
             color = td_colors$nice$light_coral, size = 1.5) +
  facet_wrap(~ population, nrow = 1) +
  scale_fill_manual(values = c(td_colors$nice$soft_orange,
                               td_colors$nice$strong_blue))
p2 <- d %>%
  ggplot(aes(y = alpha, x = population)) +
  geom_boxplot(aes(fill = population), show.legend = FALSE) +
  geom_hline(yintercept = true_alpha,
             color = td_colors$nice$light_coral, size = 1.5) +
  scale_fill_manual(values = c(td_colors$nice$soft_orange,
                               td_colors$nice$strong_blue))
p1 | p2
```

>
Note that the histogram looks very similar to the left-hand panel, which
displays the idealized histogram of the estimates of α obtained by generating 1,000 simulated data sets from the true population. In particular the
bootstrap estimate $\text{SE}(\hat{\alpha})$ from (5.8) is 0.087, very close to the estimate
of 0.083 obtained using 1,000 simulated data sets. The right-hand panel
displays the information in the center and left panels in a different way, via
boxplots of the estimates for $\alpha$ obtained by generating 1,000 simulated data
sets from the true population and using the bootstrap approach. Again, the
boxplots have similar spreads, indicating that the bootstrap approach can
be used to effectively estimate the variability associated with $\hat{\alpha}$.

## Lab: Cross-Validation and the Bootstrap

### The Validation Set Approach

Here are the MSE values as computed in \@ref(#validation-set):

```{r}
auto_poly_fits_validation %>%
  filter(poly_df <= 3, rep == 1) %>%
  mutate(mse = rmse^2)
```

### Leave-One-Out Cross-Validation

Because LOOCV is deterministic (not random), the process in \@ref(#loocv) produces the same MSE values:

```{r}
auto_poly_fits_loo_cv
```

### $k$-fold Cross-Validation

The MSE values as computed in \@ref{#kfoldcv}:

```{r}
auto_poly_fits_10_fold_cv %>%
  filter(rep == 1) %>%
  mutate(mse = rmse^2)
```

### The Bootstrap

#### Estimating the Accuracy of a Statistic of Interest

```{r}
portfolio <- ISLR2::Portfolio
glimpse(portfolio)
```

The `tidymodels` approach to bootstrap estimates, as in \@ref{#boostrap}:

```{r}
portfolio_boot <- rsample::bootstraps(portfolio, times = 1000)
portfolio_boot_alpha <- map_dbl(
  portfolio_boot$splits,
  function(split) {
    xy <- as.data.frame(split)
    (var(xy$Y) - cov(xy$X, xy$Y)) /
      (var(xy$X) + var(xy$Y) - 2 * cov(xy$X, xy$Y))
  }
)
mean(portfolio_boot_alpha); sd(portfolio_boot_alpha)
```

```{r}
xy <- portfolio_boot$splits[[1]]
xy <- as.data.frame(xy)
```

#### Estimating the Accuracy of a Regression Model

>
The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of the estimates for $\beta_0$ and $\beta_1$, the intercept and slope terms for the linear regression model that uses horsepower to predict mpg in the `Auto` data set. We will compare the estimates obtained using the bootstrap to those obtained using the formulas for $\text{SE}(\hat{\beta}_0)$ and $\text{SE}(\hat{\beta}_1)$ described in Section 3.1.2.

```{r}
auto_boot <- rsample::bootstraps(auto, times = 10)
auto_boot
```

https://www.tidymodels.org/learn/models/coefficients/

```{r}
get_lm_coefs <- function(x) {
  x %>% extract_fit_engine() %>% tidy()
}
tidy_ctrl <- control_grid(extract = get_lm_coefs)

auto_boot_fit <- workflow() %>%
  add_model(linear_reg()) %>%
  add_recipe(recipe(mpg ~ horsepower, data = auto)) %>%
  fit_resamples(auto_boot, control = tidy_ctrl)
auto_boot_fit %>%
  unnest(.extracts) %>%
  unnest(.extracts) %>%
  group_by(term) %>%
  summarise(
    mean_estimate = mean(estimate), sd_estimate = sd(estimate),
    #mean_se = mean(std.error), sd_se = sd(std.error),
    .groups = "drop"
  ) %>%
  gt() %>%
  fmt_number(columns = -term, decimals = 3)
```


## Exercises
