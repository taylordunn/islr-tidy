```{r echo=FALSE}
knitr::opts_chunk$set(fig.retina = 2.5, fig.align = "center",
                      fig.width = 6, fig.height = 3,
                      cache.path = "cache/")
```

# Tree-Based Methods

Load the usual packages:

```{r message=FALSE}
library(tidyverse)
library(tidymodels)
library(broom)
library(gt)
library(patchwork)
library(tictoc)

# Load my R package and set the ggplot theme
library(dunnr)
extrafont::loadfonts(device = "win", quiet = TRUE)
theme_set(theme_td_minimal())
set_geom_fonts()
set_palette()
```

Tree-based methods involve *stratifying* or *segmenting* the predictor space into a number of simple regions.
Predictions are typically the mean or mode of the response value for training observations in a region.
Since the set of splitting rules can be summarized in a tree, these types of approaches are known as *decision tree* methods.

>Tree-based methods are simple and useful for interpretation. However,
they typically are not competitive with the best supervised learning aproaches, such as those seen in Chapters 6 and 7, in terms of prediction
accuracy. Hence in this chapter we also introduce bagging, random forests,
boosting, and Bayesian additive regression trees. Each of these approaches
involves producing multiple trees which are then combined to yield a single
consensus prediction. We will see that combining a large number of trees
can often result in dramatic improvements in prediction accuracy, at the
expense of some loss in interpretation.

## The Basics of Decision Trees

### Regression Trees

#### Predicting Baseline Players' Salaries Using Regression Trees

To motivate *regression trees*, we use an example of predicting a baseball player's `salary` based on `years` (number of years played in the major leagues), and `hits` (number of hits made in the previous year).

```{r}
hitters <- ISLR2::Hitters %>% janitor::clean_names()

# As per the text, we remove missing `salary` values and log-transform it
hitters <- hitters %>%
  filter(!is.na(salary)) %>%
  mutate(salary = log(salary))

glimpse(hitters)
```

I'll start with the `trees` library for this example:

```{r}
library(tree)

hitters_tree <- tree(salary ~ years + hits, data = hitters,
                     # In order to limit the tree to just two partitions,
                     #  need to set the `control` option
                     control = tree.control(nrow(hitters), minsize = 100))
```

Use the built-in `plot()` to visualize the tree in Figure 8.1:

```{r figure8.1}
plot(hitters_tree)
text(hitters_tree)
```

To visualize the regions, there is no `broom::tidy()` method for `tree` objects, but I can get the cuts from the `frame$splits` object:

```{r}
hitters_tree$frame$splits
```

```{r}
splits <- hitters_tree$frame$splits %>%
  as_tibble() %>%
  filter(cutleft != "") %>%
  mutate(val = readr::parse_number(cutleft)) %>%
  pull(val)
splits
```

```{r figure8.2}
hitters %>%
  ggplot(aes(x = years, y = hits)) +
  geom_point(color = td_colors$nice$soft_orange) +
  geom_vline(xintercept = splits[1], size = 1, color = "forestgreen") +
  geom_segment(aes(x = splits[1], xend = 25, y = splits[2], yend = splits[2]),
               size = 1, color = "forestgreen") +
  annotate("text", x = 10, y = 50, label = "R[2]", size = 6, parse = TRUE) +
  annotate("text", x = 10, y = 200, label = "R[3]", size = 6, parse = TRUE) +
  annotate("text", x = 2, y = 118, label = "R[1]", size = 6, parse = TRUE) +
  coord_cartesian(xlim = c(0, 25), ylim = c(0, 240)) +
  scale_x_continuous(breaks = c(1, 4.5, 24)) +
  scale_y_continuous(breaks = c(1, 117.5, 238))
```

>
Overall, the tree stratifies
or segments the players into three regions of predictor space: players who
have played for four or fewer years, players who have played for five or more
years and who made fewer than 118 hits last year, and players who have
played for five or more years and who made at least 118 hits last year.

The regions $R_1$, $R_2$, and $R_3$ are known as *terminal nodes* or *leaves* of the tree.
The splits along the way are referred to as *internal nodes* -- the connections between nodes are called *branches*.

A key advantage of a simple decision tree like this is its ease of interpretation:

>
We might interpret the regression tree displayed in Figure 8.1 as follows:
`Years` is the most important factor in determining `Salary`, and players with
less experience earn lower salaries than more experienced players. Given
that a player is less experienced, the number of hits that he made in the
previous year seems to play little role in his salary. But among players who
have been in the major leagues for five or more years, the number of hits
made in the previous year does affect salary, and players who made more
hits last year tend to have higher salaries.


#### Prediction via Stratification of the Feature Space {-}

Roughly speaking, there are two steps in building a regression tree:

1. Divide the predictor space into $J$ distinct non-overlapping regions $R_1, \dots, R_J$.
2. For every observation that falls into the region $R_j$, we make the same prediction: the mean of the respone values of the training observations.

In theory, the regions could be any shape but we choose *boxes* for simplicity.
The goal of step 1 is the finx the boxes that minimize the RSS

$$
\sum_{j=1}^J \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2.
$$

>
Unfortunately, it is computationally infeasible to consider every
possible partition of the feature space into $J$ boxes. For this reason, we take
a *top-down, greedy* approach that is known as *recursive binary splitting*. The
approach is *top-down* because it begins at the top of the tree (at which point
all observations belong to a single region) and then successively splits the
predictor space; each split is indicated via two new branches further down
on the tree. It is *greedy* because at each step of the tree-building process,
the best split is made at that particular step, rather than looking ahead
and picking a split that will lead to a better tree in some future step.

The first split is defined as a pair of half planes:

$$
R_1 (j,s) = \{X | X_j < s \} \text{ and } R_2 (j,s) = \{ X | X_j \geq s \},
$$

and we seek the values $j$ and $s$ that minimize

$$
\sum_{i: x_i \in R_1 (j,s)} (y_i - \hat{y}_{R_1})^2 +
\sum_{i: x_i \in R_2 (j,s)} (y_i - \hat{y}_{R_2})^2.
$$

Finding the values $j$ and $s$ to minimize the above RSS can be done quite quickly, especially when the number of features $p$ is not too large.

The process is repeated -- looking for the best predictor and best cutpoint in order to split the data further and minimize the RSS within the regions -- except instead of splitting the entire predictor space, we split one of the previously identified regions.
This results in three regions, which we again look to split.
This continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations.

#### Tree Pruning {-}

The above process is likely to overfit the data, leading to poor test set performance.
This is because the resulting tree might be too complex.
A smaller tree with fewer splits/regions might lead to lower variance and better interpretation at the cost of higher bias.
One way to control this is to only choose splits that exceed some high threshold of RSS reduction.
This way is short-sighted however, because a seemingly worthless split early on might be followed by a great split.

>
Therefore, a better strategy is to grow a very large tree $T_0$, and then
*prune* it back in order to obtain a *subtree*. How do we determine the best
way to prune the tree? Intuitively, our goal is to select a subtree that 
leads to the lowest test error rate. Given a subtree, we can estimate its
test error using cross-validation or the validation set approach. However,
estimating the cross-validation error for every possible subtree would be too
cumbersome, since there is an extremely large number of possible subtrees.
Instead, we need a way to select a small set of subtrees for consideration.
>
*Cost complexity pruning* — also known as *weakest link pruning* — gives us cost
a way to do just this. Rather than considering every possible subtree, we
consider a sequence of trees indexed by a nonnegative tuning parameter $\alpha$.
For each value of α there corresponds a subtree $T \subset T0$ such that

$$
\sum^{|T|}_{m=1} \sum_{x_i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha |T|
$$

>
is as small as possible. Here $|T|$ indicates the number of terminal nodes of the tree $T$, $R_m$ is the rectangle (i.e. the subset of predictor space)
corresponding to the $m$th terminal node, and $\hat{y}_{R_m}$
is the predicted response associated with $R_m$ -- that is, the mean of the
training observations in $R_m$. The tuning parameter $\alpha$ controls a trade-off between the subtree's complexity and its fit to the training data.
When $\alpha = 0$, then the subtree $T$ will simply equal $T_0$, because then (8.4) just measures the training error.
However, as $\alpha$ increases, there is a price to pay for having a tree with many terminal nodes, and so the quantity (8.4)
will tend to be minimized for a smaller subtree. Equation 8.4 is reminiscent of the lasso (6.7) from Chapter 6,
in which a similar formulation was used in order to control the complexity of a linear model.

As $\alpha$ is increased from zero, branches get prunced in a nested and predictable way, so obtaining the full sequence of subtrees as a function of $\alpha$ is easy.
The value of $\alpha$ can then be selected using cross-validation or a validation set, which we then apply to the full data set to obtain the subtree.

The example with the `hitters` data first involves splitting the data into half training and half testing:

```{r}
set.seed(-203)
hitters_split <- initial_split(hitters,
                               # Bumped up the prop to get 132 training observations
                               prop = 0.505)

hitters_train <- training(hitters_split)
hitters_test <- testing(hitters_split)

hitters_resamples <- vfold_cv(hitters_train, v = 6)
```

Then fitting a decision tree with nine of the features, which aren't specified so instead I'll fit using the six features in Figure 8.4

```{r figure8.4, fig.height=7, fig.width=7}
hitters_train_tree <- tree(
  salary ~ years + hits + rbi + put_outs + walks + runs,
  data = hitters_train,
)
plot(hitters_train_tree)
text(hitters_train_tree, digits = 3)
```

This isn't exactly the same tree as that in the text, as expected with a different random splitting and feature selection.
I'll do the same with each CV split:

```{r}
hitters_resamples_tree <-
  # Compile all of the analysis data sets from the six splits
  map_dfr(hitters_resamples$splits, analysis, .id = "split") %>%
  # For each split...
  group_by(split) %>%
  nest() %>%
  mutate(
    # ... fit a tree to the analysis set, then...
    tree_mod = map(
      data,
      ~ tree(
        salary ~ years + hits + rbi + put_outs + walks + runs,
        data = .x,
      )
    )
  )
```

Next, we prune the large tree above from `r sum(hitters_tree$frame$var == "<leaf>")` terminal nodes down to 1.
For this, I'll vary the `best` parameter in the `prune.tree()` function:

```{r}
hitters_tree_pruned <- 
  tibble(n_terminal = 1:10) %>%
  mutate(
    train_tree_pruned = map(n_terminal,
                            ~ prune.tree(hitters_train_tree, best = .x)),
  )
hitters_tree_pruned
```

Note that, for `n_terminal` = 1, the object is `singlenode`, not `tree`.
This makes sense -- a single node can't really be called a tree -- but unfortunately it means that I can't use the `predict()` function to calculate MSE later on.
Mathematically, a single node is just a prediction of the mean of the training set, so I will replace `n_terminal` = 1 with a `lm` model with just an intercept:

```{r}
hitters_tree_pruned <- hitters_tree_pruned %>%
  mutate(
    train_tree_pruned = ifelse(
      n_terminal == 1,
      list(lm(salary ~ 1, data = hitters_train)),
      train_tree_pruned
    )
  )
```

Do the same for each CV split:

```{r}
hitters_resamples_tree_pruned <- hitters_resamples_tree %>%
  crossing(n_terminal = 1:10) %>%
  mutate(
    tree_pruned = map2(tree_mod, n_terminal,
                       ~ prune.tree(.x, best = .y)),
    # As above, replace the single node trees with lm
    tree_pruned = ifelse(
      n_terminal == 1,
      map(data, ~ lm(salary ~ 1, data = .x)),
      tree_pruned
    )
  )
```

Note the warnings.
This tells me that some of the models fit to the CV splits had 10 or fewer terminal nodes already, and so no pruning was performed.

Finally, I'll compute the MSE for the different data sets.
The training and testing sets:

```{r}
# Simple helper function to compute mean squared error
calc_mse <- function(mod, data) {
  mean((predict(mod, newdata = data) - data$salary)^2)
}

hitters_tree_pruned_mse <- hitters_tree_pruned %>%
  mutate(
    train_mse = map_dbl(
      train_tree_pruned,
      ~ calc_mse(.x, hitters_train)
    ),
    test_mse = map_dbl(
      train_tree_pruned,
      ~ calc_mse(.x, hitters_test)
    )
  )
hitters_tree_pruned_mse
```

And the CV splits:

```{r}
hitters_resamples_tree_pruned_mse <- hitters_resamples_tree_pruned %>%
  select(split, n_terminal, tree_pruned) %>%
  left_join(
    map_dfr(hitters_resamples$splits, assessment, .id = "split") %>%
      group_by(split) %>%
      nest() %>%
      rename(assessment_data = data),
    by = "split"
  ) %>%
  mutate(
    cv_mse = map2_dbl(
      tree_pruned, assessment_data,
      ~ calc_mse(.x, .y)
    )
  ) %>%
  group_by(n_terminal) %>%
  summarise(cv_mse = mean(cv_mse), .groups = "drop")
```

Finally, put it all together as in Figure 8.5 (without standard error bars):

```{r figure8.5}
hitters_tree_pruned_mse %>%
  select(-train_tree_pruned) %>%
  left_join(hitters_resamples_tree_pruned_mse, by = "n_terminal") %>%
  pivot_longer(cols = c(train_mse, test_mse, cv_mse), names_to = "data_set") %>%
  mutate(
    data_set = factor(data_set,
                      levels = c("train_mse", "test_mse", "cv_mse"),
                      labels = c("Training", "Cross-validation", "Test"))
  ) %>%
  ggplot(aes(x = n_terminal, y = value, color = data_set)) +
  geom_point(size = 3) +
  geom_line(size = 1) +
  scale_y_continuous("Mean squared error", breaks = seq(0, 1.0, 0.2)) +
  expand_limits(y = c(0, 1.0)) +
  scale_x_continuous("Tree size", breaks = seq(2, 10, 2)) +
  scale_color_manual(NULL, values = c("black", "darkgreen", "darkorange")) +
  theme(legend.position = c(0.7, 0.8))
```

As in the text, the minimum test and CV MSE correspond to the decision trees with three terminal nodes.

### Classification Trees

A classification tree is used for a qualitative response.
An observation is predicted to belong to the *most commonly occurring class* of training observations in its region.
Often, we are not only interested in the predicted class for the terminal nodes, but also the *class proportions* among the training observations.

Just as in the regression setting, we use recursive binary splitting to grow a classification tree.
Instead of RSS, the natural alternative is the *classification error rate*.
For a given region, this is simply the fraction of training observations in that region that do not belong to the most common class:

$$
E = 1 - \underset{k}{\text{max}} (\hat{p}_{mk})
$$

where $\hat{p}_{mk}$ represents the proportion of training observations in the $m$th region that are from the $k$th class.

It turns out the classification error is not sufficiently sensitive for tree-growing.
In practice, two other measures are preferable.

The *Gini index* is defined as:

$$
G = \sum_{k=1}^K \hat{p}_{mk} (1 - \hat{p}_{mk}).
$$

It measures total variance across all $K$ classes 

### delete

For the next example, I'll now switch to the `tidymodels` framework, with `decision_tree()`, the `rpart` engine, and the tunable `cost_complexity` parameter:

```{r}
decision_tree_spec <- decision_tree(cost_complexity = tune()) %>%
  set_mode("regression") %>%
  set_engine("rpart")

hitters_workflow <- workflow() %>%
  add_recipe(hitters_recipe) %>%
  add_model(decision_tree_spec)

cost_complexity_grid <-
  grid_regular(cost_complexity(range = c(-2, 0)), levels = 10)

get_rpart_mod <- function(x) {
  extract_fit_engine(x)
}

hitters_tune <- tune_grid(
  hitters_workflow, resamples = hitters_resamples,
  grid = cost_complexity_grid, metrics = metric_set(rmse),
  control = control_grid(extract = get_rpart_mod)
)
```

The number of terminal nodes can be retrieved from the `rpart` model objects like this:

```{r}
rpart_mod <- hitters_tune$.extracts[[1]]$.extracts[[1]]
sum(rpart_mod$frame$var == "<leaf>")
```

This specific model has 5 terminal nodes.
Apply this to each model in the tuning results:

```{r}
hitters_tune %>%
  unnest(.extracts) %>%
  #select(id, cost_complexity, .extracts, .config) %>%
  mutate(n_terminal = map_int(.extracts,
                          ~ sum(.x$frame$var == "<leaf>"))) %>%
  ggplot(aes(x = cost_complexity, y = n_terminal, color = id)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(breaks = 1:10) +
  scale_x_log10()
```

## Reproducibility {-}

```{r}
Sys.time()
```

```{r}
if ("git2r" %in% installed.packages()) {
  if (git2r::in_repository()) {
    git2r::repository()
  }
}
```

```{r}
sessioninfo::session_info()
```
