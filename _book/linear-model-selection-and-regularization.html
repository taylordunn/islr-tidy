<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Linear Model Selection and Regularization | An Introduction to Statistical Learning with the tidyverse</title>
  <meta name="description" content="Working through ISLR with the tidyverse and tidymodels" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Linear Model Selection and Regularization | An Introduction to Statistical Learning with the tidyverse" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Working through ISLR with the tidyverse and tidymodels" />
  <meta name="github-repo" content="taylordunn/islr-tidy" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Linear Model Selection and Regularization | An Introduction to Statistical Learning with the tidyverse" />
  
  <meta name="twitter:description" content="Working through ISLR with the tidyverse and tidymodels" />
  

<meta name="author" content="Taylor Dunn" />


<meta name="date" content="2022-04-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="resampling-methods.html"/>

<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Who, what, and why?</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#an-overview-of-statistical-learning"><i class="fa fa-check"></i>An Overview of Statistical Learning</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#wage-data"><i class="fa fa-check"></i>Wage Data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#a-brief-history-of-statistical-learning"><i class="fa fa-check"></i>A Brief History of Statistical Learning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#this-book"><i class="fa fa-check"></i>This Book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-learning.html"><a href="statistical-learning.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#what-is-statistical-learning"><i class="fa fa-check"></i><b>2.1</b> What Is Statistical Learning?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-learning.html"><a href="statistical-learning.html#why-estimate-f"><i class="fa fa-check"></i><b>2.1.1</b> Why Estimate <span class="math inline">\(f\)</span>?</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-learning.html"><a href="statistical-learning.html#how-do-we-estimate-f"><i class="fa fa-check"></i><b>2.1.2</b> How Do We Estimate f?</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-trade-off-between-prediction-accuracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.1.3</b> The Trade-Off Between Prediction Accuracy and Model Interpretability</a></li>
<li class="chapter" data-level="2.1.4" data-path="statistical-learning.html"><a href="statistical-learning.html#supervised-versus-unsupervised-learning"><i class="fa fa-check"></i><b>2.1.4</b> Supervised Versus Unsupervised Learning</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.2</b> Assessing Model Accuracy</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.2.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>2.2.2</b> The Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="2.2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-classification-setting"><i class="fa fa-check"></i><b>2.2.3</b> The Classification Setting</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#lab-introduction-to-r"><i class="fa fa-check"></i><b>2.3</b> Lab: Introduction to R</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>3.1.1</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="3.1.2" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimates"><i class="fa fa-check"></i><b>3.1.2</b> Assessing the Accuracy of the Coefficient Estimates</a></li>
<li class="chapter" data-level="3.1.3" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-model"><i class="fa fa-check"></i><b>3.1.3</b> Assessing the Accuracy of the Model</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-regression-coefficients"><i class="fa fa-check"></i><b>3.2.1</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regression.html"><a href="linear-regression.html#some-important-questions"><i class="fa fa-check"></i><b>3.2.2</b> Some Important Questions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-regression.html"><a href="linear-regression.html#other-considerations-in-the-regression-model"><i class="fa fa-check"></i><b>3.3</b> Other Considerations in the Regression Model</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="linear-regression.html"><a href="linear-regression.html#qualitative-predictors"><i class="fa fa-check"></i><b>3.3.1</b> Qualitative Predictors</a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-regression.html"><a href="linear-regression.html#extensions-of-the-linear-model"><i class="fa fa-check"></i><b>3.3.2</b> Extensions of the Linear Model</a></li>
<li class="chapter" data-level="3.3.3" data-path="linear-regression.html"><a href="linear-regression.html#potential-problems"><i class="fa fa-check"></i><b>3.3.3</b> Potential Problems</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="linear-regression.html"><a href="linear-regression.html#the-marketing-plan"><i class="fa fa-check"></i><b>3.4</b> The Marketing Plan</a></li>
<li class="chapter" data-level="3.5" data-path="linear-regression.html"><a href="linear-regression.html#comparison-of-linear-regression-with-k-nearest-neighbors"><i class="fa fa-check"></i><b>3.5</b> Comparison of Linear Regression with <span class="math inline">\(K\)</span>-Nearest Neighbors</a></li>
<li class="chapter" data-level="3.6" data-path="linear-regression.html"><a href="linear-regression.html#lab-linear-regression"><i class="fa fa-check"></i><b>3.6</b> Lab: Linear Regression</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="linear-regression.html"><a href="linear-regression.html#libraries"><i class="fa fa-check"></i><b>3.6.1</b> Libraries</a></li>
<li class="chapter" data-level="3.6.2" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>3.6.2</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="3.6.3" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression-1"><i class="fa fa-check"></i><b>3.6.3</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="3.6.4" data-path="linear-regression.html"><a href="linear-regression.html#interaction-terms"><i class="fa fa-check"></i><b>3.6.4</b> Interaction Terms</a></li>
<li class="chapter" data-level="3.6.5" data-path="linear-regression.html"><a href="linear-regression.html#non-linear-transformations-of-the-predictors"><i class="fa fa-check"></i><b>3.6.5</b> Non-linear Transformations of the Predictors</a></li>
<li class="chapter" data-level="3.6.6" data-path="linear-regression.html"><a href="linear-regression.html#qualitative-predictors-1"><i class="fa fa-check"></i><b>3.6.6</b> Qualitative Predictors</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="linear-regression.html"><a href="linear-regression.html#exercises"><i class="fa fa-check"></i><b>3.7</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#applied"><i class="fa fa-check"></i>Applied</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#reproducibility"><i class="fa fa-check"></i>Reproducibility</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>4</b> Classification</a>
<ul>
<li class="chapter" data-level="4.1" data-path="classification.html"><a href="classification.html#an-overview-of-classification"><i class="fa fa-check"></i><b>4.1</b> An Overview of Classification</a></li>
<li class="chapter" data-level="4.2" data-path="classification.html"><a href="classification.html#why-not-linear-regression"><i class="fa fa-check"></i><b>4.2</b> Why Not Linear Regression?</a></li>
<li class="chapter" data-level="4.3" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>4.3</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="classification.html"><a href="classification.html#the-logistic-model"><i class="fa fa-check"></i><b>4.3.1</b> The Logistic Model</a></li>
<li class="chapter" data-level="4.3.2" data-path="classification.html"><a href="classification.html#estimating-the-regression-coefficients-1"><i class="fa fa-check"></i><b>4.3.2</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="4.3.3" data-path="classification.html"><a href="classification.html#making-predictions"><i class="fa fa-check"></i><b>4.3.3</b> Making Predictions</a></li>
<li class="chapter" data-level="4.3.4" data-path="classification.html"><a href="classification.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>4.3.4</b> Multiple Logistic Regression</a></li>
<li class="chapter" data-level="4.3.5" data-path="classification.html"><a href="classification.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>4.3.5</b> Multinomial Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="classification.html"><a href="classification.html#generative-models-for-classification"><i class="fa fa-check"></i><b>4.4</b> Generative Models for Classification</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-for-p-1"><i class="fa fa-check"></i><b>4.4.1</b> Linear Discriminant Analysis for <span class="math inline">\(p = 1\)</span></a></li>
<li class="chapter" data-level="4.4.2" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-for-p-1-1"><i class="fa fa-check"></i><b>4.4.2</b> Linear Discriminant Analysis for <span class="math inline">\(p &gt; 1\)</span></a></li>
<li class="chapter" data-level="4.4.3" data-path="classification.html"><a href="classification.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>4.4.3</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="4.4.4" data-path="classification.html"><a href="classification.html#naive-bayes"><i class="fa fa-check"></i><b>4.4.4</b> Naive Bayes</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="classification.html"><a href="classification.html#a-comparison-of-classification-methods"><i class="fa fa-check"></i><b>4.5</b> A Comparison of Classification Methods</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="classification.html"><a href="classification.html#an-analytical-comparison"><i class="fa fa-check"></i><b>4.5.1</b> An Analytical Comparison</a></li>
<li class="chapter" data-level="4.5.2" data-path="classification.html"><a href="classification.html#an-empirical-comparison"><i class="fa fa-check"></i><b>4.5.2</b> An Empirical Comparison</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="classification.html"><a href="classification.html#generalized-linear-models"><i class="fa fa-check"></i><b>4.6</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="classification.html"><a href="classification.html#linear-regression-on-the-bikeshare-data"><i class="fa fa-check"></i><b>4.6.1</b> Linear Regression on the Bikeshare Data</a></li>
<li class="chapter" data-level="4.6.2" data-path="classification.html"><a href="classification.html#poisson-regression-on-bikeshare-data"><i class="fa fa-check"></i><b>4.6.2</b> Poisson Regression on Bikeshare Data</a></li>
<li class="chapter" data-level="4.6.3" data-path="classification.html"><a href="classification.html#generalized-linear-models-in-greater-generality"><i class="fa fa-check"></i><b>4.6.3</b> Generalized Linear Models in Greater Generality</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="classification.html"><a href="classification.html#lab-classification-methods"><i class="fa fa-check"></i><b>4.7</b> Lab: Classification Methods</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="classification.html"><a href="classification.html#the-stock-market-data"><i class="fa fa-check"></i><b>4.7.1</b> The Stock Market Data</a></li>
<li class="chapter" data-level="4.7.2" data-path="classification.html"><a href="classification.html#logistic-regression-1"><i class="fa fa-check"></i><b>4.7.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="4.7.3" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>4.7.3</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="4.7.4" data-path="classification.html"><a href="classification.html#quadratic-discriminant-analysis-1"><i class="fa fa-check"></i><b>4.7.4</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="4.7.5" data-path="classification.html"><a href="classification.html#naive-bayes-1"><i class="fa fa-check"></i><b>4.7.5</b> Naive Bayes</a></li>
<li class="chapter" data-level="4.7.6" data-path="classification.html"><a href="classification.html#k-nearest-neighbors-1"><i class="fa fa-check"></i><b>4.7.6</b> <span class="math inline">\(K\)</span>-Nearest Neighbors</a></li>
<li class="chapter" data-level="4.7.7" data-path="classification.html"><a href="classification.html#poisson-regression"><i class="fa fa-check"></i><b>4.7.7</b> Poisson Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="classification.html"><a href="classification.html#exercises-1"><i class="fa fa-check"></i><b>4.8</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="classification.html"><a href="classification.html#applied-1"><i class="fa fa-check"></i>Applied</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="classification.html"><a href="classification.html#reproducibility-1"><i class="fa fa-check"></i>Reproducibility</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="resampling-methods.html"><a href="resampling-methods.html"><i class="fa fa-check"></i><b>5</b> Resampling Methods</a>
<ul>
<li class="chapter" data-level="5.1" data-path="resampling-methods.html"><a href="resampling-methods.html#cross-validation"><i class="fa fa-check"></i><b>5.1</b> Cross Validation</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="resampling-methods.html"><a href="resampling-methods.html#validation-set"><i class="fa fa-check"></i><b>5.1.1</b> The Validation Set Approach</a></li>
<li class="chapter" data-level="5.1.2" data-path="resampling-methods.html"><a href="resampling-methods.html#loocv"><i class="fa fa-check"></i><b>5.1.2</b> Leave-One-Out Cross Validation</a></li>
<li class="chapter" data-level="5.1.3" data-path="resampling-methods.html"><a href="resampling-methods.html#kfoldcv"><i class="fa fa-check"></i><b>5.1.3</b> <span class="math inline">\(k\)</span>-fold Cross-Validation</a></li>
<li class="chapter" data-level="5.1.4" data-path="resampling-methods.html"><a href="resampling-methods.html#bias-variance-trade-off-for-k-fold-cross-validation"><i class="fa fa-check"></i><b>5.1.4</b> Bias-Variance Trade-Off for <span class="math inline">\(k\)</span>-Fold Cross-Validation</a></li>
<li class="chapter" data-level="5.1.5" data-path="resampling-methods.html"><a href="resampling-methods.html#cross-validation-on-classification-problems"><i class="fa fa-check"></i><b>5.1.5</b> Cross-Validation on Classification Problems</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="resampling-methods.html"><a href="resampling-methods.html#bootstrap-lab"><i class="fa fa-check"></i><b>5.2</b> The Bootstrap</a></li>
<li class="chapter" data-level="5.3" data-path="resampling-methods.html"><a href="resampling-methods.html#lab-cross-validation-and-the-bootstrap"><i class="fa fa-check"></i><b>5.3</b> Lab: Cross-Validation and the Bootstrap</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="resampling-methods.html"><a href="resampling-methods.html#the-validation-set-approach"><i class="fa fa-check"></i><b>5.3.1</b> The Validation Set Approach</a></li>
<li class="chapter" data-level="5.3.2" data-path="resampling-methods.html"><a href="resampling-methods.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>5.3.2</b> Leave-One-Out Cross-Validation</a></li>
<li class="chapter" data-level="5.3.3" data-path="resampling-methods.html"><a href="resampling-methods.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.3.3</b> <span class="math inline">\(k\)</span>-fold Cross-Validation</a></li>
<li class="chapter" data-level="5.3.4" data-path="resampling-methods.html"><a href="resampling-methods.html#bootstrap"><i class="fa fa-check"></i><b>5.3.4</b> The Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="resampling-methods.html"><a href="resampling-methods.html#exercises-2"><i class="fa fa-check"></i><b>5.4</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="resampling-methods.html"><a href="resampling-methods.html#applied-2"><i class="fa fa-check"></i>Applied</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="resampling-methods.html"><a href="resampling-methods.html#reproducibility-2"><i class="fa fa-check"></i>Reproducibility</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html"><i class="fa fa-check"></i><b>6</b> Linear Model Selection and Regularization</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#subset-selection"><i class="fa fa-check"></i><b>6.1</b> Subset Selection</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#best-subset-selection"><i class="fa fa-check"></i><b>6.1.1</b> Best Subset Selection</a></li>
<li class="chapter" data-level="6.1.2" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#stepwise-selection"><i class="fa fa-check"></i><b>6.1.2</b> Stepwise Selection</a></li>
<li class="chapter" data-level="6.1.3" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#choosing-the-optimal-model"><i class="fa fa-check"></i><b>6.1.3</b> Choosing the Optimal Model</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#shrinkage-methods"><i class="fa fa-check"></i><b>6.2</b> Shrinkage Methods</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#ridge-regression"><i class="fa fa-check"></i><b>6.2.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="6.2.2" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#the-lasso"><i class="fa fa-check"></i><b>6.2.2</b> The Lasso</a></li>
<li class="chapter" data-level="6.2.3" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#selecting-the-tuning-parameter"><i class="fa fa-check"></i><b>6.2.3</b> Selecting the Tuning Parameter</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#dimension-reduction-methods"><i class="fa fa-check"></i><b>6.3</b> Dimension Reduction Methods</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#principal-components-regression"><i class="fa fa-check"></i><b>6.3.1</b> Principal Components Regression</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#partial-least-squares"><i class="fa fa-check"></i><b>6.3.2</b> Partial Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#considerations-in-high-dimensions"><i class="fa fa-check"></i><b>6.4</b> Considerations in High Dimensions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#high-dimensional-data"><i class="fa fa-check"></i><b>6.4.1</b> High-Dimensional Data</a></li>
<li class="chapter" data-level="6.4.2" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#what-goes-wrong-in-high-dimensions"><i class="fa fa-check"></i><b>6.4.2</b> What Goes Wrong in High Dimensions?</a></li>
<li class="chapter" data-level="6.4.3" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#regression-in-high-dimensions"><i class="fa fa-check"></i><b>6.4.3</b> Regression in High Dimensions</a></li>
<li class="chapter" data-level="6.4.4" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#interpreting-results-in-high-dimensions"><i class="fa fa-check"></i><b>6.4.4</b> Interpreting Results in High Dimensions</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#lab-linear-models-and-regularization-methods"><i class="fa fa-check"></i><b>6.5</b> Lab: Linear Models and Regularization Methods</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#subset-selection-methods"><i class="fa fa-check"></i><b>6.5.1</b> Subset Selection Methods</a></li>
<li class="chapter" data-level="6.5.2" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#ridge-regression-and-the-lasso"><i class="fa fa-check"></i><b>6.5.2</b> Ridge Regression and the Lasso</a></li>
<li class="chapter" data-level="6.5.3" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#pcr-and-pls-regression"><i class="fa fa-check"></i><b>6.5.3</b> PCR and PLS Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#exercises-3"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
<li class="chapter" data-level="" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#reproducibility-3"><i class="fa fa-check"></i>Reproducibility</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Statistical Learning with the tidyverse</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-model-selection-and-regularization" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Linear Model Selection and Regularization</h1>
<p>Load the packages used in this chapter:</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="linear-model-selection-and-regularization.html#cb477-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb477-2"><a href="linear-model-selection-and-regularization.html#cb477-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb477-3"><a href="linear-model-selection-and-regularization.html#cb477-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(broom)</span>
<span id="cb477-4"><a href="linear-model-selection-and-regularization.html#cb477-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gt)</span>
<span id="cb477-5"><a href="linear-model-selection-and-regularization.html#cb477-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb477-6"><a href="linear-model-selection-and-regularization.html#cb477-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tictoc)</span>
<span id="cb477-7"><a href="linear-model-selection-and-regularization.html#cb477-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb477-8"><a href="linear-model-selection-and-regularization.html#cb477-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load my R package and set the ggplot theme</span></span>
<span id="cb477-9"><a href="linear-model-selection-and-regularization.html#cb477-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dunnr)</span>
<span id="cb477-10"><a href="linear-model-selection-and-regularization.html#cb477-10" aria-hidden="true" tabindex="-1"></a>extrafont<span class="sc">::</span><span class="fu">loadfonts</span>(<span class="at">device =</span> <span class="st">&quot;win&quot;</span>, <span class="at">quiet =</span> <span class="cn">TRUE</span>)</span>
<span id="cb477-11"><a href="linear-model-selection-and-regularization.html#cb477-11" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_td</span>())</span>
<span id="cb477-12"><a href="linear-model-selection-and-regularization.html#cb477-12" aria-hidden="true" tabindex="-1"></a><span class="fu">set_geom_fonts</span>()</span>
<span id="cb477-13"><a href="linear-model-selection-and-regularization.html#cb477-13" aria-hidden="true" tabindex="-1"></a><span class="fu">set_palette</span>()</span></code></pre></div>
<p>Before discussing non-linear models in Chapters 7, 8 and 10, this chapter discusses some ways in which the simple linear model can be improved by replacing the familiar least squares fitting with some alternative fitting procedures.
These alternatives can sometimes yield better <em>prediction accuracy</em> and <em>model interpretability</em>.</p>
<blockquote>
<p>Prediction Accuracy: Provided that the true relationship between the
response and the predictors is approximately linear, the least squares
estimates will have low bias. If <span class="math inline">\(n &gt;&gt; p\)</span>—that is, if <span class="math inline">\(n\)</span>, the number of
observations, is much larger than <span class="math inline">\(p\)</span>, the number of variables—then the
least squares estimates tend to also have low variance, and hence will
perform well on test observations. However, if <span class="math inline">\(n\)</span> is not much larger
than <span class="math inline">\(p\)</span>, then there can be a lot of variability in the least squares fit,
resulting in overfitting and consequently poor predictions on future
observations not used in model training. And if <span class="math inline">\(p &gt; n\)</span>, then there
is no longer a unique least squares coefficient estimate: the variance
is infinite so the method cannot be used at all. By constraining or
shrinking the estimated coefficients, we can often substantially reduce
the variance at the cost of a negligible increase in bias. This can
lead to substantial improvements in the accuracy with which we can
predict the response for observations not used in model training.</p>
</blockquote>
<blockquote>
<p>Model Interpretability: It is often the case that some or many of the
variables used in a multiple regression model are in fact not associated with the response. Including such irrelevant variables leads to
unnecessary complexity in the resulting model. By removing these
variables—that is, by setting the corresponding coefficient estimates
to zero—we can obtain a model that is more easily interpreted. Now
least squares is extremely unlikely to yield any coefficient estimates
that are exactly zero. In this chapter, we see some approaches for au-
tomatically performing feature selection or variable selection—that is, feature
for excluding irrelevant variables from a multiple regression model.</p>
</blockquote>
<p>In this chapter, we discuss three important classes of methods:</p>
<blockquote>
<p>Subset Selection. This approach involves identifying a subset of the <span class="math inline">\(p\)</span>
predictors that we believe to be related to the response. We then fit
a model using least squares on the reduced set of variables.</p>
<p>Shrinkage. This approach involves fitting a model involving all <span class="math inline">\(p\)</span> predictors. However, the estimated coefficients are shrunken towards zero
relative to the least squares estimates. This shrinkage (also known as
regularization) has the effect of reducing variance. Depending on what
type of shrinkage is performed, some of the coefficients may be esti-
mated to be exactly zero. Hence, shrinkage methods can also perform
variable selection.</p>
<p>Dimension Reduction. This approach involves projecting the <span class="math inline">\(p\)</span> predictors into an <span class="math inline">\(M\)</span>-dimensional subspace, where <span class="math inline">\(M &lt; p\)</span>. This is achieved
by computing <span class="math inline">\(M\)</span> different linear combinations, or projections, of the
variables. Then these <span class="math inline">\(M\)</span> projections are used as predictors to fit a
linear regression model by least squares.</p>
</blockquote>
<p>Although this chapter is specifically about extensions to the linear model for regression, the same concepts apply to other methods, such as the classification models in Chapter 4.</p>
<div id="subset-selection" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Subset Selection</h2>
<p>Disclaimer at the top: as mentioned in section 3.1, there are a lot of reasons to avoid subset and stepwise model selection.
Here are some resources on this topic:</p>
<ul>
<li>The 2018 paper by <span class="citation"><a href="#ref-Smith2018" role="doc-biblioref">Smith</a> (<a href="#ref-Smith2018" role="doc-biblioref">2018</a>)</span>.</li>
<li><a href="https://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856">A Stack Overflow response.</a></li>
<li><a href="https://www.stata.com/support/faqs/statistics/stepwise-regression-problems/">Frank Harrell comments.</a></li>
</ul>
<p>Regardless, I will still work through the examples in the text as a programming exercise.
However, <code>tidymodels</code> does not have the functionality for subset/stepwise selection, so I will be using alternatives.</p>
<div id="best-subset-selection" class="section level3" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Best Subset Selection</h3>
<p>To perform <em>best subset selection</em>, we fit <span class="math inline">\(p\)</span> models that contain exactly one predictor, <span class="math inline">\({p \choose 2} = p(p-1)/2\)</span> models that contain exactly two predictors, and so on.
In total, this involves fitting <span class="math inline">\(2^p\)</span> models.
Then we select the model that is best, usually following these steps</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(\mathcal{M}_0\)</span> denote the <em>null model</em>, which contains no predictors. This model simply predicts the sample mean.</li>
<li>For <span class="math inline">\(k = 1, 2, \dots, p\)</span>:
<ul>
<li>Fit all <span class="math inline">\({p \choose k}\)</span> models that contain exactly <span class="math inline">\(k\)</span> predictors.</li>
<li>Pick the best among these <span class="math inline">\({p \choose k}\)</span> models, and call it <span class="math inline">\(\mathcal{M}_k\)</span>. Here, <em>best</em> is defined as having the smallest RSS, or equivalently the largest <span class="math inline">\(R^2\)</span>.</li>
</ul></li>
<li>Select a single best model from among <span class="math inline">\(\mathcal{M}_0, \dots, \mathcal{M}_p\)</span> using cross-validated prediction error <span class="math inline">\(C_p\)</span> (AIC), BIC, or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
<p>Step 2 identifies the best model (on the training data) for each subset size, in order to reduce the problem from <span class="math inline">\(2^p\)</span> to <span class="math inline">\(p + 1\)</span> possible models.
Choosing the single best model from the <span class="math inline">\(p + 1\)</span> options must be done with care, because the RSS of these models decreases monotonically, and the <span class="math inline">\(R^2\)</span> increases monotonically, as the number of predictors increases.
A model that minimizes these metrics will have a low <em>training error</em>, but not necessarily a low <em>test error</em>, as we saw in Chapter 2 in Figures 2.9-2.11.
Therefore, in step 3, we use a cross-validated prediction error <span class="math inline">\(C_p\)</span>, BIC, or adjusted <span class="math inline">\(R^2\)</span> in order to select the best model.</p>
<p>Figure 6.1 includes many least squares regression models predicting <code>Balance</code>, and fit using a different subsets of 10 predictors.
Load the <code>credit</code> data set:</p>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb478-1"><a href="linear-model-selection-and-regularization.html#cb478-1" aria-hidden="true" tabindex="-1"></a>credit <span class="ot">&lt;-</span> ISLR2<span class="sc">::</span>Credit</span>
<span id="cb478-2"><a href="linear-model-selection-and-regularization.html#cb478-2" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(credit)</span></code></pre></div>
<pre><code>## Rows: 400
## Columns: 11
## $ Income    &lt;dbl&gt; 14.891, 106.025, 104.593, 148.924, 55.882, 80.180, 20.996, 7~
## $ Limit     &lt;dbl&gt; 3606, 6645, 7075, 9504, 4897, 8047, 3388, 7114, 3300, 6819, ~
## $ Rating    &lt;dbl&gt; 283, 483, 514, 681, 357, 569, 259, 512, 266, 491, 589, 138, ~
## $ Cards     &lt;dbl&gt; 2, 3, 4, 3, 2, 4, 2, 2, 5, 3, 4, 3, 1, 1, 2, 3, 3, 3, 1, 2, ~
## $ Age       &lt;dbl&gt; 34, 82, 71, 36, 68, 77, 37, 87, 66, 41, 30, 64, 57, 49, 75, ~
## $ Education &lt;dbl&gt; 11, 15, 11, 11, 16, 10, 12, 9, 13, 19, 14, 16, 7, 9, 13, 15,~
## $ Own       &lt;fct&gt; No, Yes, No, Yes, No, No, Yes, No, Yes, Yes, No, No, Yes, No~
## $ Student   &lt;fct&gt; No, Yes, No, No, No, No, No, No, No, Yes, No, No, No, No, No~
## $ Married   &lt;fct&gt; Yes, Yes, No, No, Yes, No, No, No, No, Yes, Yes, No, Yes, Ye~
## $ Region    &lt;fct&gt; South, West, West, West, South, South, East, West, South, Ea~
## $ Balance   &lt;dbl&gt; 333, 903, 580, 964, 331, 1151, 203, 872, 279, 1350, 1407, 0,~</code></pre>
<p>For <span class="math inline">\(p = 10\)</span> predictors, there are <span class="math inline">\(2^{10} = 1024\)</span> possible combinations for models (including the null model, but this example doesn’t include it).
To get every combination, I’ll use the <code>utils::combn()</code> function:</p>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb480-1"><a href="linear-model-selection-and-regularization.html#cb480-1" aria-hidden="true" tabindex="-1"></a>credit_predictors <span class="ot">&lt;-</span> <span class="fu">names</span>(credit)</span>
<span id="cb480-2"><a href="linear-model-selection-and-regularization.html#cb480-2" aria-hidden="true" tabindex="-1"></a>credit_predictors <span class="ot">&lt;-</span> credit_predictors[credit_predictors <span class="sc">!=</span> <span class="st">&quot;Balance&quot;</span>]</span>
<span id="cb480-3"><a href="linear-model-selection-and-regularization.html#cb480-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb480-4"><a href="linear-model-selection-and-regularization.html#cb480-4" aria-hidden="true" tabindex="-1"></a>credit_model_subsets <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb480-5"><a href="linear-model-selection-and-regularization.html#cb480-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">n_preds =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,</span>
<span id="cb480-6"><a href="linear-model-selection-and-regularization.html#cb480-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">predictors =</span> <span class="fu">map</span>(n_preds,</span>
<span id="cb480-7"><a href="linear-model-selection-and-regularization.html#cb480-7" aria-hidden="true" tabindex="-1"></a>                   <span class="sc">~</span> utils<span class="sc">::</span><span class="fu">combn</span>(credit_predictors, .x, <span class="at">simplify =</span> <span class="cn">FALSE</span>))</span>
<span id="cb480-8"><a href="linear-model-selection-and-regularization.html#cb480-8" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb480-9"><a href="linear-model-selection-and-regularization.html#cb480-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(predictors) <span class="sc">%&gt;%</span></span>
<span id="cb480-10"><a href="linear-model-selection-and-regularization.html#cb480-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb480-11"><a href="linear-model-selection-and-regularization.html#cb480-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">model_formula =</span> <span class="fu">map</span>(predictors,</span>
<span id="cb480-12"><a href="linear-model-selection-and-regularization.html#cb480-12" aria-hidden="true" tabindex="-1"></a>                        <span class="sc">~</span> <span class="fu">as.formula</span>(<span class="fu">paste</span>(<span class="st">&quot;Balance ~&quot;</span>, <span class="fu">paste</span>(.x, <span class="at">collapse =</span> <span class="st">&quot;+&quot;</span>))))</span>
<span id="cb480-13"><a href="linear-model-selection-and-regularization.html#cb480-13" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb480-14"><a href="linear-model-selection-and-regularization.html#cb480-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb480-15"><a href="linear-model-selection-and-regularization.html#cb480-15" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(credit_model_subsets)</span></code></pre></div>
<pre><code>## Rows: 1,023
## Columns: 3
## $ n_preds       &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,~
## $ predictors    &lt;list&gt; &quot;Income&quot;, &quot;Limit&quot;, &quot;Rating&quot;, &quot;Cards&quot;, &quot;Age&quot;, &quot;Education~
## $ model_formula &lt;list&gt; &lt;Balance ~ Income&gt;, &lt;Balance ~ Limit&gt;, &lt;Balance ~ Ratin~</code></pre>
<p>For differing numbers of predictors <span class="math inline">\(k = 1, 2, \dots, p\)</span>, we should have <span class="math inline">\({p \choose k}\)</span> models:</p>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb482-1"><a href="linear-model-selection-and-regularization.html#cb482-1" aria-hidden="true" tabindex="-1"></a>credit_model_subsets <span class="sc">%&gt;%</span></span>
<span id="cb482-2"><a href="linear-model-selection-and-regularization.html#cb482-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(n_preds) <span class="sc">%&gt;%</span></span>
<span id="cb482-3"><a href="linear-model-selection-and-regularization.html#cb482-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">p_choose_k =</span> <span class="fu">choose</span>(<span class="dv">10</span>, n_preds))</span></code></pre></div>
<pre><code>## # A tibble: 10 x 3
##    n_preds     n p_choose_k
##      &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;
##  1       1    10         10
##  2       2    45         45
##  3       3   120        120
##  4       4   210        210
##  5       5   252        252
##  6       6   210        210
##  7       7   120        120
##  8       8    45         45
##  9       9    10         10
## 10      10     1          1</code></pre>
<p>Fit all of the models and extract RSS and <span class="math inline">\(R^2\)</span> metrics:</p>
<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb484-1"><a href="linear-model-selection-and-regularization.html#cb484-1" aria-hidden="true" tabindex="-1"></a>credit_model_subsets <span class="ot">&lt;-</span> credit_model_subsets <span class="sc">%&gt;%</span></span>
<span id="cb484-2"><a href="linear-model-selection-and-regularization.html#cb484-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb484-3"><a href="linear-model-selection-and-regularization.html#cb484-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">model_fit =</span> <span class="fu">map</span>(model_formula, <span class="sc">~</span> <span class="fu">lm</span>(.x, <span class="at">data =</span> credit)),</span>
<span id="cb484-4"><a href="linear-model-selection-and-regularization.html#cb484-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">RSS =</span> <span class="fu">map_dbl</span>(model_fit, <span class="sc">~</span> <span class="fu">sum</span>(.x<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)),</span>
<span id="cb484-5"><a href="linear-model-selection-and-regularization.html#cb484-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">R2 =</span> <span class="fu">map_dbl</span>(model_fit, <span class="sc">~</span> <span class="fu">summary</span>(.x)<span class="sc">$</span>r.squared),</span>
<span id="cb484-6"><a href="linear-model-selection-and-regularization.html#cb484-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Because of one of the categorical variables (Region) having three levels,</span></span>
<span id="cb484-7"><a href="linear-model-selection-and-regularization.html#cb484-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># some models will have +1 dummy variable predictor, which I can calculate</span></span>
<span id="cb484-8"><a href="linear-model-selection-and-regularization.html#cb484-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># from the number of coefficients returned from the fit</span></span>
<span id="cb484-9"><a href="linear-model-selection-and-regularization.html#cb484-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">n_preds_adj =</span> <span class="fu">map_int</span>(model_fit, <span class="sc">~</span> <span class="fu">length</span>(.x<span class="sc">$</span>coefficients) <span class="sc">-</span> 1L)</span>
<span id="cb484-10"><a href="linear-model-selection-and-regularization.html#cb484-10" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p>Figure 6.1:</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb485-1"><a href="linear-model-selection-and-regularization.html#cb485-1" aria-hidden="true" tabindex="-1"></a>credit_model_subsets <span class="sc">%&gt;%</span></span>
<span id="cb485-2"><a href="linear-model-selection-and-regularization.html#cb485-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">c</span>(RSS, R2), <span class="at">names_to =</span> <span class="st">&quot;metric&quot;</span>, <span class="at">values_to =</span> <span class="st">&quot;value&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb485-3"><a href="linear-model-selection-and-regularization.html#cb485-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">metric =</span> <span class="fu">factor</span>(metric, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">&quot;RSS&quot;</span>, <span class="st">&quot;R2&quot;</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb485-4"><a href="linear-model-selection-and-regularization.html#cb485-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(n_preds_adj, metric) <span class="sc">%&gt;%</span></span>
<span id="cb485-5"><a href="linear-model-selection-and-regularization.html#cb485-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb485-6"><a href="linear-model-selection-and-regularization.html#cb485-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The &quot;best&quot; model has the lowest value by RSS...</span></span>
<span id="cb485-7"><a href="linear-model-selection-and-regularization.html#cb485-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">best_model =</span> (metric <span class="sc">==</span> <span class="st">&quot;RSS&quot;</span> <span class="sc">&amp;</span> value <span class="sc">==</span> <span class="fu">min</span>(value)) <span class="sc">|</span></span>
<span id="cb485-8"><a href="linear-model-selection-and-regularization.html#cb485-8" aria-hidden="true" tabindex="-1"></a>      <span class="co"># ... and the highest value by R2</span></span>
<span id="cb485-9"><a href="linear-model-selection-and-regularization.html#cb485-9" aria-hidden="true" tabindex="-1"></a>      (metric <span class="sc">==</span> <span class="st">&quot;R2&quot;</span> <span class="sc">&amp;</span> value <span class="sc">==</span> <span class="fu">max</span>(value))</span>
<span id="cb485-10"><a href="linear-model-selection-and-regularization.html#cb485-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb485-11"><a href="linear-model-selection-and-regularization.html#cb485-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb485-12"><a href="linear-model-selection-and-regularization.html#cb485-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> n_preds_adj, <span class="at">y =</span> value)) <span class="sc">+</span></span>
<span id="cb485-13"><a href="linear-model-selection-and-regularization.html#cb485-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> . <span class="sc">%&gt;%</span> <span class="fu">filter</span>(best_model), <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb485-14"><a href="linear-model-selection-and-regularization.html#cb485-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_jitter</span>(<span class="at">width =</span> <span class="fl">0.05</span>, <span class="at">height =</span> <span class="dv">0</span>, <span class="at">alpha =</span> <span class="fl">0.3</span>,</span>
<span id="cb485-15"><a href="linear-model-selection-and-regularization.html#cb485-15" aria-hidden="true" tabindex="-1"></a>              <span class="at">color =</span> td_colors<span class="sc">$</span>nice<span class="sc">$</span>opera_mauve) <span class="sc">+</span></span>
<span id="cb485-16"><a href="linear-model-selection-and-regularization.html#cb485-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> metric, <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">scales =</span> <span class="st">&quot;free_y&quot;</span>) <span class="sc">+</span></span>
<span id="cb485-17"><a href="linear-model-selection-and-regularization.html#cb485-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="st">&quot;Number of predictors&quot;</span>, <span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">2</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/figure6.1-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>As expected, the values are monotonically decreasing (RSS) and increasing (<span class="math inline">\(R^2\)</span>) with number of predictors.
There is little improvement past 3 predictors, however.</p>
<blockquote>
<p>Although we have presented best subset selection here for least squares
regression, the same ideas apply to other types of models, such as logistic
regression. In the case of logistic regression, instead of ordering models by
RSS in Step 2 of Algorithm 6.1, we instead use the <em>deviance</em>, a measure
that plays the role of RSS for a broader class of models. The deviance is
negative two times the maximized log-likelihood; the smaller the deviance,
the better the fit.</p>
</blockquote>
<p>Because it scales as <span class="math inline">\(2^p\)</span> models, best subset selection can quickly become computationally expensive.
The next sections explore computationally efficient alternatives.</p>
</div>
<div id="stepwise-selection" class="section level3" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Stepwise Selection</h3>
<div id="forward-stepwise-selection" class="section level4 unnumbered">
<h4>Forward Stepwise Selection</h4>
<p>Forward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all predictors are in the model.</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(\mathcal{M}_0\)</span> denote the <em>null model</em>, which contains no predictors.</li>
<li>For <span class="math inline">\(k = 0, 1, \dots, p-1\)</span>:
<ul>
<li>Consider all <span class="math inline">\(p - k\)</span> models that augment the predictors in <span class="math inline">\(\mathcal{M}_k\)</span> with one additional predictor.</li>
<li>Choose the best among these <span class="math inline">\(p - k\)</span> models, and call it <span class="math inline">\(\mathcal{M}_{k+1}\)</span>. Here, <em>best</em> is defined as having smallest RSS or highest <span class="math inline">\(R^2\)</span>.</li>
</ul></li>
<li>Select a single best model from among <span class="math inline">\(\mathcal{M}_0, \dots, \mathcal{M}_p\)</span> using cross-validated prediction error <span class="math inline">\(C_p\)</span> (AIC), BIC, or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
<p>Step 2 is similar to step 2 in best subset selection, in that we simply choose the model with the lowest RSS or highest <span class="math inline">\(R^2\)</span>.
Step 3 is more tricky, and is discussed in Section 6.1.3.</p>
<p>Though much more computationally efficient, it is not guaranteed to find the best possible model (via best subset selection) out of all <span class="math inline">\(2^p\)</span> possible models.
As a comparison, the example in the text involves performing four forward steps to find the best predictors.
The <code>MASS</code> package has an <code>addTerm()</code> function for taking a single step:</p>
<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb486-1"><a href="linear-model-selection-and-regularization.html#cb486-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Model with no predictors</span></span>
<span id="cb486-2"><a href="linear-model-selection-and-regularization.html#cb486-2" aria-hidden="true" tabindex="-1"></a>balance_null <span class="ot">&lt;-</span> <span class="fu">lm</span>(Balance <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> credit)</span>
<span id="cb486-3"><a href="linear-model-selection-and-regularization.html#cb486-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Model with all predictors</span></span>
<span id="cb486-4"><a href="linear-model-selection-and-regularization.html#cb486-4" aria-hidden="true" tabindex="-1"></a>balance_full <span class="ot">&lt;-</span> <span class="fu">lm</span>(Balance <span class="sc">~</span> ., <span class="at">data =</span> credit)</span>
<span id="cb486-5"><a href="linear-model-selection-and-regularization.html#cb486-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb486-6"><a href="linear-model-selection-and-regularization.html#cb486-6" aria-hidden="true" tabindex="-1"></a>MASS<span class="sc">::</span><span class="fu">addterm</span>(balance_null, <span class="at">scope =</span> balance_full, <span class="at">sorted =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## Single term additions
## 
## Model:
## Balance ~ 1
##           Df Sum of Sq      RSS    AIC
## Rating     1  62904790 21435122 4359.6
## Limit      1  62624255 21715657 4364.8
## Income     1  18131167 66208745 4810.7
## Student    1   5658372 78681540 4879.8
## Cards      1    630416 83709496 4904.6
## &lt;none&gt;                 84339912 4905.6
## Own        1     38892 84301020 4907.4
## Education  1      5481 84334431 4907.5
## Married    1      2715 84337197 4907.5
## Age        1       284 84339628 4907.6
## Region     2     18454 84321458 4909.5</code></pre>
<p>Here, the <code>Rating</code> variable offers the best improvement over the null model (by both RSS and AIC).
To run this four times, I’ll use a for loop:</p>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb488-1"><a href="linear-model-selection-and-regularization.html#cb488-1" aria-hidden="true" tabindex="-1"></a>balance_preds <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;1&quot;</span>)</span>
<span id="cb488-2"><a href="linear-model-selection-and-regularization.html#cb488-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (forward_step <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>) {</span>
<span id="cb488-3"><a href="linear-model-selection-and-regularization.html#cb488-3" aria-hidden="true" tabindex="-1"></a>  balance_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(</span>
<span id="cb488-4"><a href="linear-model-selection-and-regularization.html#cb488-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">paste</span>(<span class="st">&quot;Balance ~&quot;</span>, <span class="fu">str_replace_all</span>(balance_preds[forward_step], <span class="st">&quot;,&quot;</span>, <span class="st">&quot;+&quot;</span>))</span>
<span id="cb488-5"><a href="linear-model-selection-and-regularization.html#cb488-5" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb488-6"><a href="linear-model-selection-and-regularization.html#cb488-6" aria-hidden="true" tabindex="-1"></a>  balance_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(balance_formula, <span class="at">data =</span> credit)</span>
<span id="cb488-7"><a href="linear-model-selection-and-regularization.html#cb488-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb488-8"><a href="linear-model-selection-and-regularization.html#cb488-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Find the next predictor by RSS</span></span>
<span id="cb488-9"><a href="linear-model-selection-and-regularization.html#cb488-9" aria-hidden="true" tabindex="-1"></a>  new_predictor <span class="ot">&lt;-</span> MASS<span class="sc">::</span><span class="fu">addterm</span>(balance_model, <span class="at">scope =</span> balance_full) <span class="sc">%&gt;%</span></span>
<span id="cb488-10"><a href="linear-model-selection-and-regularization.html#cb488-10" aria-hidden="true" tabindex="-1"></a>    broom<span class="sc">::</span><span class="fu">tidy</span>() <span class="sc">%&gt;%</span></span>
<span id="cb488-11"><a href="linear-model-selection-and-regularization.html#cb488-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(rss <span class="sc">==</span> <span class="fu">min</span>(rss)) <span class="sc">%&gt;%</span></span>
<span id="cb488-12"><a href="linear-model-selection-and-regularization.html#cb488-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pull</span>(term)</span>
<span id="cb488-13"><a href="linear-model-selection-and-regularization.html#cb488-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb488-14"><a href="linear-model-selection-and-regularization.html#cb488-14" aria-hidden="true" tabindex="-1"></a>  balance_preds <span class="ot">&lt;-</span> <span class="fu">append</span>(balance_preds,</span>
<span id="cb488-15"><a href="linear-model-selection-and-regularization.html#cb488-15" aria-hidden="true" tabindex="-1"></a>                          <span class="fu">paste</span>(balance_preds[forward_step], new_predictor,</span>
<span id="cb488-16"><a href="linear-model-selection-and-regularization.html#cb488-16" aria-hidden="true" tabindex="-1"></a>                                <span class="at">sep =</span> <span class="st">&quot;, &quot;</span>))</span>
<span id="cb488-17"><a href="linear-model-selection-and-regularization.html#cb488-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb488-18"><a href="linear-model-selection-and-regularization.html#cb488-18" aria-hidden="true" tabindex="-1"></a>balance_preds</span></code></pre></div>
<pre><code>## [1] &quot;1&quot;                                 &quot;1, Rating&quot;                        
## [3] &quot;1, Rating, Income&quot;                 &quot;1, Rating, Income, Student&quot;       
## [5] &quot;1, Rating, Income, Student, Limit&quot;</code></pre>
<p>Now re-create Table 6.1:</p>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb490-1"><a href="linear-model-selection-and-regularization.html#cb490-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_cols</span>(</span>
<span id="cb490-2"><a href="linear-model-selection-and-regularization.html#cb490-2" aria-hidden="true" tabindex="-1"></a>  credit_model_subsets <span class="sc">%&gt;%</span></span>
<span id="cb490-3"><a href="linear-model-selection-and-regularization.html#cb490-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(n_preds_adj <span class="sc">&lt;=</span> <span class="dv">4</span>) <span class="sc">%&gt;%</span></span>
<span id="cb490-4"><a href="linear-model-selection-and-regularization.html#cb490-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">group_by</span>(n_preds_adj) <span class="sc">%&gt;%</span></span>
<span id="cb490-5"><a href="linear-model-selection-and-regularization.html#cb490-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(RSS <span class="sc">==</span> <span class="fu">min</span>(RSS)) <span class="sc">%&gt;%</span></span>
<span id="cb490-6"><a href="linear-model-selection-and-regularization.html#cb490-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb490-7"><a href="linear-model-selection-and-regularization.html#cb490-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">transmute</span>(</span>
<span id="cb490-8"><a href="linear-model-selection-and-regularization.html#cb490-8" aria-hidden="true" tabindex="-1"></a>      <span class="st">`</span><span class="at"># variables</span><span class="st">`</span> <span class="ot">=</span> n_preds_adj,</span>
<span id="cb490-9"><a href="linear-model-selection-and-regularization.html#cb490-9" aria-hidden="true" tabindex="-1"></a>      <span class="st">`</span><span class="at">Best subset</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">map_chr</span>(predictors, str_c, <span class="at">collapse =</span> <span class="st">&quot;, &quot;</span>)</span>
<span id="cb490-10"><a href="linear-model-selection-and-regularization.html#cb490-10" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb490-11"><a href="linear-model-selection-and-regularization.html#cb490-11" aria-hidden="true" tabindex="-1"></a>  <span class="st">`</span><span class="at">Forward stepwise</span><span class="st">`</span> <span class="ot">=</span> balance_preds[<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>] <span class="sc">%&gt;%</span> <span class="fu">str_remove</span>(<span class="st">&quot;1, &quot;</span>)</span>
<span id="cb490-12"><a href="linear-model-selection-and-regularization.html#cb490-12" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb490-13"><a href="linear-model-selection-and-regularization.html#cb490-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gt</span>(<span class="at">rowname_col =</span> <span class="st">&quot;# variables&quot;</span>)</span></code></pre></div>
<div id="gehakohhxl" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#gehakohhxl .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#gehakohhxl .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#gehakohhxl .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#gehakohhxl .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 6px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#gehakohhxl .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#gehakohhxl .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#gehakohhxl .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#gehakohhxl .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#gehakohhxl .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#gehakohhxl .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#gehakohhxl .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#gehakohhxl .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#gehakohhxl .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#gehakohhxl .gt_from_md > :first-child {
  margin-top: 0;
}

#gehakohhxl .gt_from_md > :last-child {
  margin-bottom: 0;
}

#gehakohhxl .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#gehakohhxl .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#gehakohhxl .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#gehakohhxl .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#gehakohhxl .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#gehakohhxl .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#gehakohhxl .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#gehakohhxl .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#gehakohhxl .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#gehakohhxl .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#gehakohhxl .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#gehakohhxl .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#gehakohhxl .gt_left {
  text-align: left;
}

#gehakohhxl .gt_center {
  text-align: center;
}

#gehakohhxl .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#gehakohhxl .gt_font_normal {
  font-weight: normal;
}

#gehakohhxl .gt_font_bold {
  font-weight: bold;
}

#gehakohhxl .gt_font_italic {
  font-style: italic;
}

#gehakohhxl .gt_super {
  font-size: 65%;
}

#gehakohhxl .gt_footnote_marks {
  font-style: italic;
  font-weight: normal;
  font-size: 65%;
}
</style>
<table class="gt_table">
  
  <thead class="gt_col_headings">
    <tr>
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1"></th>
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1">Best subset</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1">Forward stepwise</th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td class="gt_row gt_left gt_stub">1</td>
<td class="gt_row gt_left">Rating</td>
<td class="gt_row gt_left">Rating</td></tr>
    <tr><td class="gt_row gt_left gt_stub">2</td>
<td class="gt_row gt_left">Income, Rating</td>
<td class="gt_row gt_left">Rating, Income</td></tr>
    <tr><td class="gt_row gt_left gt_stub">3</td>
<td class="gt_row gt_left">Income, Rating, Student</td>
<td class="gt_row gt_left">Rating, Income, Student</td></tr>
    <tr><td class="gt_row gt_left gt_stub">4</td>
<td class="gt_row gt_left">Income, Limit, Cards, Student</td>
<td class="gt_row gt_left">Rating, Income, Student, Limit</td></tr>
  </tbody>
  
  
</table>
</div>
</div>
<div id="backward-stepwise-selection" class="section level4 unnumbered">
<h4>Backward Stepwise Selection</h4>
<p>Backwards stepwise selection begins with the full model containing all <span class="math inline">\(p\)</span> predictors, and then iteratively removes the least useful predictor.</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(\mathcal{M}_p\)</span> denote the <em>full model</em>, which contains all <span class="math inline">\(p\)</span> predictors.</li>
<li>For <span class="math inline">\(k = p, p-1, \dots, 1\)</span>:
<ul>
<li>Consider all <span class="math inline">\(k\)</span> models that contain all but one of the predictors in <span class="math inline">\(\mathcal{M}_k\)</span> for a total of <span class="math inline">\(k - 1\)</span> predictors.</li>
<li>Choose the best among these <span class="math inline">\(k\)</span> models, and call it <span class="math inline">\(\mathcal{M}_{k-1}\)</span>. Here, <em>best</em> is defined as having smallest RSS or highest <span class="math inline">\(R^2\)</span>.</li>
</ul></li>
<li>Select a single best model from among <span class="math inline">\(\mathcal{M}_0, \dots, \mathcal{M}_p\)</span> using cross-validated prediction error <span class="math inline">\(C_p\)</span> (AIC), BIC, or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
<blockquote>
<p>Like forward stepwise selection, the backward selection approach searches
through only <span class="math inline">\(1+p(p+1)/2\)</span> models, and so can be applied in settings where
<span class="math inline">\(p\)</span> is too large to apply best subset selection. Also like forward stepwise
selection, backward stepwise selection is not guaranteed to yield the best
model containing a subset of the <span class="math inline">\(p\)</span> predictors.</p>
</blockquote>
<p>Backward selection requires that the number of samples <span class="math inline">\(n\)</span> is larger than
the number of variables <span class="math inline">\(p\)</span> (so that the full model can be fit). In contrast,
forward stepwise can be used even when <span class="math inline">\(n &lt; p\)</span>, and so is the only viable
subset method when <span class="math inline">\(p\)</span> is very large.</p>
</div>
<div id="hybrid-approaches" class="section level4 unnumbered">
<h4>Hybrid Approaches</h4>
<blockquote>
<p>The best subset, forward stepwise, and backward stepwise selection approaches generally give similar but not identical models. As another alternative, hybrid versions of forward and backward stepwise selection are
available, in which variables are added to the model sequentially, in analogy
to forward selection. However, after adding each new variable, the method
may also remove any variables that no longer provide an improvement in
the model fit. Such an approach attempts to more closely mimic best subset selection while retaining the computational advantages of forward and
backward stepwise selection.</p>
</blockquote>
</div>
</div>
<div id="choosing-the-optimal-model" class="section level3" number="6.1.3">
<h3><span class="header-section-number">6.1.3</span> Choosing the Optimal Model</h3>
<p>To apply these subset selection methods, we need to determine which model is <em>best</em>.
Since more predictors will always lead to smaller RSS and larger <span class="math inline">\(R^2\)</span> (training error), we need to estimate the test error.
There are two common approaches:</p>
<ol style="list-style-type: decimal">
<li>We can indirectly estimate test error by making an adjustment to the
training error to account for the bias due to overfitting.</li>
<li>We can directly estimate the test error, using either a validation set
approach or a cross-validation approach, as discussed in Chapter 5.</li>
</ol>
<div id="c_p-aic-bic-and-adjusted-r2" class="section level4 unnumbered">
<h4><span class="math inline">\(C_p\)</span>, AIC, BIC, and Adjusted <span class="math inline">\(R^2\)</span></h4>
<p>These techniques involve <em>adjusting</em> the training error to select among a set a models with different numbers of variables.</p>
<p>For a fitted least squares model containg <span class="math inline">\(d\)</span> predictors, the <span class="math inline">\(C_p\)</span> estimate of test MSE is computed as:</p>
<p><span class="math display">\[
C_p = \frac{1}{n} (\text{RSS} + 2d \hat{\sigma}^2),
\]</span></p>
<p>where <span class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the variance of the error <span class="math inline">\(\epsilon\)</span> associated with each response measurement.
Typically, this is estimated using the full model containing all predictors.</p>
<blockquote>
<p>Essentially, the <span class="math inline">\(C_p\)</span> statistic adds a penalty
of <span class="math inline">\(2d\hat{\sigma}^2\)</span> to the training RSS in order to adjust for the fact that the training
error tends to underestimate the test error. Clearly, the penalty increases as
the number of predictors in the model increases; this is intended to adjust
for the corresponding decrease in training RSS. Though it is beyond the
scope of this book, one can show that if <span class="math inline">\(\hat{\sigma}^2\)</span> is an unbiased estimate of <span class="math inline">\(\sigma^2\)</span> in
(6.2), then <span class="math inline">\(C_p\)</span> is an unbiased estimate of test MSE. As a consequence, the
<span class="math inline">\(C_p\)</span> statistic tends to take on a small value for models with a low test error,
so when determining which of a set of models is best, we choose the model
with the lowest <span class="math inline">\(C_p\)</span> value.</p>
</blockquote>
<p>Compute <span class="math inline">\(\hat{\sigma}\)</span> and <span class="math inline">\(C_p\)</span> for the best model at the different numbers of predictors:</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="linear-model-selection-and-regularization.html#cb491-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the estimated variance of the error for calculating C_p</span></span>
<span id="cb491-2"><a href="linear-model-selection-and-regularization.html#cb491-2" aria-hidden="true" tabindex="-1"></a>sigma_hat <span class="ot">&lt;-</span> <span class="fu">summary</span>(balance_full)<span class="sc">$</span>sigma</span>
<span id="cb491-3"><a href="linear-model-selection-and-regularization.html#cb491-3" aria-hidden="true" tabindex="-1"></a>credit_model_best <span class="ot">&lt;-</span> credit_model_subsets <span class="sc">%&gt;%</span></span>
<span id="cb491-4"><a href="linear-model-selection-and-regularization.html#cb491-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(n_preds_adj) <span class="sc">%&gt;%</span></span>
<span id="cb491-5"><a href="linear-model-selection-and-regularization.html#cb491-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(RSS <span class="sc">==</span> <span class="fu">min</span>(RSS)) <span class="sc">%&gt;%</span></span>
<span id="cb491-6"><a href="linear-model-selection-and-regularization.html#cb491-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb491-7"><a href="linear-model-selection-and-regularization.html#cb491-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb491-8"><a href="linear-model-selection-and-regularization.html#cb491-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">C_p</span><span class="st">`</span> <span class="ot">=</span> (<span class="dv">1</span> <span class="sc">/</span> <span class="fu">nrow</span>(credit)) <span class="sc">*</span> (RSS <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> n_preds_adj <span class="sc">*</span> sigma_hat<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb491-9"><a href="linear-model-selection-and-regularization.html#cb491-9" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<blockquote>
<p>The AIC criterion is defined for a large class of models fit by maximum
likelihood. In the case of the model (6.1) with Gaussian errors, maximum
likelihood and least squares are the same thing. In this case AIC is given by</p>
</blockquote>
<p><span class="math display">\[
\text{AIC} = \frac{1}{n} (\text{RSS} + 2 d \hat{\sigma}^2),
\]</span></p>
<blockquote>
<p>where, for simplicity, we have omitted irrelevant constants. Hence for least
squares models, <span class="math inline">\(C_p\)</span> and AIC are proportional to each other, and so only
<span class="math inline">\(C_p\)</span> is displayed in Figure 6.2.</p>
</blockquote>
<p>BIC is derived from a Bayesian point of view, and looks similar to the AIC/<span class="math inline">\(C_p\)</span>:</p>
<p><span class="math display">\[
\text{BIC} = \frac{1}{n} (\text{RSS} + \log (n) d \hat{\sigma}^2),
\]</span></p>
<p>where irrelevant constants were excluded.
Here, the factor of 2 in the AIC/<span class="math inline">\(C_p\)</span> is replaced with <span class="math inline">\(\log (n)\)</span>.
Since <span class="math inline">\(\log n &gt; 2\)</span> for any <span class="math inline">\(n &gt; 7\)</span>, the BIC statistic generally penalizes models with many variables more heavily.</p>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb492-1"><a href="linear-model-selection-and-regularization.html#cb492-1" aria-hidden="true" tabindex="-1"></a>credit_model_best <span class="ot">&lt;-</span> credit_model_best <span class="sc">%&gt;%</span></span>
<span id="cb492-2"><a href="linear-model-selection-and-regularization.html#cb492-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb492-3"><a href="linear-model-selection-and-regularization.html#cb492-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">BIC =</span> (<span class="dv">1</span> <span class="sc">/</span> <span class="fu">nrow</span>(credit)) <span class="sc">*</span></span>
<span id="cb492-4"><a href="linear-model-selection-and-regularization.html#cb492-4" aria-hidden="true" tabindex="-1"></a>      (RSS <span class="sc">+</span> <span class="fu">log</span>(<span class="fu">nrow</span>(credit)) <span class="sc">*</span> n_preds_adj <span class="sc">*</span> sigma_hat<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb492-5"><a href="linear-model-selection-and-regularization.html#cb492-5" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p>Recall that the usual <span class="math inline">\(R^2\)</span> is defined as 1 - RSS/TSS, where TSS = <span class="math inline">\(\sum (y_i - \bar{y})^2\)</span> is the total sum of squares for the response.
The adjusted <span class="math inline">\(R^2\)</span> statistic is calculated as</p>
<p><span class="math display">\[
\text{Adjusted } R^2 = 1 - \frac{\text{RSS}/(n - d - 1)}{\text{TSS}/(n - 1)}
\]</span></p>
<p>Unlike <span class="math inline">\(C_p\)</span>, AIC, and BIC, for which a smaller value indicates a lower test error, a larger value of adjusted <span class="math inline">\(R^2\)</span> indicates smaller test error.</p>
<blockquote>
<p>The intuition behind the adjusted <span class="math inline">\(R^2\)</span> is that once all of the correct
variables have been included in the model, adding additional noise variables
will lead to only a very small decrease in RSS. Since adding noise variables
leads to an increase in <span class="math inline">\(d\)</span>, such variables will lead to an increase in <span class="math inline">\(\text{RSS}/(n−d−1)\)</span>,
and consequently a decrease in the adjusted <span class="math inline">\(R^2\)</span>. Therefore, in theory, the
model with the largest adjusted <span class="math inline">\(R^2\)</span> will have only correct variables and
no noise variables. Unlike the <span class="math inline">\(R^2\)</span> statistic, the adjusted <span class="math inline">\(R^2\)</span> statistic pays
a price for the inclusion of unnecessary variables in the model.</p>
</blockquote>
<p>A model’s <span class="math inline">\(R^2\)</span> value can be obtained directly from the <code>lm</code> object, so I don’t need to manually compute it:</p>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="linear-model-selection-and-regularization.html#cb493-1" aria-hidden="true" tabindex="-1"></a>credit_model_best <span class="ot">&lt;-</span> credit_model_best <span class="sc">%&gt;%</span></span>
<span id="cb493-2"><a href="linear-model-selection-and-regularization.html#cb493-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb493-3"><a href="linear-model-selection-and-regularization.html#cb493-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">Adjusted R2</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">map_dbl</span>(model_fit, <span class="sc">~</span> <span class="fu">summary</span>(.x)<span class="sc">$</span>adj.r.squared)</span>
<span id="cb493-4"><a href="linear-model-selection-and-regularization.html#cb493-4" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p>Figure 6.2:</p>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb494-1"><a href="linear-model-selection-and-regularization.html#cb494-1" aria-hidden="true" tabindex="-1"></a>credit_model_best <span class="sc">%&gt;%</span></span>
<span id="cb494-2"><a href="linear-model-selection-and-regularization.html#cb494-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">c</span>(<span class="st">&quot;C_p&quot;</span>, <span class="st">&quot;BIC&quot;</span>, <span class="st">&quot;Adjusted R2&quot;</span>),</span>
<span id="cb494-3"><a href="linear-model-selection-and-regularization.html#cb494-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">names_to =</span> <span class="st">&quot;metric&quot;</span>, <span class="at">values_to =</span> <span class="st">&quot;value&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb494-4"><a href="linear-model-selection-and-regularization.html#cb494-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(metric) <span class="sc">%&gt;%</span></span>
<span id="cb494-5"><a href="linear-model-selection-and-regularization.html#cb494-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb494-6"><a href="linear-model-selection-and-regularization.html#cb494-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">best_model =</span> <span class="fu">ifelse</span>(metric <span class="sc">==</span> <span class="st">&quot;Adjusted R2&quot;</span>,</span>
<span id="cb494-7"><a href="linear-model-selection-and-regularization.html#cb494-7" aria-hidden="true" tabindex="-1"></a>                        value <span class="sc">==</span> <span class="fu">max</span>(value),</span>
<span id="cb494-8"><a href="linear-model-selection-and-regularization.html#cb494-8" aria-hidden="true" tabindex="-1"></a>                        value <span class="sc">==</span> <span class="fu">min</span>(value))</span>
<span id="cb494-9"><a href="linear-model-selection-and-regularization.html#cb494-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb494-10"><a href="linear-model-selection-and-regularization.html#cb494-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb494-11"><a href="linear-model-selection-and-regularization.html#cb494-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">metric =</span> <span class="fu">factor</span>(metric, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">&quot;C_p&quot;</span>, <span class="st">&quot;BIC&quot;</span>, <span class="st">&quot;Adjusted R2&quot;</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb494-12"><a href="linear-model-selection-and-regularization.html#cb494-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(n_preds_adj <span class="sc">&gt;=</span> <span class="dv">2</span>) <span class="sc">%&gt;%</span></span>
<span id="cb494-13"><a href="linear-model-selection-and-regularization.html#cb494-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> n_preds_adj, <span class="at">y =</span> value)) <span class="sc">+</span></span>
<span id="cb494-14"><a href="linear-model-selection-and-regularization.html#cb494-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> td_colors<span class="sc">$</span>nice<span class="sc">$</span>spanish_blue) <span class="sc">+</span></span>
<span id="cb494-15"><a href="linear-model-selection-and-regularization.html#cb494-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> td_colors<span class="sc">$</span>nice<span class="sc">$</span>opera_mauve) <span class="sc">+</span></span>
<span id="cb494-16"><a href="linear-model-selection-and-regularization.html#cb494-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> . <span class="sc">%&gt;%</span> <span class="fu">filter</span>(best_model), <span class="at">size =</span> <span class="dv">5</span>, <span class="at">shape =</span> <span class="dv">4</span>,</span>
<span id="cb494-17"><a href="linear-model-selection-and-regularization.html#cb494-17" aria-hidden="true" tabindex="-1"></a>             <span class="at">color =</span> td_colors<span class="sc">$</span>nice<span class="sc">$</span>spanish_blue) <span class="sc">+</span></span>
<span id="cb494-18"><a href="linear-model-selection-and-regularization.html#cb494-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> metric, <span class="at">nrow =</span> <span class="dv">1</span>, <span class="at">scales =</span> <span class="st">&quot;free_y&quot;</span>) <span class="sc">+</span></span>
<span id="cb494-19"><a href="linear-model-selection-and-regularization.html#cb494-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="st">&quot;Number of predictors&quot;</span>, <span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">2</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/figure6.2-1.png" width="576" style="display: block; margin: auto;" /></p>
<blockquote>
<p><span class="math inline">\(C_p\)</span>, AIC, and BIC all have rigorous theoretical justifications that are
beyond the scope of this book. These justifications rely on asymptotic arguments
(scenarios where the sample size <span class="math inline">\(n\)</span> is very large). Despite its popularity,
and even though it is quite intuitive, the adjusted <span class="math inline">\(R^2\)</span> is not as well
motivated in statistical theory as AIC, BIC, and <span class="math inline">\(C_p\)</span>. All of these measures
are simple to use and compute. Here we have presented their formulas in
the case of a linear model fit using least squares; however, AIC and BIC
can also be defined for more general types of models.</p>
</blockquote>
</div>
<div id="validation-and-cross-validation" class="section level4 unnumbered">
<h4>Validation and Cross-validation</h4>
<p>Validation and cross-validation from Chapter 5 provide an advantage over AIC, BIC, <span class="math inline">\(C_p\)</span> and adjusted <span class="math inline">\(R^2\)</span>, in that they provide a direct estimate of the test error, and make fewer assumptions about the true underlying model.
It can also be used in a wider ranger of model selection tasks, including scenarios where the model degrees of freedom (e.g. the number of predictors) or error variance <span class="math inline">\(\sigma^2\)</span> are hard to estimate.</p>
<p>To re-create Figure 6.3, I’ll use the <code>tidymodels</code> approach with <code>rsample</code> to make the validation set and cross-validation splits:</p>
<div class="sourceCode" id="cb495"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb495-1"><a href="linear-model-selection-and-regularization.html#cb495-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">499</span>)</span>
<span id="cb495-2"><a href="linear-model-selection-and-regularization.html#cb495-2" aria-hidden="true" tabindex="-1"></a><span class="fu">tic</span>()</span>
<span id="cb495-3"><a href="linear-model-selection-and-regularization.html#cb495-3" aria-hidden="true" tabindex="-1"></a>credit_model_best <span class="ot">&lt;-</span> credit_model_best <span class="sc">%&gt;%</span></span>
<span id="cb495-4"><a href="linear-model-selection-and-regularization.html#cb495-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb495-5"><a href="linear-model-selection-and-regularization.html#cb495-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">validation_set_error =</span> <span class="fu">map</span>(</span>
<span id="cb495-6"><a href="linear-model-selection-and-regularization.html#cb495-6" aria-hidden="true" tabindex="-1"></a>      model_formula,</span>
<span id="cb495-7"><a href="linear-model-selection-and-regularization.html#cb495-7" aria-hidden="true" tabindex="-1"></a>      <span class="cf">function</span>(model_formula) {</span>
<span id="cb495-8"><a href="linear-model-selection-and-regularization.html#cb495-8" aria-hidden="true" tabindex="-1"></a>        <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb495-9"><a href="linear-model-selection-and-regularization.html#cb495-9" aria-hidden="true" tabindex="-1"></a>          <span class="fu">add_model</span>(<span class="fu">linear_reg</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb495-10"><a href="linear-model-selection-and-regularization.html#cb495-10" aria-hidden="true" tabindex="-1"></a>          <span class="fu">add_recipe</span>(<span class="fu">recipe</span>(model_formula, credit)) <span class="sc">%&gt;%</span></span>
<span id="cb495-11"><a href="linear-model-selection-and-regularization.html#cb495-11" aria-hidden="true" tabindex="-1"></a>          <span class="fu">fit_resamples</span>(<span class="fu">validation_split</span>(credit, <span class="at">prop =</span> <span class="fl">0.75</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb495-12"><a href="linear-model-selection-and-regularization.html#cb495-12" aria-hidden="true" tabindex="-1"></a>          <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb495-13"><a href="linear-model-selection-and-regularization.html#cb495-13" aria-hidden="true" tabindex="-1"></a>          <span class="fu">filter</span>(.metric <span class="sc">==</span> <span class="st">&quot;rmse&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb495-14"><a href="linear-model-selection-and-regularization.html#cb495-14" aria-hidden="true" tabindex="-1"></a>          <span class="fu">select</span>(<span class="st">`</span><span class="at">Validation set error</span><span class="st">`</span> <span class="ot">=</span> mean, <span class="at">validation_std_err =</span> std_err)</span>
<span id="cb495-15"><a href="linear-model-selection-and-regularization.html#cb495-15" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb495-16"><a href="linear-model-selection-and-regularization.html#cb495-16" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb495-17"><a href="linear-model-selection-and-regularization.html#cb495-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">cross_validation_error =</span> <span class="fu">map</span>(</span>
<span id="cb495-18"><a href="linear-model-selection-and-regularization.html#cb495-18" aria-hidden="true" tabindex="-1"></a>      model_formula,</span>
<span id="cb495-19"><a href="linear-model-selection-and-regularization.html#cb495-19" aria-hidden="true" tabindex="-1"></a>      <span class="cf">function</span>(model_formula) {</span>
<span id="cb495-20"><a href="linear-model-selection-and-regularization.html#cb495-20" aria-hidden="true" tabindex="-1"></a>        <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb495-21"><a href="linear-model-selection-and-regularization.html#cb495-21" aria-hidden="true" tabindex="-1"></a>          <span class="fu">add_model</span>(<span class="fu">linear_reg</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb495-22"><a href="linear-model-selection-and-regularization.html#cb495-22" aria-hidden="true" tabindex="-1"></a>          <span class="fu">add_recipe</span>(<span class="fu">recipe</span>(model_formula, credit)) <span class="sc">%&gt;%</span></span>
<span id="cb495-23"><a href="linear-model-selection-and-regularization.html#cb495-23" aria-hidden="true" tabindex="-1"></a>          <span class="fu">fit_resamples</span>(<span class="fu">vfold_cv</span>(credit, <span class="at">v =</span> <span class="dv">10</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb495-24"><a href="linear-model-selection-and-regularization.html#cb495-24" aria-hidden="true" tabindex="-1"></a>          <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb495-25"><a href="linear-model-selection-and-regularization.html#cb495-25" aria-hidden="true" tabindex="-1"></a>          <span class="fu">filter</span>(.metric <span class="sc">==</span> <span class="st">&quot;rmse&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb495-26"><a href="linear-model-selection-and-regularization.html#cb495-26" aria-hidden="true" tabindex="-1"></a>          <span class="fu">select</span>(<span class="st">`</span><span class="at">Cross-validation error</span><span class="st">`</span> <span class="ot">=</span> mean, <span class="at">cv_std_err =</span> std_err)</span>
<span id="cb495-27"><a href="linear-model-selection-and-regularization.html#cb495-27" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb495-28"><a href="linear-model-selection-and-regularization.html#cb495-28" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb495-29"><a href="linear-model-selection-and-regularization.html#cb495-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">Square root of BIC</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">sqrt</span>(BIC)</span>
<span id="cb495-30"><a href="linear-model-selection-and-regularization.html#cb495-30" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb495-31"><a href="linear-model-selection-and-regularization.html#cb495-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(<span class="fu">c</span>(validation_set_error, cross_validation_error))</span>
<span id="cb495-32"><a href="linear-model-selection-and-regularization.html#cb495-32" aria-hidden="true" tabindex="-1"></a><span class="fu">toc</span>()</span></code></pre></div>
<pre><code>## 13.95 sec elapsed</code></pre>
<div class="sourceCode" id="cb497"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb497-1"><a href="linear-model-selection-and-regularization.html#cb497-1" aria-hidden="true" tabindex="-1"></a>credit_model_best <span class="sc">%&gt;%</span></span>
<span id="cb497-2"><a href="linear-model-selection-and-regularization.html#cb497-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">c</span>(<span class="st">&quot;Validation set error&quot;</span>, <span class="st">&quot;Cross-validation error&quot;</span>,</span>
<span id="cb497-3"><a href="linear-model-selection-and-regularization.html#cb497-3" aria-hidden="true" tabindex="-1"></a>                        <span class="st">&quot;Square root of BIC&quot;</span>),</span>
<span id="cb497-4"><a href="linear-model-selection-and-regularization.html#cb497-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">names_to =</span> <span class="st">&quot;metric&quot;</span>, <span class="at">values_to =</span> <span class="st">&quot;value&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb497-5"><a href="linear-model-selection-and-regularization.html#cb497-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(metric) <span class="sc">%&gt;%</span></span>
<span id="cb497-6"><a href="linear-model-selection-and-regularization.html#cb497-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">best_model =</span> value <span class="sc">==</span> <span class="fu">min</span>(value)) <span class="sc">%&gt;%</span></span>
<span id="cb497-7"><a href="linear-model-selection-and-regularization.html#cb497-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb497-8"><a href="linear-model-selection-and-regularization.html#cb497-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb497-9"><a href="linear-model-selection-and-regularization.html#cb497-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">metric =</span> <span class="fu">factor</span>(metric,</span>
<span id="cb497-10"><a href="linear-model-selection-and-regularization.html#cb497-10" aria-hidden="true" tabindex="-1"></a>                    <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">&quot;Square root of BIC&quot;</span>, <span class="st">&quot;Validation set error&quot;</span>,</span>
<span id="cb497-11"><a href="linear-model-selection-and-regularization.html#cb497-11" aria-hidden="true" tabindex="-1"></a>                               <span class="st">&quot;Cross-validation error&quot;</span>))</span>
<span id="cb497-12"><a href="linear-model-selection-and-regularization.html#cb497-12" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb497-13"><a href="linear-model-selection-and-regularization.html#cb497-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> n_preds_adj, <span class="at">y =</span> value)) <span class="sc">+</span></span>
<span id="cb497-14"><a href="linear-model-selection-and-regularization.html#cb497-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> td_colors<span class="sc">$</span>nice<span class="sc">$</span>spanish_blue) <span class="sc">+</span></span>
<span id="cb497-15"><a href="linear-model-selection-and-regularization.html#cb497-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> td_colors<span class="sc">$</span>nice<span class="sc">$</span>opera_mauve) <span class="sc">+</span></span>
<span id="cb497-16"><a href="linear-model-selection-and-regularization.html#cb497-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> . <span class="sc">%&gt;%</span> <span class="fu">filter</span>(best_model), <span class="at">size =</span> <span class="dv">5</span>, <span class="at">shape =</span> <span class="dv">4</span>,</span>
<span id="cb497-17"><a href="linear-model-selection-and-regularization.html#cb497-17" aria-hidden="true" tabindex="-1"></a>             <span class="at">color =</span> td_colors<span class="sc">$</span>nice<span class="sc">$</span>spanish_blue) <span class="sc">+</span></span>
<span id="cb497-18"><a href="linear-model-selection-and-regularization.html#cb497-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> metric, <span class="at">nrow =</span> <span class="dv">1</span>, <span class="at">scales =</span> <span class="st">&quot;free_y&quot;</span>) <span class="sc">+</span></span>
<span id="cb497-19"><a href="linear-model-selection-and-regularization.html#cb497-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="st">&quot;Number of predictors&quot;</span>, <span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">2</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/figure6.3-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Because the randomness associated with splitting the data in the validation set and cross-validation approaches, we will likely find a different best model with different splits.
In this case, we can select a model using the <em>one-standard-error rule</em>, where we calculate the standard error of the test MSE for each model, and then select the smallest model for which the estimated test error is within one standard error of the lowest test error.</p>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="linear-model-selection-and-regularization.html#cb498-1" aria-hidden="true" tabindex="-1"></a>credit_model_best <span class="sc">%&gt;%</span></span>
<span id="cb498-2"><a href="linear-model-selection-and-regularization.html#cb498-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transmute</span>(</span>
<span id="cb498-3"><a href="linear-model-selection-and-regularization.html#cb498-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at"># predictors</span><span class="st">`</span> <span class="ot">=</span> n_preds_adj, <span class="st">`</span><span class="at">Cross-validation error</span><span class="st">`</span>, cv_std_err,</span>
<span id="cb498-4"><a href="linear-model-selection-and-regularization.html#cb498-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">lowest_error =</span> <span class="fu">min</span>(<span class="st">`</span><span class="at">Cross-validation error</span><span class="st">`</span>),</span>
<span id="cb498-5"><a href="linear-model-selection-and-regularization.html#cb498-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">lowest_std_error =</span> cv_std_err[<span class="st">`</span><span class="at">Cross-validation error</span><span class="st">`</span> <span class="sc">==</span> lowest_error],</span>
<span id="cb498-6"><a href="linear-model-selection-and-regularization.html#cb498-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The best model is the minimum `n_preds_adj` (number of predictors) for</span></span>
<span id="cb498-7"><a href="linear-model-selection-and-regularization.html#cb498-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># which the CV test error is within the standard error of the lowest error</span></span>
<span id="cb498-8"><a href="linear-model-selection-and-regularization.html#cb498-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">best_model =</span> n_preds_adj <span class="sc">==</span> <span class="fu">min</span>(n_preds_adj[<span class="st">`</span><span class="at">Cross-validation error</span><span class="st">`</span> <span class="sc">&lt;</span> lowest_error <span class="sc">+</span> lowest_std_error])</span>
<span id="cb498-9"><a href="linear-model-selection-and-regularization.html#cb498-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb498-10"><a href="linear-model-selection-and-regularization.html#cb498-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gt</span>() <span class="sc">%&gt;%</span></span>
<span id="cb498-11"><a href="linear-model-selection-and-regularization.html#cb498-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tab_style</span>(<span class="at">style =</span> <span class="fu">cell_text</span>(<span class="at">weight =</span> <span class="st">&quot;bold&quot;</span>),</span>
<span id="cb498-12"><a href="linear-model-selection-and-regularization.html#cb498-12" aria-hidden="true" tabindex="-1"></a>            <span class="at">locations =</span> <span class="fu">cells_body</span>(<span class="at">rows =</span> best_model))</span></code></pre></div>
<div id="nzusayavvr" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#nzusayavvr .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#nzusayavvr .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#nzusayavvr .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#nzusayavvr .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 6px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#nzusayavvr .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nzusayavvr .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#nzusayavvr .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#nzusayavvr .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#nzusayavvr .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#nzusayavvr .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#nzusayavvr .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#nzusayavvr .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#nzusayavvr .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#nzusayavvr .gt_from_md > :first-child {
  margin-top: 0;
}

#nzusayavvr .gt_from_md > :last-child {
  margin-bottom: 0;
}

#nzusayavvr .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#nzusayavvr .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#nzusayavvr .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#nzusayavvr .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#nzusayavvr .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#nzusayavvr .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#nzusayavvr .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#nzusayavvr .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nzusayavvr .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#nzusayavvr .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#nzusayavvr .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#nzusayavvr .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#nzusayavvr .gt_left {
  text-align: left;
}

#nzusayavvr .gt_center {
  text-align: center;
}

#nzusayavvr .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#nzusayavvr .gt_font_normal {
  font-weight: normal;
}

#nzusayavvr .gt_font_bold {
  font-weight: bold;
}

#nzusayavvr .gt_font_italic {
  font-style: italic;
}

#nzusayavvr .gt_super {
  font-size: 65%;
}

#nzusayavvr .gt_footnote_marks {
  font-style: italic;
  font-weight: normal;
  font-size: 65%;
}
</style>
<table class="gt_table">
  
  <thead class="gt_col_headings">
    <tr>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1"># predictors</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">Cross-validation error</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">cv_std_err</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">lowest_error</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">lowest_std_error</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">best_model</th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td class="gt_row gt_right">1</td>
<td class="gt_row gt_right">230.43401</td>
<td class="gt_row gt_right">12.663270</td>
<td class="gt_row gt_right">98.69851</td>
<td class="gt_row gt_right">3.405358</td>
<td class="gt_row gt_center">FALSE</td></tr>
    <tr><td class="gt_row gt_right">2</td>
<td class="gt_row gt_right">161.43387</td>
<td class="gt_row gt_right">8.886096</td>
<td class="gt_row gt_right">98.69851</td>
<td class="gt_row gt_right">3.405358</td>
<td class="gt_row gt_center">FALSE</td></tr>
    <tr><td class="gt_row gt_right">3</td>
<td class="gt_row gt_right">103.70715</td>
<td class="gt_row gt_right">2.457797</td>
<td class="gt_row gt_right">98.69851</td>
<td class="gt_row gt_right">3.405358</td>
<td class="gt_row gt_center">FALSE</td></tr>
    <tr><td class="gt_row gt_right" style="font-weight: bold;">4</td>
<td class="gt_row gt_right" style="font-weight: bold;">99.24797</td>
<td class="gt_row gt_right" style="font-weight: bold;">4.544705</td>
<td class="gt_row gt_right" style="font-weight: bold;">98.69851</td>
<td class="gt_row gt_right" style="font-weight: bold;">3.405358</td>
<td class="gt_row gt_center" style="font-weight: bold;">TRUE</td></tr>
    <tr><td class="gt_row gt_right">5</td>
<td class="gt_row gt_right">99.34556</td>
<td class="gt_row gt_right">3.231063</td>
<td class="gt_row gt_right">98.69851</td>
<td class="gt_row gt_right">3.405358</td>
<td class="gt_row gt_center">FALSE</td></tr>
    <tr><td class="gt_row gt_right">6</td>
<td class="gt_row gt_right">99.01851</td>
<td class="gt_row gt_right">4.147442</td>
<td class="gt_row gt_right">98.69851</td>
<td class="gt_row gt_right">3.405358</td>
<td class="gt_row gt_center">FALSE</td></tr>
    <tr><td class="gt_row gt_right">7</td>
<td class="gt_row gt_right">98.69851</td>
<td class="gt_row gt_right">3.405358</td>
<td class="gt_row gt_right">98.69851</td>
<td class="gt_row gt_right">3.405358</td>
<td class="gt_row gt_center">FALSE</td></tr>
    <tr><td class="gt_row gt_right">8</td>
<td class="gt_row gt_right">99.65805</td>
<td class="gt_row gt_right">2.937664</td>
<td class="gt_row gt_right">98.69851</td>
<td class="gt_row gt_right">3.405358</td>
<td class="gt_row gt_center">FALSE</td></tr>
    <tr><td class="gt_row gt_right">9</td>
<td class="gt_row gt_right">99.63657</td>
<td class="gt_row gt_right">4.109147</td>
<td class="gt_row gt_right">98.69851</td>
<td class="gt_row gt_right">3.405358</td>
<td class="gt_row gt_center">FALSE</td></tr>
    <tr><td class="gt_row gt_right">10</td>
<td class="gt_row gt_right">99.67476</td>
<td class="gt_row gt_right">3.288357</td>
<td class="gt_row gt_right">98.69851</td>
<td class="gt_row gt_right">3.405358</td>
<td class="gt_row gt_center">FALSE</td></tr>
    <tr><td class="gt_row gt_right">11</td>
<td class="gt_row gt_right">99.95464</td>
<td class="gt_row gt_right">3.082506</td>
<td class="gt_row gt_right">98.69851</td>
<td class="gt_row gt_right">3.405358</td>
<td class="gt_row gt_center">FALSE</td></tr>
  </tbody>
  
  
</table>
</div>
<blockquote>
<p>The rationale here is that
if a set of models appear to be more or less equally good, then we might
as well choose the simplest model—that is, the model with the smallest
number of predictors. In this case, applying the one-standard-error rule
to the validation set or cross-validation approach leads to selection of the
three-variable model.</p>
</blockquote>
<p>Here, the four-variable model was selected, not the three-variable model like in the text, but I would chalk that up to the random sampling.</p>
</div>
</div>
</div>
<div id="shrinkage-methods" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Shrinkage Methods</h2>
<p>As already discussed, subset selection has a lot of issues.
A much better alternative is use a technique that <em>contrains</em> or <em>regularizes</em> or <em>shrinks</em> the coefficient estimates towards zero in a model fit with all <span class="math inline">\(p\)</span> predictors.
It turns out that shrinking the coefficient estimates can significantly reduce the variance.</p>
<div id="ridge-regression" class="section level3" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Ridge Regression</h3>
<p>Recall from Chapter 3 that the least squares fitting procedure involves estimating <span class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span> by minimizing the RSS:</p>
<p><span class="math display">\[
\text{RSS} = \sum_{i=1}^n \left(y_i - \hat{y}_i \right)^2 = \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2.
\]</span></p>
<p><em>Ridge regression</em> is very similar to least squares, except the quantity minimized is slightly different:</p>
<p><span class="math display">\[
\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^p \beta_j^2 = \text{RSS} + \lambda \sum_{j=1}^p \beta_j^2,
\]</span></p>
<p>where <span class="math inline">\(\lambda \geq 0\)</span> is a <em>tuning parameter</em>, to be determined separately.
The second term above, called a <em>shrinkage penalty</em> is small when the coefficients are close to zero, and so has the effect of shrinking the estimates <span class="math inline">\(\beta_j\)</span> towards zero.
The tuning parameter <span class="math inline">\(\lambda\)</span> serves to control the relative impact of these two terms on the regression coefficient estimates.
When <span class="math inline">\(\lambda = 0\)</span>, the penalty term has no effect, and ridge regression will produce the same least squares estimates.
Unlike least squares, which generates only one set of coefficient estimates (the “best fit”), ridge regression will produce a different set of coefficient estimates <span class="math inline">\(\hat{\beta}_{\lambda}^R\)</span> for each value <span class="math inline">\(\lambda\)</span>.
Selecting a good value for <span class="math inline">\(\lambda\)</span> is critical.</p>
<p>Note that the shrinkage penalty is not applied to the intercept <span class="math inline">\(\beta_0\)</span>, which is simply a measure of the mean value of the response when all predictors are zero (<span class="math inline">\(x_{i1} = x_{i2} = \dots = 0\)</span>)</p>
<div id="an-application-to-the-credit-data" class="section level4 unnumbered">
<h4>An Application to the Credit Data</h4>
<p>In <code>tidymodels</code>, regularized least squares is done with the <code>glmnet</code> engine.
(See <a href="https://www.tidymodels.org/find/parsnip/">this article for a list of models available in <code>parnsip</code></a> and <a href="https://parsnip.tidymodels.org/reference/details_linear_reg_glmnet.html">this article for examples using <code>glmnet</code></a>.)</p>
<p>Specify the model:</p>
<div class="sourceCode" id="cb499"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb499-1"><a href="linear-model-selection-and-regularization.html#cb499-1" aria-hidden="true" tabindex="-1"></a>ridge_spec <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>(<span class="at">penalty =</span> <span class="dv">0</span>, <span class="at">mixture =</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span></span>
<span id="cb499-2"><a href="linear-model-selection-and-regularization.html#cb499-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>)</span>
<span id="cb499-3"><a href="linear-model-selection-and-regularization.html#cb499-3" aria-hidden="true" tabindex="-1"></a><span class="co"># The `parnship::translate()` function is a helpful way to &quot;decode&quot; a model spec</span></span>
<span id="cb499-4"><a href="linear-model-selection-and-regularization.html#cb499-4" aria-hidden="true" tabindex="-1"></a>ridge_spec <span class="sc">%&gt;%</span> <span class="fu">translate</span>()</span></code></pre></div>
<pre><code>## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = 0
##   mixture = 0
## 
## Computational engine: glmnet 
## 
## Model fit template:
## glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     alpha = 0, family = &quot;gaussian&quot;)</code></pre>
<p>The <code>penalty</code> argument above refers to the <span class="math inline">\(\lambda\)</span> tuning parameter.
The <code>mixture</code> variable ranges from 0 to 1, with 0 corresponding to ridge regression, 1 corresponding to lasso regression, and values between using a mixture of both.</p>
<p>Fit <code>Balance</code> to all the predictors:</p>
<div class="sourceCode" id="cb501"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb501-1"><a href="linear-model-selection-and-regularization.html#cb501-1" aria-hidden="true" tabindex="-1"></a>credit_ridge_fit <span class="ot">&lt;-</span> <span class="fu">fit</span>(ridge_spec, Balance <span class="sc">~</span> ., <span class="at">data =</span> credit)</span>
<span id="cb501-2"><a href="linear-model-selection-and-regularization.html#cb501-2" aria-hidden="true" tabindex="-1"></a><span class="fu">tidy</span>(credit_ridge_fit)</span></code></pre></div>
<pre><code>## # A tibble: 12 x 3
##    term        estimate penalty
##    &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;
##  1 (Intercept) -401.          0
##  2 Income        -5.18        0
##  3 Limit          0.114       0
##  4 Rating         1.66        0
##  5 Cards         15.8         0
##  6 Age           -0.957       0
##  7 Education     -0.474       0
##  8 OwnYes        -4.86        0
##  9 StudentYes   382.          0
## 10 MarriedYes   -12.1         0
## 11 RegionSouth    9.11        0
## 12 RegionWest    13.1         0</code></pre>
<p>Because our <code>ridge_spec</code> had <code>penalty = 0</code>, the coefficients here correspond to no penalty, but the <code>glmnet::glmnet()</code> function fits a range of <code>penalty</code> values all at once, which we can extract with <code>broom::tidy</code> like so:</p>
<div class="sourceCode" id="cb503"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb503-1"><a href="linear-model-selection-and-regularization.html#cb503-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tidy</span>(credit_ridge_fit, <span class="at">penalty =</span> <span class="dv">100</span>)</span></code></pre></div>
<pre><code>## # A tibble: 12 x 3
##    term         estimate penalty
##    &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;
##  1 (Intercept) -307.         100
##  2 Income        -3.04       100
##  3 Limit          0.0951     100
##  4 Rating         1.40       100
##  5 Cards         16.4        100
##  6 Age           -1.10       100
##  7 Education     -0.178      100
##  8 OwnYes        -0.341      100
##  9 StudentYes   335.         100
## 10 MarriedYes   -12.4        100
## 11 RegionSouth    7.45       100
## 12 RegionWest     8.79       100</code></pre>
<p>To re-create Figure 6.4, I’ll first re-fit the data on <em>standardized data</em> (continuous variables re-scaled to have mean of 0, standard deviation of 1):</p>
<div class="sourceCode" id="cb505"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb505-1"><a href="linear-model-selection-and-regularization.html#cb505-1" aria-hidden="true" tabindex="-1"></a>credit_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(Balance <span class="sc">~</span> ., <span class="at">data =</span> credit) <span class="sc">%&gt;%</span></span>
<span id="cb505-2"><a href="linear-model-selection-and-regularization.html#cb505-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_dummy</span>(<span class="fu">all_nominal_predictors</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb505-3"><a href="linear-model-selection-and-regularization.html#cb505-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_predictors</span>())</span>
<span id="cb505-4"><a href="linear-model-selection-and-regularization.html#cb505-4" aria-hidden="true" tabindex="-1"></a>credit_ridge_workflow <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb505-5"><a href="linear-model-selection-and-regularization.html#cb505-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(credit_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb505-6"><a href="linear-model-selection-and-regularization.html#cb505-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(ridge_spec)</span>
<span id="cb505-7"><a href="linear-model-selection-and-regularization.html#cb505-7" aria-hidden="true" tabindex="-1"></a>credit_ridge_fit <span class="ot">&lt;-</span> <span class="fu">fit</span>(credit_ridge_workflow, <span class="at">data =</span> credit)</span></code></pre></div>
<p>Then I’ll compile coefficient estimates for a wide range of <span class="math inline">\(\lambda\)</span> values with <code>purrr::map_dfr()</code>:</p>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="linear-model-selection-and-regularization.html#cb506-1" aria-hidden="true" tabindex="-1"></a><span class="fu">map_dfr</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">5</span>, <span class="fl">0.1</span>),</span>
<span id="cb506-2"><a href="linear-model-selection-and-regularization.html#cb506-2" aria-hidden="true" tabindex="-1"></a>        <span class="sc">~</span> <span class="fu">tidy</span>(credit_ridge_fit, <span class="at">penalty =</span> <span class="dv">10</span><span class="sc">^</span>.x)) <span class="sc">%&gt;%</span></span>
<span id="cb506-3"><a href="linear-model-selection-and-regularization.html#cb506-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(term <span class="sc">!=</span> <span class="st">&quot;(Intercept)&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb506-4"><a href="linear-model-selection-and-regularization.html#cb506-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb506-5"><a href="linear-model-selection-and-regularization.html#cb506-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">term_highlight =</span> <span class="fu">fct_other</span>(</span>
<span id="cb506-6"><a href="linear-model-selection-and-regularization.html#cb506-6" aria-hidden="true" tabindex="-1"></a>      term, <span class="at">keep =</span> <span class="fu">c</span>(<span class="st">&quot;Income&quot;</span>, <span class="st">&quot;Limit&quot;</span>, <span class="st">&quot;Rating&quot;</span>, <span class="st">&quot;Student_Yes&quot;</span>)</span>
<span id="cb506-7"><a href="linear-model-selection-and-regularization.html#cb506-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb506-8"><a href="linear-model-selection-and-regularization.html#cb506-8" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb506-9"><a href="linear-model-selection-and-regularization.html#cb506-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> penalty, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb506-10"><a href="linear-model-selection-and-regularization.html#cb506-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> term, <span class="at">color =</span> term_highlight), <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb506-11"><a href="linear-model-selection-and-regularization.html#cb506-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>(<span class="at">breaks =</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">4</span>)) <span class="sc">+</span></span>
<span id="cb506-12"><a href="linear-model-selection-and-regularization.html#cb506-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">40</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb506-13"><a href="linear-model-selection-and-regularization.html#cb506-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(lambda), <span class="at">y =</span> <span class="st">&quot;Standardized coefficients&quot;</span>, <span class="at">color =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb506-14"><a href="linear-model-selection-and-regularization.html#cb506-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(td_colors<span class="sc">$</span>pastel6[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="st">&quot;grey80&quot;</span>)) <span class="sc">+</span></span>
<span id="cb506-15"><a href="linear-model-selection-and-regularization.html#cb506-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="fu">c</span>(<span class="fl">0.8</span>, <span class="fl">0.8</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/figure6.4_incorrect-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>From this figure, it is clear that the fitting procedure is truncated at a penalty values of <span class="math inline">\(\lambda\)</span> = 40 (the vertical line above).
This has to do with the <em>regularization path</em> chosen by <code>glmnet</code>.
We can see from the grid of values for <span class="math inline">\(\lambda\)</span> that the minimum value is 40:</p>
<div class="sourceCode" id="cb507"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb507-1"><a href="linear-model-selection-and-regularization.html#cb507-1" aria-hidden="true" tabindex="-1"></a><span class="fu">extract_fit_engine</span>(credit_ridge_fit)<span class="sc">$</span>lambda</span></code></pre></div>
<pre><code>##   [1] 396562.69957 361333.16232 329233.32005 299985.13930 273335.28632
##   [6] 249052.93283 226927.75669 206768.12023 188399.41030 171662.52594
##  [11] 156412.50026 142517.24483 129856.40559 118320.32042 107809.06926
##  [16]  98231.60868  89504.98330  81553.60727  74308.60957  67707.23750
##  [21]  61692.31313  56211.73806  51218.04218  46667.97247  42522.11842
##  [26]  38744.57062  35302.60975  32166.42320  29308.84681  26705.12964
##  [31]  24332.71953  22171.06779  20201.45123  18406.80998  16771.59971
##  [36]  15281.65702  13924.07673  12687.10013  11560.01312  10533.05342
##  [41]   9597.32598   8744.72599   7967.86864   7260.02515   6615.06452
##  [46]   6027.40042   5491.94278   5004.05372   4559.50738   4154.45331
##  [51]   3785.38313   3449.10012   3142.69158   2863.50352   2609.11776
##  [56]   2377.33093   2166.13540   1973.70190   1798.36366   1638.60199
##  [61]   1493.03311   1360.39616   1239.54232   1129.42479   1029.08981
##  [66]    937.66830    854.36844    778.46870    709.31169    646.29839
##  [71]    588.88302    536.56828    488.90103    445.46841    405.89423
##  [76]    369.83570    336.98052    307.04410    279.76714    254.91340
##  [81]    232.26760    211.63359    192.83264    175.70192    160.09305
##  [86]    145.87082    132.91206    121.10452    110.34593    100.54310
##  [91]     91.61113     83.47265     76.05717     69.30046     63.14400
##  [96]     57.53446     52.42326     47.76612     43.52271     39.65627</code></pre>
<p>The regularization path can be set manually using the <code>path_values</code> argument (see <a href="https://parsnip.tidymodels.org/reference/glmnet-details.html">this article for more details</a>):</p>
<div class="sourceCode" id="cb509"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb509-1"><a href="linear-model-selection-and-regularization.html#cb509-1" aria-hidden="true" tabindex="-1"></a>coef_path_values <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">5</span>, <span class="fl">0.1</span>)</span>
<span id="cb509-2"><a href="linear-model-selection-and-regularization.html#cb509-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb509-3"><a href="linear-model-selection-and-regularization.html#cb509-3" aria-hidden="true" tabindex="-1"></a>ridge_spec_path <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>(<span class="at">penalty =</span> <span class="dv">0</span>, <span class="at">mixture =</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span></span>
<span id="cb509-4"><a href="linear-model-selection-and-regularization.html#cb509-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>, <span class="at">path_values =</span> coef_path_values)</span>
<span id="cb509-5"><a href="linear-model-selection-and-regularization.html#cb509-5" aria-hidden="true" tabindex="-1"></a>credit_ridge_workflow_path <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb509-6"><a href="linear-model-selection-and-regularization.html#cb509-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(credit_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb509-7"><a href="linear-model-selection-and-regularization.html#cb509-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(ridge_spec_path)</span>
<span id="cb509-8"><a href="linear-model-selection-and-regularization.html#cb509-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb509-9"><a href="linear-model-selection-and-regularization.html#cb509-9" aria-hidden="true" tabindex="-1"></a>credit_ridge_fit <span class="ot">&lt;-</span> <span class="fu">fit</span>(credit_ridge_workflow_path, <span class="at">data =</span> credit)</span></code></pre></div>
<p>Now with the full range of <span class="math inline">\(\lambda\)</span> values, I can re-create the figure properly:</p>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb510-1"><a href="linear-model-selection-and-regularization.html#cb510-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the l2 norm for the least squares model</span></span>
<span id="cb510-2"><a href="linear-model-selection-and-regularization.html#cb510-2" aria-hidden="true" tabindex="-1"></a>credit_lm_fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Balance <span class="sc">~</span> ., <span class="at">data =</span> credit)</span>
<span id="cb510-3"><a href="linear-model-selection-and-regularization.html#cb510-3" aria-hidden="true" tabindex="-1"></a>credit_lm_fit_l2_norm <span class="ot">&lt;-</span> <span class="fu">sum</span>(credit_lm_fit<span class="sc">$</span>coefficients[<span class="sc">-</span><span class="dv">1</span>]<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb510-4"><a href="linear-model-selection-and-regularization.html#cb510-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb510-5"><a href="linear-model-selection-and-regularization.html#cb510-5" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">map_dfr</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">5</span>, <span class="fl">0.1</span>),</span>
<span id="cb510-6"><a href="linear-model-selection-and-regularization.html#cb510-6" aria-hidden="true" tabindex="-1"></a>        <span class="sc">~</span> <span class="fu">tidy</span>(credit_ridge_fit, <span class="at">penalty =</span> <span class="dv">10</span><span class="sc">^</span>.x)) <span class="sc">%&gt;%</span></span>
<span id="cb510-7"><a href="linear-model-selection-and-regularization.html#cb510-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(term <span class="sc">!=</span> <span class="st">&quot;(Intercept)&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb510-8"><a href="linear-model-selection-and-regularization.html#cb510-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(penalty) <span class="sc">%&gt;%</span></span>
<span id="cb510-9"><a href="linear-model-selection-and-regularization.html#cb510-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">l2_norm =</span> <span class="fu">sum</span>(estimate<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb510-10"><a href="linear-model-selection-and-regularization.html#cb510-10" aria-hidden="true" tabindex="-1"></a>         <span class="at">l2_norm_ratio =</span> l2_norm <span class="sc">/</span> credit_lm_fit_l2_norm) <span class="sc">%&gt;%</span></span>
<span id="cb510-11"><a href="linear-model-selection-and-regularization.html#cb510-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb510-12"><a href="linear-model-selection-and-regularization.html#cb510-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb510-13"><a href="linear-model-selection-and-regularization.html#cb510-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">term_highlight =</span> <span class="fu">fct_other</span>(</span>
<span id="cb510-14"><a href="linear-model-selection-and-regularization.html#cb510-14" aria-hidden="true" tabindex="-1"></a>      term, <span class="at">keep =</span> <span class="fu">c</span>(<span class="st">&quot;Income&quot;</span>, <span class="st">&quot;Limit&quot;</span>, <span class="st">&quot;Rating&quot;</span>, <span class="st">&quot;Student_Yes&quot;</span>)</span>
<span id="cb510-15"><a href="linear-model-selection-and-regularization.html#cb510-15" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb510-16"><a href="linear-model-selection-and-regularization.html#cb510-16" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb510-17"><a href="linear-model-selection-and-regularization.html#cb510-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb510-18"><a href="linear-model-selection-and-regularization.html#cb510-18" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> d <span class="sc">%&gt;%</span></span>
<span id="cb510-19"><a href="linear-model-selection-and-regularization.html#cb510-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> penalty, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb510-20"><a href="linear-model-selection-and-regularization.html#cb510-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> term, <span class="at">color =</span> term_highlight), <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb510-21"><a href="linear-model-selection-and-regularization.html#cb510-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>(<span class="at">breaks =</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">4</span>)) <span class="sc">+</span></span>
<span id="cb510-22"><a href="linear-model-selection-and-regularization.html#cb510-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(lambda), <span class="at">y =</span> <span class="st">&quot;Standardized coefficients&quot;</span>, <span class="at">color =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb510-23"><a href="linear-model-selection-and-regularization.html#cb510-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(td_colors<span class="sc">$</span>pastel6[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="st">&quot;grey80&quot;</span>)) <span class="sc">+</span></span>
<span id="cb510-24"><a href="linear-model-selection-and-regularization.html#cb510-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="fu">c</span>(<span class="fl">0.7</span>, <span class="fl">0.8</span>))</span>
<span id="cb510-25"><a href="linear-model-selection-and-regularization.html#cb510-25" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> d <span class="sc">%&gt;%</span></span>
<span id="cb510-26"><a href="linear-model-selection-and-regularization.html#cb510-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> l2_norm_ratio, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb510-27"><a href="linear-model-selection-and-regularization.html#cb510-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> term, <span class="at">color =</span> term_highlight), <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb510-28"><a href="linear-model-selection-and-regularization.html#cb510-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb510-29"><a href="linear-model-selection-and-regularization.html#cb510-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;||&quot;</span>, <span class="fu">hat</span>(beta[lambda]), <span class="st">&quot;||2 / ||&quot;</span>, <span class="fu">hat</span>(beta), <span class="st">&quot;||2&quot;</span>)),</span>
<span id="cb510-30"><a href="linear-model-selection-and-regularization.html#cb510-30" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="cn">NULL</span>, <span class="at">color =</span> <span class="cn">NULL</span></span>
<span id="cb510-31"><a href="linear-model-selection-and-regularization.html#cb510-31" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb510-32"><a href="linear-model-selection-and-regularization.html#cb510-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(td_colors<span class="sc">$</span>pastel6[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="st">&quot;grey80&quot;</span>)) <span class="sc">+</span></span>
<span id="cb510-33"><a href="linear-model-selection-and-regularization.html#cb510-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>)</span>
<span id="cb510-34"><a href="linear-model-selection-and-regularization.html#cb510-34" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">|</span> p2</span></code></pre></div>
<p><img src="_main_files/figure-html/figure6.4-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>On the subject of standardizing predictors:</p>
<blockquote>
<p>The standard least squares coefficient estimates discussed in Chapter 3
are scale equivariant: multiplying <span class="math inline">\(X_j\)</span> by a constant <span class="math inline">\(c\)</span> simply leads to a
scaling of the least squares coefficient estimates by a factor of <span class="math inline">\(1/c\)</span>. In other
words, regardless of how the <span class="math inline">\(j\)</span>th predictor is scaled, <span class="math inline">\(X_j \hat{\beta}_j\)</span> will remain the
same. In contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant. For instance,
consider the income variable, which is measured in dollars. One could reasonably have measured income in thousands of dollars, which would result
in a reduction in the observed values of income by a factor of 1,000. Now due
to the sum of squared coefficients term in the ridge regression formulation
(6.5), such a change in scale will not simply cause the ridge regression coefficient estimate for income to change by a factor of 1,000. In other words,
<span class="math inline">\(X_j \hat{\beta}^R_{j,λ}\)</span> will depend not only on the value of <span class="math inline">\(\lambda\)</span>, but also on the scaling of the
<span class="math inline">\(j\)</span>th predictor. In fact, the value of <span class="math inline">\(X_j \hat{\beta}^R_{j, \lambda}\)</span>
may even depend on the scaling
of the other predictors! Therefore, it is best to apply ridge regression after
standardizing the predictors, … so that they are all on the same scale.</p>
</blockquote>
</div>
<div id="why-does-ridge-regression-improve-over-least-squares" class="section level4 unnumbered">
<h4>Why Does Ridge Regression Improve Over Least Squares?</h4>
<p>Ridge regression’s advantage over least squares has to do with the <em>bias-variance trade-off</em>.
As <span class="math inline">\(\lambda\)</span> increases, the flexibility of the fit decreases, leading to decreased variance but increased bias.
So ridge regression works best in situations where the least squares estimates have high variance, like when the number of variables <span class="math inline">\(p\)</span> is almost as large as the number of observations <span class="math inline">\(n\)</span> (as in Figure 6.5).</p>
<blockquote>
<p>Ridge regression also has substantial computational advantages over best
subset selection, which requires searching through <span class="math inline">\(2p\)</span> models. As we discussed previously, even for moderate values of <span class="math inline">\(p\)</span>, such a search can be
computationally infeasible. In contrast, for any fixed value of <span class="math inline">\(\lambda\)</span>, ridge regression only fits a single model, and the model-fitting procedure can be
performed quite quickly. In fact, one can show that the computations required to solve (6.5), simultaneously for all values of <span class="math inline">\(\lambda\)</span>, are almost identical to those for fitting a model using least squares.</p>
</blockquote>
</div>
</div>
<div id="the-lasso" class="section level3" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> The Lasso</h3>
<p>Ridge regression will shrink all coefficients towards zero, but will not set them to exactly zero (unless <span class="math inline">\(\lambda = \infty\)</span>).
This doesn’t affect prediction accuracy, but may cause challenges in model interpretation for large numbers of variables <span class="math inline">\(p\)</span>.</p>
<p>The <em>lasso</em> is a relatively recent alternative to ridge regression with a similar formulation as ridge regression.
The coefficients <span class="math inline">\(\hat{\beta}_{\lambda}^L\)</span> minimize the quantity:</p>
<p><span class="math display">\[
\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^p | \beta_j | = \text{RSS} + \lambda \sum_{j=1}^p | \beta_j |.
\]</span></p>
<blockquote>
<p>The only difference is that the <span class="math inline">\(\beta_j^2\)</span> term in the ridge
regression penalty (6.5) has been replaced by <span class="math inline">\(|\beta_j|\)</span> in the lasso penalty (6.7).
In statistical parlance, the lasso uses an <span class="math inline">\(\mathcal{l}_1\)</span> (pronounced “ell 1”) penalty
instead of an <span class="math inline">\(\mathcal{l}_2\)</span> penalty. The <span class="math inline">\(\mathcal{l}_1\)</span> norm of a coefficient vector <span class="math inline">\(\beta\)</span> is given by <span class="math inline">\(||\beta||_1 = \sum |\beta_j|\)</span>.</p>
</blockquote>
<p>The <span class="math inline">\(\mathcal{l}_1\)</span> penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero for sufficiently large <span class="math inline">\(\lambda\)</span>.
Functionally, this is a form of automatic <em>variable selection</em> like the best subset techniques.
We say that the lasso yields <em>sparse</em> models which only involve a subset of the variables.
This makes lasso models much easier to interpret than those produced by ridge regression.</p>
<p>Fit the <code>credit</code> model with lasso regression:</p>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb511-1"><a href="linear-model-selection-and-regularization.html#cb511-1" aria-hidden="true" tabindex="-1"></a>credit_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(Balance <span class="sc">~</span> ., <span class="at">data =</span> credit) <span class="sc">%&gt;%</span></span>
<span id="cb511-2"><a href="linear-model-selection-and-regularization.html#cb511-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_dummy</span>(<span class="fu">all_nominal_predictors</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb511-3"><a href="linear-model-selection-and-regularization.html#cb511-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_predictors</span>())</span></code></pre></div>
<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb512-1"><a href="linear-model-selection-and-regularization.html#cb512-1" aria-hidden="true" tabindex="-1"></a>lasso_spec <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>(<span class="at">penalty =</span> <span class="dv">20</span>, <span class="at">mixture =</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span></span>
<span id="cb512-2"><a href="linear-model-selection-and-regularization.html#cb512-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>)</span>
<span id="cb512-3"><a href="linear-model-selection-and-regularization.html#cb512-3" aria-hidden="true" tabindex="-1"></a>credit_lasso_workflow <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb512-4"><a href="linear-model-selection-and-regularization.html#cb512-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(credit_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb512-5"><a href="linear-model-selection-and-regularization.html#cb512-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(lasso_spec)</span>
<span id="cb512-6"><a href="linear-model-selection-and-regularization.html#cb512-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb512-7"><a href="linear-model-selection-and-regularization.html#cb512-7" aria-hidden="true" tabindex="-1"></a>credit_lasso_fit <span class="ot">&lt;-</span> <span class="fu">fit</span>(credit_lasso_workflow, <span class="at">data =</span> credit)</span></code></pre></div>
<div class="sourceCode" id="cb513"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb513-1"><a href="linear-model-selection-and-regularization.html#cb513-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the l1 norm for the least squares model</span></span>
<span id="cb513-2"><a href="linear-model-selection-and-regularization.html#cb513-2" aria-hidden="true" tabindex="-1"></a>credit_lm_fit_l1_norm <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">abs</span>(credit_lm_fit<span class="sc">$</span>coefficients[<span class="sc">-</span><span class="dv">1</span>]))</span>
<span id="cb513-3"><a href="linear-model-selection-and-regularization.html#cb513-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb513-4"><a href="linear-model-selection-and-regularization.html#cb513-4" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">map_dfr</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>, <span class="fl">0.1</span>),</span>
<span id="cb513-5"><a href="linear-model-selection-and-regularization.html#cb513-5" aria-hidden="true" tabindex="-1"></a>             <span class="sc">~</span> <span class="fu">tidy</span>(credit_lasso_fit, <span class="at">penalty =</span> <span class="dv">10</span><span class="sc">^</span>.x)) <span class="sc">%&gt;%</span></span>
<span id="cb513-6"><a href="linear-model-selection-and-regularization.html#cb513-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(term <span class="sc">!=</span> <span class="st">&quot;(Intercept)&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb513-7"><a href="linear-model-selection-and-regularization.html#cb513-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(penalty) <span class="sc">%&gt;%</span></span>
<span id="cb513-8"><a href="linear-model-selection-and-regularization.html#cb513-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">l1_norm =</span> <span class="fu">sum</span>(<span class="fu">abs</span>(estimate)),</span>
<span id="cb513-9"><a href="linear-model-selection-and-regularization.html#cb513-9" aria-hidden="true" tabindex="-1"></a>         <span class="at">l1_norm_ratio =</span> l1_norm <span class="sc">/</span> credit_lm_fit_l1_norm) <span class="sc">%&gt;%</span></span>
<span id="cb513-10"><a href="linear-model-selection-and-regularization.html#cb513-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb513-11"><a href="linear-model-selection-and-regularization.html#cb513-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb513-12"><a href="linear-model-selection-and-regularization.html#cb513-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">term_highlight =</span> <span class="fu">fct_other</span>(</span>
<span id="cb513-13"><a href="linear-model-selection-and-regularization.html#cb513-13" aria-hidden="true" tabindex="-1"></a>      term, <span class="at">keep =</span> <span class="fu">c</span>(<span class="st">&quot;Income&quot;</span>, <span class="st">&quot;Limit&quot;</span>, <span class="st">&quot;Rating&quot;</span>, <span class="st">&quot;Student_Yes&quot;</span>)</span>
<span id="cb513-14"><a href="linear-model-selection-and-regularization.html#cb513-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb513-15"><a href="linear-model-selection-and-regularization.html#cb513-15" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb513-16"><a href="linear-model-selection-and-regularization.html#cb513-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb513-17"><a href="linear-model-selection-and-regularization.html#cb513-17" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> d <span class="sc">%&gt;%</span></span>
<span id="cb513-18"><a href="linear-model-selection-and-regularization.html#cb513-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> penalty, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb513-19"><a href="linear-model-selection-and-regularization.html#cb513-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> term, <span class="at">color =</span> term_highlight), <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb513-20"><a href="linear-model-selection-and-regularization.html#cb513-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>() <span class="sc">+</span></span>
<span id="cb513-21"><a href="linear-model-selection-and-regularization.html#cb513-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(lambda), <span class="at">y =</span> <span class="st">&quot;Standardized coefficients&quot;</span>, <span class="at">color =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb513-22"><a href="linear-model-selection-and-regularization.html#cb513-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(td_colors<span class="sc">$</span>pastel6[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="st">&quot;grey80&quot;</span>)) <span class="sc">+</span></span>
<span id="cb513-23"><a href="linear-model-selection-and-regularization.html#cb513-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="fu">c</span>(<span class="fl">0.7</span>, <span class="fl">0.2</span>))</span>
<span id="cb513-24"><a href="linear-model-selection-and-regularization.html#cb513-24" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> d <span class="sc">%&gt;%</span></span>
<span id="cb513-25"><a href="linear-model-selection-and-regularization.html#cb513-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> l1_norm_ratio, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb513-26"><a href="linear-model-selection-and-regularization.html#cb513-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> term, <span class="at">color =</span> term_highlight), <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb513-27"><a href="linear-model-selection-and-regularization.html#cb513-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb513-28"><a href="linear-model-selection-and-regularization.html#cb513-28" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;||&quot;</span>, <span class="fu">hat</span>(beta[lambda]), <span class="st">&quot;||1 / ||&quot;</span>, <span class="fu">hat</span>(beta), <span class="st">&quot;||1&quot;</span>)),</span>
<span id="cb513-29"><a href="linear-model-selection-and-regularization.html#cb513-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="cn">NULL</span>, <span class="at">color =</span> <span class="cn">NULL</span></span>
<span id="cb513-30"><a href="linear-model-selection-and-regularization.html#cb513-30" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb513-31"><a href="linear-model-selection-and-regularization.html#cb513-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(td_colors<span class="sc">$</span>pastel6[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="st">&quot;grey80&quot;</span>)) <span class="sc">+</span></span>
<span id="cb513-32"><a href="linear-model-selection-and-regularization.html#cb513-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>)</span>
<span id="cb513-33"><a href="linear-model-selection-and-regularization.html#cb513-33" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">|</span> p2</span></code></pre></div>
<p><img src="_main_files/figure-html/figure6.6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The shapes of the curves are right, but the scale of the <span class="math inline">\(\lambda\)</span> parameter in the left panel and the ratio in the right panel are much different from the text – not sure what happened there.</p>
<div id="another-formulation-for-ridge-regression-and-the-lasso" class="section level4 unnumbered">
<h4>Another Formulation for Ridge Regression and the Lasso</h4>
<p>One can show that the lasso and ridge regression coefficient estimates solve the problems:</p>
<p><span class="math display">\[
\begin{align}
&amp;\text{minimize } \beta \left[ \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2 \right] \text{ subject to } \sum_{j=1}^p |\beta_j| \leq s \\
&amp;\text{minimize } \beta \left[ \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2 \right] \text{ subject to } \sum_{j=1}^p \beta_j^2 \leq s,
\end{align}
\]</span></p>
<p>respectively.
In other words, for every value of <span class="math inline">\(\lambda\)</span>, there is some <span class="math inline">\(s\)</span> associated with the lasso/ridge coefficient estimates.
For <span class="math inline">\(p = 2\)</span>, then the lasso coefficient estimates have the smallest RSS out of all points that lie within the diamond defined by <span class="math inline">\(|\beta_1| + |\beta_2| \leq s\)</span>;
likewise, the circle <span class="math inline">\(\beta_1^2 + \beta_2^2 \leq s\)</span> for ridge regression.</p>
<p>This formulation with <span class="math inline">\(s\)</span> can be though of in terms of a <em>budget</em> of coefficient size:</p>
<blockquote>
<p>When we perform the lasso we are trying
to find the set of coefficient estimates that lead to the smallest RSS, subject
to the constraint that there is a budget <span class="math inline">\(s\)</span> for how large <span class="math inline">\(\sum_{j=1}^p |\beta_j|\)</span> can be.
When <span class="math inline">\(s\)</span> is extremely large, then this budget is not very restrictive, and so the coefficient estimates can be large. In fact, if <span class="math inline">\(s\)</span> is large enough that the least squares solution falls within the budget, then (6.8) will simply yield the least squares solution. In contrast, if <span class="math inline">\(s\)</span> is small, then <span class="math inline">\(\sum_{j=1}^p |\beta_j|\)</span> must be
small in order to avoid violating the budget. Similarly, (6.9) indicates that
when we perform ridge regression, we seek a set of coefficient estimates
such that the RSS is as small as possible, subject to the requirement that
<span class="math inline">\(\sum^p_{j=1} \beta_j^2\)</span> does not exceed the budget <span class="math inline">\(s\)</span>.</p>
</blockquote>
<p>This formulation also allows us to see the close connection between lasso, ridge and best subset selection.
Best subset selection is the minimization problem:</p>
<p><span class="math display">\[
\text{minimize } \beta \left[ \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2 \right] \text{ subject to } \sum_{j=1}^p I(\beta_j \neq 0) \leq s.
\]</span></p>
</div>
<div id="the-variable-selection-property-of-the-lasso" class="section level4 unnumbered">
<h4>The Variable Selection Property of the Lasso</h4>
<p>To understand why the lasso can remove predictors by shrinking coefficients to exactly zero, we can think of the shapes of the constraint functions, and the contours of RSS around the least squares estimate
(see Figure 6.7 in the book for a visualization).
For <span class="math inline">\(p = 2\)</span>, this is a circle (<span class="math inline">\(\beta_1^2 + \beta_2^2 \leq s\)</span>) for ridge, and square (<span class="math inline">\(|\beta_1| + |\beta_2| \leq s\)</span>) for lasso regression.</p>
<blockquote>
<p>Each of the ellipses centered around <span class="math inline">\(\hat{\beta}\)</span> represents a contour: this means that all of the points on a particular ellipse have the same RSS value.
As the ellipses expand away from the least squares coefficient estimates, the
RSS increases. Equations (6.8) and (6.9) indicate that the lasso and ridge
regression coefficient estimates are given by the first point at which an
ellipse contacts the constraint region. Since ridge regression has a circular
constraint with no sharp points, this intersection will not generally occur on
an axis, and so the ridge regression coefficient estimates will be exclusively
non-zero. However, the lasso constraint has corners at each of the axes, and
so the ellipse will often intersect the constraint region at an axis. When this
occurs, one of the coefficients will equal zero. In higher dimensions, many of
the coefficient estimates may equal zero simultaneously. In Figure 6.7, the
intersection occurs at <span class="math inline">\(\beta_1 = 0\)</span>, and so the resulting model will only include <span class="math inline">\(\beta_2\)</span>.</p>
</blockquote>
<p>This key idea holds for large dimension <span class="math inline">\(p &gt; 2\)</span> – the lasso constraint will always have sharp corners, and the ridge constraint will not.</p>
</div>
<div id="comparing-the-lasso-and-ridge-regression" class="section level4 unnumbered">
<h4>Comparing the Lasso and Ridge Regression</h4>
<p>Generally, the lasso leads to qualitatively similar behavior to ridge regression, in that a larger <span class="math inline">\(\lambda\)</span> increases bias and decreases variance.
A helpful way to compare models with different types of regularization is to plot <span class="math inline">\(R^2\)</span> as in Figure 6.8 comparing lasso and ridge MSE.
In this example, the results are almost identical, with a slight edge to the ridge regression due to lower variance.
This advantage is due to the fact that the simulated data consisted of 45 predictors, that were all related to the response – that is, none of the true coefficients equaled zero.
Lasso implicitly assumes that a number of the coefficients are truly zero, so it is not surprising it performs slightly worse on this example.
By contrast, the example in Figure 6.9 was simulated so that only 2 out of 45 predictors were related to the response.
In this case, the lasso tends to outperform ridge regression in terms of bias, variance and MSE.</p>
<blockquote>
<p>These two examples illustrate that neither ridge regression nor the lasso
will universally dominate the other. In general, one might expect the lasso
to perform better in a setting where a relatively small number of predictors
have substantial coefficients, and the remaining predictors have coefficients
that are very small or that equal zero. Ridge regression will perform better
when the response is a function of many predictors, all with coefficients of
roughly equal size. However, the number of predictors that is related to the
response is never known a priori for real data sets. A technique such as
cross-validation can be used in order to determine which approach is better
on a particular data set.</p>
<p>As with ridge regression, when the least squares estimates have excessively high variance, the lasso solution can yield a reduction in variance
at the expense of a small increase in bias, and consequently can gener-
ate more accurate predictions. Unlike ridge regression, the lasso performs
variable selection, and hence results in models that are easier to interpret.</p>
<p>There are very efficient algorithms for fitting both ridge and lasso models;
in both cases the entire coefficient paths can be computed with about the
same amount of work as a single least squares fit.</p>
</blockquote>
</div>
<div id="a-simple-special-case-for-ridge-regression-and-the-lasso" class="section level4 unnumbered">
<h4>A Simple Special Case for Ridge Regression and the Lasso</h4>
<p>Consider a simple case with <span class="math inline">\(n = p\)</span> and a diagonal matrix <span class="math inline">\(\bf{X}\)</span> with 1’s on the diagonal and 0’s in all off-diagonal elements.
To simplify further, assume that we are performing regression without an intercept.
Then the usual least squares problem involves minimizing:</p>
<p><span class="math display">\[
\sum_{j=1}^p (y_j - \beta_j)^2.
\]</span></p>
<p>Which has the solution <span class="math inline">\(\hat{\beta}_j = y_j\)</span>.
In ridge and lasso regression, the following are minimized:</p>
<p><span class="math display">\[
\begin{align}
&amp;\sum_{j=1}^p (y_j - \beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2 \\
&amp;\sum_{j=1}^p (y_j - \beta_j)^2 + \lambda \sum_{j=1}^p |\beta_j|.
\end{align}
\]</span></p>
<p>One can show that the ridge regression estimates take the form</p>
<p><span class="math display">\[
\hat{\beta}_j^R = y_j / (1 + \lambda)
\]</span></p>
<p>and the lasso estimates take the form</p>
<p><span class="math display">\[
\hat{\beta}_j^L = \left\{\begin{array}{lr}
y_j - \lambda / 2, &amp;\text{if } y_j &gt; \lambda / 2 \\
y_j + \lambda / 2, &amp;\text{if } y_j &lt; - \lambda / 2 \\
0, &amp;\text{if } |y_j| \leq \lambda / 2
\end{array}\right.
\]</span></p>
<p>The coefficient estimates are shown in Figure 6.10.
The ridge regression estimates are all shrunken towards zero by the same proportion, while the lasso shrinks each coefficient by the same amount and values less than <span class="math inline">\(\lambda / 2\)</span> are shrunken to exactly zero.
This latter type of shrinkage is known as <em>soft-thresholding</em>, and is how lasso performs feature selection.</p>
<blockquote>
<p>…ridge regression more or less shrinks every dimension
of the data by the same proportion, whereas the lasso more or less shrinks
all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero.</p>
</blockquote>
</div>
<div id="bayesian-interpretation-for-ridge-regression-and-the-lasso" class="section level4 unnumbered">
<h4>Bayesian Interpretation for Ridge Regression and the Lasso</h4>
<p>A Bayesian viewpoint for regression assumes that the vector of coefficients <span class="math inline">\(\beta\)</span> has some <em>prior</em> distribution <span class="math inline">\(p(\beta)\)</span>, where <span class="math inline">\(\beta = (\beta_0, \dots , \beta_p)^T\)</span>.
The likelihood of the data is then <span class="math inline">\(f(Y|X,\beta)\)</span>.
Multiplying the prior by the likelihood gives us (up to a proportionality constant) the <em>posterior distribution</em> from Bayes’ theorem:</p>
<p><span class="math display">\[
p (\beta|X, Y) \propto f(Y|X,\beta) p(\beta|X) = f(Y|X,\beta) p(\beta).
\]</span></p>
<p>If we assume</p>
<ul>
<li>the usual linear model <span class="math inline">\(Y = \beta_0 + X_1 \beta_1 + \dots X_p \beta_p + \epsilon\)</span>,</li>
<li>the errors are independent and drawn from a normal distribution,</li>
<li>that <span class="math inline">\(p(\beta) = \Pi_{j=1}^p g(\beta_j)\)</span> for some density function <span class="math inline">\(g\)</span>.</li>
</ul>
<p>It turns out that ridge and lasso follow naturally from two special cases of <span class="math inline">\(g\)</span>:</p>
<ul>
<li>If <span class="math inline">\(g\)</span> is a Gaussian distribution with mean zero and standard deviation a function of <span class="math inline">\(\lambda\)</span>, then it follow that the <em>posterior mode</em> for <span class="math inline">\(\beta\)</span> – that is, the most likely value for <span class="math inline">\(\beta\)</span>, given the data – is given by the ridge regression solution. (In fact, the ridge regression solution is also the posterior mean.)</li>
<li>If <span class="math inline">\(g\)</span> is a double-exponential (Laplace) distribution with mean zero and scale parameter a function of <span class="math inline">\(\lambda\)</span>, then it follow that the posterior mode for <span class="math inline">\(\beta\)</span> is the lasso solution. (However, the lasso solution is <em>not</em> the posterior mean, and in fact, the posterior mean does not yield a spare coefficient vector).</li>
</ul>
<blockquote>
<p>The Gaussian and double-exponential priors are displayed in Figure 6.11.
Therefore, from a Bayesian viewpoint, ridge regression and the lasso follow
directly from assuming the usual linear model with normal errors, together
with a simple prior distribution for <span class="math inline">\(β\)</span>. Notice that the lasso prior is steeply
peaked at zero, while the Gaussian is flatter and fatter at zero. Hence, the
lasso expects a priori that many of the coefficients are (exactly) zero, while
ridge assumes the coefficients are randomly distributed about zero.</p>
</blockquote>
</div>
</div>
<div id="selecting-the-tuning-parameter" class="section level3" number="6.2.3">
<h3><span class="header-section-number">6.2.3</span> Selecting the Tuning Parameter</h3>
<p>Just like subset selection requires a method to determine which models are best, these regularization methods require a method for selecting a value for the tuning parameter <span class="math inline">\(\lambda\)</span> (or constraint <span class="math inline">\(s\)</span>).
Cross-validation is the simple way to tackle this.
We choose a grid of <span class="math inline">\(\lambda\)</span> values, and compute CV error for each value of <span class="math inline">\(\lambda\)</span>, and select the value for which error is smallest.
The model is then re-fit using all available observations with the selected tuning parameter <span class="math inline">\(\lambda\)</span>.</p>
<p>As <a href="resampling-methods.html#loocv">explained previously</a>, the <code>tidydmodels</code> framework does not allow fitting by LOOCV.</p>
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb514-1"><a href="linear-model-selection-and-regularization.html#cb514-1" aria-hidden="true" tabindex="-1"></a>credit_splits <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(credit, <span class="at">v =</span> <span class="dv">10</span>)</span></code></pre></div>
<p>Then the recipe (nothing new here):</p>
<div class="sourceCode" id="cb515"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb515-1"><a href="linear-model-selection-and-regularization.html#cb515-1" aria-hidden="true" tabindex="-1"></a>credit_ridge_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(Balance <span class="sc">~</span> ., <span class="at">data =</span> credit) <span class="sc">%&gt;%</span></span>
<span id="cb515-2"><a href="linear-model-selection-and-regularization.html#cb515-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_dummy</span>(<span class="fu">all_nominal_predictors</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb515-3"><a href="linear-model-selection-and-regularization.html#cb515-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_predictors</span>())</span></code></pre></div>
<p>In the model specification, I set <code>mixture = 0</code> for ridge regression, and set <code>penalty = tune()</code> to indicate it as a tunable parameter:</p>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="linear-model-selection-and-regularization.html#cb516-1" aria-hidden="true" tabindex="-1"></a>ridge_spec <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>(<span class="at">mixture =</span> <span class="dv">0</span>, <span class="at">penalty =</span> <span class="fu">tune</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb516-2"><a href="linear-model-selection-and-regularization.html#cb516-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>)</span></code></pre></div>
<p>Combine into a <code>workflow</code>:</p>
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb517-1"><a href="linear-model-selection-and-regularization.html#cb517-1" aria-hidden="true" tabindex="-1"></a>credit_ridge_workflow <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb517-2"><a href="linear-model-selection-and-regularization.html#cb517-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(credit_ridge_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb517-3"><a href="linear-model-selection-and-regularization.html#cb517-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(ridge_spec)</span>
<span id="cb517-4"><a href="linear-model-selection-and-regularization.html#cb517-4" aria-hidden="true" tabindex="-1"></a>credit_ridge_workflow</span></code></pre></div>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: linear_reg()
## 
## -- Preprocessor ----------------------------------------------------------------
## 2 Recipe Steps
## 
## * step_dummy()
## * step_normalize()
## 
## -- Model -----------------------------------------------------------------------
## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = tune()
##   mixture = 0
## 
## Computational engine: glmnet</code></pre>
<p>Lastly, because we are tuning <code>penalty</code> (<span class="math inline">\(\lambda\)</span>), we need to define a grid of values to try when fitting the model.
The <code>dials</code> package provides many tools for tuning in <code>tidymodels</code>.
<code>grid_regular()</code> creates a grid of evenly spaced points.
As the first argument, I provide a <code>penalty()</code> with argument <code>range</code> that takes minimum and maximum values on a log scale:</p>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb519-1"><a href="linear-model-selection-and-regularization.html#cb519-1" aria-hidden="true" tabindex="-1"></a>penalty_grid <span class="ot">&lt;-</span> <span class="fu">grid_regular</span>(<span class="fu">penalty</span>(<span class="at">range =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">4</span>)), <span class="at">levels =</span> <span class="dv">10</span>)</span>
<span id="cb519-2"><a href="linear-model-selection-and-regularization.html#cb519-2" aria-hidden="true" tabindex="-1"></a>penalty_grid</span></code></pre></div>
<pre><code>## # A tibble: 10 x 1
##       penalty
##         &lt;dbl&gt;
##  1     0.01  
##  2     0.0464
##  3     0.215 
##  4     1     
##  5     4.64  
##  6    21.5   
##  7   100     
##  8   464.    
##  9  2154.    
## 10 10000</code></pre>
<p>To fit the models using this grid of values, we use <code>tune_grid()</code>:</p>
<div class="sourceCode" id="cb521"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb521-1"><a href="linear-model-selection-and-regularization.html#cb521-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tic</span>()</span>
<span id="cb521-2"><a href="linear-model-selection-and-regularization.html#cb521-2" aria-hidden="true" tabindex="-1"></a>credit_ridge_tune <span class="ot">&lt;-</span> <span class="fu">tune_grid</span>(</span>
<span id="cb521-3"><a href="linear-model-selection-and-regularization.html#cb521-3" aria-hidden="true" tabindex="-1"></a>  credit_ridge_workflow,</span>
<span id="cb521-4"><a href="linear-model-selection-and-regularization.html#cb521-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">resamples =</span> credit_splits,</span>
<span id="cb521-5"><a href="linear-model-selection-and-regularization.html#cb521-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">grid =</span> penalty_grid</span>
<span id="cb521-6"><a href="linear-model-selection-and-regularization.html#cb521-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb521-7"><a href="linear-model-selection-and-regularization.html#cb521-7" aria-hidden="true" tabindex="-1"></a><span class="fu">toc</span>()</span></code></pre></div>
<pre><code>## 1.38 sec elapsed</code></pre>
<div class="sourceCode" id="cb523"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb523-1"><a href="linear-model-selection-and-regularization.html#cb523-1" aria-hidden="true" tabindex="-1"></a>credit_ridge_tune <span class="sc">%&gt;%</span> <span class="fu">autoplot</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-284-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="dimension-reduction-methods" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> Dimension Reduction Methods</h2>
<blockquote>
<p>The methods that we have discussed so far in this chapter have controlled
variance in two different ways, either by using a subset of the original variables, or by shrinking their coefficients toward zero. All of these methods
are defined using the original predictors, <span class="math inline">\(X_1, X_2, \dots , X_p\)</span>. We now explore
a class of approaches that <em>transform</em> the predictors and then fit a least
squares model using the transformed variables. We will refer to these techniques as <em>dimension reduction methods</em>.</p>
</blockquote>
<p>We represent our original <span class="math inline">\(p\)</span> predictors <span class="math inline">\(X_j\)</span> as <span class="math inline">\(M\)</span> (<span class="math inline">\(&lt; p\)</span>) linear combinations:</p>
<p><span class="math display">\[
Z_m = \sum_{j=1}^p \phi_{jm} X_j
\]</span></p>
<p>for some constants <span class="math inline">\(\phi_{1m}, \phi_{2m}, \dots \phi_{pm}\)</span> , <span class="math inline">\(m = 1, \dots, M\)</span>.
We can then fit a linear regression model:</p>
<p><span class="math display">\[
y_i = \theta_0 + \sum_{m=1}^M \theta_m z_{im} + \epsilon_i, \ \ \ \ i = 1, \dots, n,
\]</span></p>
<p>using least squares.
Note that the regression coefficients here are represented by <span class="math inline">\(\theta_0, \theta_1, \dots, \theta_M\)</span>.</p>
<blockquote>
<p>If the constants <span class="math inline">\(\phi_{1m}, \phi_{2m}, \dots , \phi_{pm}\)</span> are chosen wisely, then
such dimension reduction approaches can often outperform least squares
regression.</p>
</blockquote>
<p>This method is called <em>dimension reduction</em> because it involves reducing the <span class="math inline">\(p+1\)</span> coefficients <span class="math inline">\(\beta_0, \dots, \beta_p\)</span> down to <span class="math inline">\(M + 1\)</span> coefficients <span class="math inline">\(\theta_0, \dots, \theta_M\)</span> (<span class="math inline">\(M &lt; p\)</span>).</p>
<p>Notice that the models are numerically equivalent:</p>
<p><span class="math display">\[
\sum_{m=1}^M \theta_m z_{im} = \sum_{m=1}^M \theta_m \sum_{j=1}^p \phi_{jm} x_{ij} =
\sum_{j=1}^p \sum_{m=1}^M \theta_m \phi_{jm} x_{ij} = \sum_{j=1}^p \beta_j x_{ij},
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\beta_j = \sum_{m=1}^M \theta_m \phi_{jm}.
\]</span></p>
<p>Hence, this can be thought of as a special case of the original linear regression model.
Dimension reduction serves to constrain the estimated <span class="math inline">\(\beta_j\)</span> coefficients to the above form, which has the potential to bias the estimates.
However, in situations where <span class="math inline">\(p\)</span> is large relative to <span class="math inline">\(n\)</span>, selecting a value of <span class="math inline">\(M \ll p\)</span> can significantly reduce the variance of the fitted coefficients.</p>
<blockquote>
<p>All dimension reduction methods work in two steps. First, the transformed predictors <span class="math inline">\(Z_1, Z_2, \dots Z_M\)</span> are obtained. Second, the model is fit
using these <span class="math inline">\(M\)</span> predictors. However, the choice of <span class="math inline">\(Z_1, Z_2, \dots Z_M\)</span>, or equivalently, the selection of the <span class="math inline">\(\phi_{jm}\)</span>’s, can be achieved in different ways. In this
chapter, we will consider two approaches for this task: <em>principal components</em>
and <em>partial least squares</em>.</p>
</blockquote>
<div id="principal-components-regression" class="section level3" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Principal Components Regression</h3>
<div id="an-overview-of-principal-components-analysis" class="section level4 unnumbered">
<h4>An Overview of Principal Components Analysis</h4>
<p>Principal components analysis (PCA) is a technique for reducing the dimension of an <span class="math inline">\(n \times p\)</span> data matrix <span class="math inline">\(\textbf{X}\)</span>.
The <em>first principal component</em> direction of the data is that along which the observations <em>vary the most</em>.
If the observations were to be projected onto the line definition this principal component, then the resulting projected observations would have the largest possible variance.
Alternatively: the first principal component defines the line that is as close as possible to the data.
The principal component can be summarized mathematically as a linear combination of the variables, <span class="math inline">\(Z_1\)</span>.</p>
<p>The second principal component <span class="math inline">\(Z_2\)</span> is a linear combination of the variables that is uncorrelated with <span class="math inline">\(Z_1\)</span>, and has the largest variance subject to this constraint.
Visually, the direction of this line must be perpendicular or orthogonal to the first principal component.</p>
<p>Up to <span class="math inline">\(p\)</span> distinct principal components can be constructed, but the first component will contain the most information.
Each component successively maximizes variance, subject to the constraint of being uncorrelated with the previous components.</p>
</div>
<div id="the-principal-components-regression-approach" class="section level4 unnumbered">
<h4>The Principal Components Regression Approach</h4>
<blockquote>
<p>The principal components regression (PCR) approach involves constructing
the first <span class="math inline">\(M\)</span> principal components, <span class="math inline">\(Z_1, \dots, Z_M\)</span>, and then using these
components as the predictors in a linear regression model that is fit using
least squares. The key idea is that often a small number of principal
components suffice to explain most of the variability in the data, as well
as the relationship with the response. In other words, we assume that the
directions in which <span class="math inline">\(X_1, \dots, X_p\)</span> show the most variation are the directions
that are associated with <span class="math inline">\(Y\)</span>. While this assumption is not guaranteed to be
true, it often turns out to be a reasonable enough approximation to give
good results.</p>
<p>If the assumption underlying PCR holds, then fitting a least squares
model to <span class="math inline">\(Z_1,\dots, Z_M\)</span> will lead to better results than fitting a least squares
model to <span class="math inline">\(X_1,\dots,X_p\)</span>, since most or all of the information in the data that
relates to the response is contained in <span class="math inline">\(Z_1,\dots,Z_M\)</span>, and by estimating only
<span class="math inline">\(M ≪ p\)</span> coefficients we can mitigate overfitting</p>
</blockquote>
<p>It is important to note that, although PCR offers a simple way to perform regression using a smaller number of predictors (<span class="math inline">\(M &lt; p\)</span>), it is <em>not</em> a feature selection method.
The <span class="math inline">\(M\)</span> principal components used in the regression is a linear combination of all <span class="math inline">\(p\)</span> of the <em>original</em> features.
In this sense, PCR is more closely related to ridge regression than the lasso (because the lasso shrinks coefficients to exactly zero, essentially removing them).
In fact, ridge regression can be considered a continuous version of PCR.</p>
<p>In <code>tidymodels</code>, principal components are extracted in pre-processing via <code>recipes::step_pca()</code>.
Here is the workflow applied to the <code>credit</code> data set with a linear model:</p>
<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb524-1"><a href="linear-model-selection-and-regularization.html#cb524-1" aria-hidden="true" tabindex="-1"></a>lm_spec <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>() <span class="sc">%&gt;%</span> <span class="fu">set_engine</span>(<span class="st">&quot;lm&quot;</span>)</span>
<span id="cb524-2"><a href="linear-model-selection-and-regularization.html#cb524-2" aria-hidden="true" tabindex="-1"></a>credit_splits <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(credit, <span class="at">v =</span> <span class="dv">10</span>)</span>
<span id="cb524-3"><a href="linear-model-selection-and-regularization.html#cb524-3" aria-hidden="true" tabindex="-1"></a>credit_pca_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(Balance <span class="sc">~</span> ., <span class="at">data =</span> credit) <span class="sc">%&gt;%</span></span>
<span id="cb524-4"><a href="linear-model-selection-and-regularization.html#cb524-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_dummy</span>(<span class="fu">all_nominal_predictors</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb524-5"><a href="linear-model-selection-and-regularization.html#cb524-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_predictors</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb524-6"><a href="linear-model-selection-and-regularization.html#cb524-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_pca</span>(<span class="fu">all_predictors</span>(), <span class="at">num_comp =</span> <span class="fu">tune</span>())</span>
<span id="cb524-7"><a href="linear-model-selection-and-regularization.html#cb524-7" aria-hidden="true" tabindex="-1"></a>credit_pca_workflow <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb524-8"><a href="linear-model-selection-and-regularization.html#cb524-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(credit_pca_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb524-9"><a href="linear-model-selection-and-regularization.html#cb524-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(lm_spec)</span></code></pre></div>
<p>The <code>num_comp = tune()</code> argument to <code>step_pca()</code> allows variation in the number of principal components <span class="math inline">\(M\)</span>.
To <code>tune_grid()</code>, I only need to provide a data frame of possible <code>num_comp</code> values, but here is the <code>dials::grid_regular()</code> approach to doing that:</p>
<div class="sourceCode" id="cb525"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb525-1"><a href="linear-model-selection-and-regularization.html#cb525-1" aria-hidden="true" tabindex="-1"></a>pca_grid <span class="ot">&lt;-</span> <span class="fu">grid_regular</span>(<span class="fu">num_comp</span>(<span class="at">range =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">11</span>)), <span class="at">levels =</span> <span class="dv">11</span>)</span>
<span id="cb525-2"><a href="linear-model-selection-and-regularization.html#cb525-2" aria-hidden="true" tabindex="-1"></a>pca_grid</span></code></pre></div>
<pre><code>## # A tibble: 11 x 1
##    num_comp
##       &lt;int&gt;
##  1        1
##  2        2
##  3        3
##  4        4
##  5        5
##  6        6
##  7        7
##  8        8
##  9        9
## 10       10
## 11       11</code></pre>
<p>Perform 10 fold cross-validation for PCR with 1 to 11 components:</p>
<div class="sourceCode" id="cb527"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb527-1"><a href="linear-model-selection-and-regularization.html#cb527-1" aria-hidden="true" tabindex="-1"></a>credit_pca_tune <span class="ot">&lt;-</span> <span class="fu">tune_grid</span>(</span>
<span id="cb527-2"><a href="linear-model-selection-and-regularization.html#cb527-2" aria-hidden="true" tabindex="-1"></a>  credit_pca_workflow,</span>
<span id="cb527-3"><a href="linear-model-selection-and-regularization.html#cb527-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">resamples =</span> credit_splits, <span class="at">grid =</span> pca_grid,</span>
<span id="cb527-4"><a href="linear-model-selection-and-regularization.html#cb527-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This option extracts the model fits, which are otherwise discarded</span></span>
<span id="cb527-5"><a href="linear-model-selection-and-regularization.html#cb527-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">control =</span> <span class="fu">control_grid</span>(<span class="at">extract =</span> <span class="cf">function</span>(m) <span class="fu">extract_fit_engine</span>(m))</span>
<span id="cb527-6"><a href="linear-model-selection-and-regularization.html#cb527-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>And re-create the right panel of Figure 6.20:</p>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb528-1"><a href="linear-model-selection-and-regularization.html#cb528-1" aria-hidden="true" tabindex="-1"></a>credit_pca_tune <span class="sc">%&gt;%</span></span>
<span id="cb528-2"><a href="linear-model-selection-and-regularization.html#cb528-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb528-3"><a href="linear-model-selection-and-regularization.html#cb528-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(.metric <span class="sc">==</span> <span class="st">&quot;rmse&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb528-4"><a href="linear-model-selection-and-regularization.html#cb528-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">mse =</span> mean<span class="sc">^</span><span class="dv">2</span>) <span class="sc">%&gt;%</span></span>
<span id="cb528-5"><a href="linear-model-selection-and-regularization.html#cb528-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> num_comp, <span class="at">y =</span> mse)) <span class="sc">+</span></span>
<span id="cb528-6"><a href="linear-model-selection-and-regularization.html#cb528-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> td_colors<span class="sc">$</span>nice<span class="sc">$</span>spanish_blue, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb528-7"><a href="linear-model-selection-and-regularization.html#cb528-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">21</span>, <span class="at">size =</span> <span class="dv">3</span>,</span>
<span id="cb528-8"><a href="linear-model-selection-and-regularization.html#cb528-8" aria-hidden="true" tabindex="-1"></a>             <span class="at">fill =</span> td_colors<span class="sc">$</span>nice<span class="sc">$</span>spanish_blue, <span class="at">color =</span> <span class="st">&quot;white&quot;</span>) <span class="sc">+</span></span>
<span id="cb528-9"><a href="linear-model-selection-and-regularization.html#cb528-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;Cross-validation MSE&quot;</span>,</span>
<span id="cb528-10"><a href="linear-model-selection-and-regularization.html#cb528-10" aria-hidden="true" tabindex="-1"></a>                     <span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">20000</span>, <span class="dv">40000</span>, <span class="dv">60000</span>, <span class="dv">80000</span>)) <span class="sc">+</span></span>
<span id="cb528-11"><a href="linear-model-selection-and-regularization.html#cb528-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="st">&quot;Number of components&quot;</span>, <span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">10</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/figure6.20-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>By MSE, the best performing models are <span class="math inline">\(M\)</span> = 10 and 11 principal components, which mean that dimension reduction is not needed here because <span class="math inline">\(p = 11\)</span>.</p>
<p>Note the <code>step_normalize()</code> function in the recipe used here.
This is important when performing PCR:</p>
<blockquote>
<p>We generally recommend standardizing each predictor, using (6.6), prior to generating the principal components. This standardization ensures that all variables are on the same scale. In the absence
of standardization, the high-variance variables will tend to play a larger
role in the principal components obtained, and the scale on which the variables are measured will ultimately have an effect on the final PCR model.
However, if the variables are all measured in the same units (say, kilograms,
or inches), then one might choose not to standardize them.</p>
</blockquote>
</div>
</div>
<div id="partial-least-squares" class="section level3" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Partial Least Squares</h3>
<blockquote>
<p>The PCR approach that we just described involves identifying linear combinations, or directions, that best represent the predictors <span class="math inline">\(X1,\dots,X_p\)</span>. These
directions are identified in an unsupervised way, since the response <span class="math inline">\(Y\)</span> is not
used to help determine the principal component directions. That is, the
response does not supervise the identification of the principal components.
Consequently, PCR suffers from a drawback: there is no guarantee that the
directions that best explain the predictors will also be the best directions
to use for predicting the response. Unsupervised methods are discussed
further in Chapter 12.</p>
</blockquote>
<p>Like PCR, <em>partial least squares</em> (PLS) is a dimension reduction method which identifies a set of features <span class="math inline">\(Z_1, \dots, Z_M\)</span> that are linear combinations of the <span class="math inline">\(p\)</span> original predictors.
Unlike PCR, PLS identifies the new features which are <em>related to the response</em> <span class="math inline">\(Y\)</span>.
In this way, PLS is a supervised alternative to PCR.
Roughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors.</p>
<p>After standardizing the <span class="math inline">\(p\)</span> predictors, the first PLS direction <span class="math inline">\(Z_1 = \sum_{j=1}^p \phi_{j1} X_j\)</span> is found from the simple linear regression of <span class="math inline">\(Y\)</span> onto <span class="math inline">\(X_j\)</span>.
This means that PLS places the highest weight on the variables that are most strongly related to the response.
For the second PLS direction, each of the variables is <em>adjusted</em> by regression on <span class="math inline">\(Z_1\)</span> and taking residuals.
These residuals can be interpreted as the remanining information that has not been explained by the first PLS direction.
<span class="math inline">\(Z_2\)</span> is then computed using this <em>orthogonalized</em> data in the same way as <span class="math inline">\(Z_1\)</span> was computed on the original data.
This iterative approach is repeated <span class="math inline">\(M\)</span> times, and the final step is to fit the linear model to predict <span class="math inline">\(Y\)</span> using <span class="math inline">\(Z_1, \dots, Z_M\)</span> in exactly the same way as for PCR.</p>
<blockquote>
<p>As with PCR, the number <span class="math inline">\(M\)</span> of partial least squares directions used in
PLS is a tuning parameter that is typically chosen by cross-validation. We
generally standardize the predictors and response before performing PLS.</p>
</blockquote>
<p>In practice, PLS often performs no better than ridge regression or PCR.
It can often reduce bias, but it also has the potential to increase variance, so it evens out relative to other methods.</p>
</div>
</div>
<div id="considerations-in-high-dimensions" class="section level2" number="6.4">
<h2><span class="header-section-number">6.4</span> Considerations in High Dimensions</h2>
<div id="high-dimensional-data" class="section level3" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> High-Dimensional Data</h3>
<blockquote>
<p>Most traditional statistical techniques for regression and classification are
intended for the <em>low-dimensional</em> setting in which <span class="math inline">\(n\)</span>, the number of observations, is much greater than <span class="math inline">\(p\)</span>, the number of features. This is due in
part to the fact that throughout most of the field’s history, the bulk of scientific problems requiring the use of statistics have been low-dimensional.</p>
</blockquote>
<p>To be clear, by dimension, we are referring to the size of <span class="math inline">\(p\)</span>.</p>
<blockquote>
<p>In the past 20 years, new technologies have changed the way that data
are collected in fields as diverse as finance, marketing, and medicine. It is
now commonplace to collect an almost unlimited number of feature measurements (<span class="math inline">\(p\)</span> very large). While <span class="math inline">\(p\)</span> can be extremely large, the number of
observations <span class="math inline">\(n\)</span> is often limited due to cost, sample availability, or other
considerations.</p>
</blockquote>
<p>These <em>high-dimensional</em> problems, in which the number of features <span class="math inline">\(p\)</span> is larger than observations <span class="math inline">\(n\)</span>, are becoming more commonplace.</p>
</div>
<div id="what-goes-wrong-in-high-dimensions" class="section level3" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> What Goes Wrong in High Dimensions?</h3>
<p>To illustrate the <span class="math inline">\(p &gt; n\)</span> issue, we examine least squares regression (but the same concepts apply to logistic regression, linear discriminant analysis and others).
Least squares cannot be (or rather <em>should not</em>) be performed in this setting because it will yield coefficient estimates that perfectly fit the data, such that the residuals are zero.</p>
<div class="sourceCode" id="cb529"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb529-1"><a href="linear-model-selection-and-regularization.html#cb529-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">n_obs =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">20</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb529-2"><a href="linear-model-selection-and-regularization.html#cb529-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">x =</span> <span class="fu">map</span>(n_obs, rnorm)) <span class="sc">%&gt;%</span></span>
<span id="cb529-3"><a href="linear-model-selection-and-regularization.html#cb529-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(x) <span class="sc">%&gt;%</span></span>
<span id="cb529-4"><a href="linear-model-selection-and-regularization.html#cb529-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> <span class="dv">5</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">n</span>(), <span class="dv">0</span>, <span class="dv">3</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb529-5"><a href="linear-model-selection-and-regularization.html#cb529-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span></span>
<span id="cb529-6"><a href="linear-model-selection-and-regularization.html#cb529-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb529-7"><a href="linear-model-selection-and-regularization.html#cb529-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">formula =</span> <span class="st">&quot;y ~ x&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb529-8"><a href="linear-model-selection-and-regularization.html#cb529-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> <span class="fu">fct_rev</span>(<span class="fu">paste0</span>(<span class="st">&quot;n = &quot;</span>, <span class="fu">as.character</span>(n_obs)))) <span class="sc">+</span></span>
<span id="cb529-9"><a href="linear-model-selection-and-regularization.html#cb529-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_facet_borders</span>()</span></code></pre></div>
<pre><code>## Warning in qt((1 - level)/2, df): NaNs produced</code></pre>
<pre><code>## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning
## -Inf</code></pre>
<p><img src="_main_files/figure-html/figure6.22-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Note the warning returned by <code>geom_smooth</code> (NaNs and Infs produced) and the lack of errors around the best fit line in the right panel.</p>
<blockquote>
<p>On the
other hand, when there are only two observations, then regardless of the
values of those observations, the regression line will fit the data exactly.
This is problematic because this perfect fit will almost certainly lead to
overfitting of the data. In other words, though it is possible to perfectly fit
the training data in the high-dimensional setting, the resulting linear model
will perform extremely poorly on an independent test set, and therefore
does not constitute a useful model. In fact, we can see that this happened
in Figure 6.22: the least squares line obtained in the right-hand panel will
perform very poorly on a test set comprised of the observations in the lefth-and panel.
The problem is simple: when <span class="math inline">\(p&gt;n\)</span> or <span class="math inline">\(p \approx n\)</span>, a simple least
squares regression line is too flexible and hence overfits the data.</p>
</blockquote>
<p>Previously we saw a number of approaches for adjusting the training set error to account for the number of variables used to fit a least squares model.
Unfortunately, the <span class="math inline">\(C_p\)</span>, AIC, and BIC approaches are not appropriate in this setting, because estimating <span class="math inline">\(\hat{\sigma}^2\)</span> is problematic).
Similarly, a model can easily obtain an adjusted <span class="math inline">\(R^2\)</span> value of 1.
Clearly, alternative approaches that are better-suited to the high-dimensional setting are required.</p>
</div>
<div id="regression-in-high-dimensions" class="section level3" number="6.4.3">
<h3><span class="header-section-number">6.4.3</span> Regression in High Dimensions</h3>
<p>The methods learned in this chapter – stepwise selection, ridge and lasso regression, and principal components regression – are useful for performing regression in the high-dimensional setting because they are <em>less flexible</em> fitting procedures.</p>
<p>The lasso example in Figure 6.24 highlights three important points:</p>
<ol style="list-style-type: decimal">
<li>regularlization or shrinkage plays a key role in high-dimensional problems,</li>
<li>appropriate tuning parameter selection is crucial for good predictive performance, and</li>
<li>the test error tends to increase as the dimensionality of the problem (i.e. the number of feature or predictors) increases, unless those additional features are truly associated with the response.</li>
</ol>
<p>The third point is known as <em>the curve of dimensionality</em>.
In general, adding additional signal features that are truly associated with the response will improve the fitted model (and a reduction in test set error).
However, adding noise features that are not truly associated with teh response will lead to a worse fitted model (and an increase in test set error).</p>
<blockquote>
<p>Thus, we see that new technologies that allow for the collection
of measurements for thousands or millions of features are a double-edged
sword: they can lead to improved predictive models if these features are in
fact relevant to the problem at hand, but will lead to worse results if the
features are not relevant. Even if they are relevant, the variance incurred
in fitting their coefficients may outweigh the reduction in bias that they
bring.</p>
</blockquote>
</div>
<div id="interpreting-results-in-high-dimensions" class="section level3" number="6.4.4">
<h3><span class="header-section-number">6.4.4</span> Interpreting Results in High Dimensions</h3>
<p>When performing lasso, ridge or other regression in the high-dimensional setting, <em>multicollinearity</em> (where variables in a regression are correlated with each other) can be a big problem.
Any variable in the model can be written as a linear combination of all other variables in the model,
so we can never know exactly which variables (if any) truly are predictive of the outcome, and can never identify the <em>best</em> coefficients for use in regression.
A model like this can still have very high predictive value, but we must be careful not to overstate the results and make it clear that we have identified <em>one of many possible models</em> for predicting the outcome, and that is must be further validated on independent data sets.
It is also important to be careful in reporting errors.
We have seen in the example that when <span class="math inline">\(p &gt; n\)</span>, it is easy to obtain a useful model that has zero residuals.
In this case, traditional measures of model fit (e.g. <span class="math inline">\(R^2\)</span>) are misleading; instead, models should be assessed on an independent test set or cross-validation errors.</p>
</div>
</div>
<div id="lab-linear-models-and-regularization-methods" class="section level2" number="6.5">
<h2><span class="header-section-number">6.5</span> Lab: Linear Models and Regularization Methods</h2>
<div id="subset-selection-methods" class="section level3" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Subset Selection Methods</h3>
<div id="best-subset-selection-1" class="section level4 unnumbered">
<h4>Best Subset Selection</h4>
<p>Here, we aim to predict a baseball player’s <code>Salary</code> from performance statistics in the previous year:</p>
<div class="sourceCode" id="cb532"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb532-1"><a href="linear-model-selection-and-regularization.html#cb532-1" aria-hidden="true" tabindex="-1"></a>hitters <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(ISLR2<span class="sc">::</span>Hitters) <span class="sc">%&gt;%</span></span>
<span id="cb532-2"><a href="linear-model-selection-and-regularization.html#cb532-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.na</span>(Salary))</span>
<span id="cb532-3"><a href="linear-model-selection-and-regularization.html#cb532-3" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(hitters)</span></code></pre></div>
<pre><code>## Rows: 263
## Columns: 20
## $ AtBat     &lt;int&gt; 315, 479, 496, 321, 594, 185, 298, 323, 401, 574, 202, 418, ~
## $ Hits      &lt;int&gt; 81, 130, 141, 87, 169, 37, 73, 81, 92, 159, 53, 113, 60, 43,~
## $ HmRun     &lt;int&gt; 7, 18, 20, 10, 4, 1, 0, 6, 17, 21, 4, 13, 0, 7, 20, 2, 8, 16~
## $ Runs      &lt;int&gt; 24, 66, 65, 39, 74, 23, 24, 26, 49, 107, 31, 48, 30, 29, 89,~
## $ RBI       &lt;int&gt; 38, 72, 78, 42, 51, 8, 24, 32, 66, 75, 26, 61, 11, 27, 75, 8~
## $ Walks     &lt;int&gt; 39, 76, 37, 30, 35, 21, 7, 8, 65, 59, 27, 47, 22, 30, 73, 15~
## $ Years     &lt;int&gt; 14, 3, 11, 2, 11, 2, 3, 2, 13, 10, 9, 4, 6, 13, 15, 5, 8, 1,~
## $ CAtBat    &lt;int&gt; 3449, 1624, 5628, 396, 4408, 214, 509, 341, 5206, 4631, 1876~
## $ CHits     &lt;int&gt; 835, 457, 1575, 101, 1133, 42, 108, 86, 1332, 1300, 467, 392~
## $ CHmRun    &lt;int&gt; 69, 63, 225, 12, 19, 1, 0, 6, 253, 90, 15, 41, 4, 36, 177, 5~
## $ CRuns     &lt;int&gt; 321, 224, 828, 48, 501, 30, 41, 32, 784, 702, 192, 205, 309,~
## $ CRBI      &lt;int&gt; 414, 266, 838, 46, 336, 9, 37, 34, 890, 504, 186, 204, 103, ~
## $ CWalks    &lt;int&gt; 375, 263, 354, 33, 194, 24, 12, 8, 866, 488, 161, 203, 207, ~
## $ League    &lt;fct&gt; N, A, N, N, A, N, A, N, A, A, N, N, A, N, N, A, N, N, A, N, ~
## $ Division  &lt;fct&gt; W, W, E, E, W, E, W, W, E, E, W, E, E, E, W, W, W, E, W, W, ~
## $ PutOuts   &lt;int&gt; 632, 880, 200, 805, 282, 76, 121, 143, 0, 238, 304, 211, 121~
## $ Assists   &lt;int&gt; 43, 82, 11, 40, 421, 127, 283, 290, 0, 445, 45, 11, 151, 45,~
## $ Errors    &lt;int&gt; 10, 14, 3, 4, 25, 7, 9, 19, 0, 22, 11, 7, 6, 8, 10, 16, 2, 5~
## $ Salary    &lt;dbl&gt; 475.000, 480.000, 500.000, 91.500, 750.000, 70.000, 100.000,~
## $ NewLeague &lt;fct&gt; N, A, N, N, A, A, A, N, A, A, N, N, A, N, N, A, N, N, N, N, ~</code></pre>
<p>As in the text, I’ll use the <code>leaps</code> package to perform best subset selection via RSS.</p>
<div class="sourceCode" id="cb534"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb534-1"><a href="linear-model-selection-and-regularization.html#cb534-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(leaps)</span>
<span id="cb534-2"><a href="linear-model-selection-and-regularization.html#cb534-2" aria-hidden="true" tabindex="-1"></a>regsubsets_hitters_salary <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> hitters)</span>
<span id="cb534-3"><a href="linear-model-selection-and-regularization.html#cb534-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(regsubsets_hitters_salary)</span></code></pre></div>
<pre><code>## Subset selection object
## Call: regsubsets.formula(Salary ~ ., data = hitters)
## 19 Variables  (and intercept)
##            Forced in Forced out
## AtBat          FALSE      FALSE
## Hits           FALSE      FALSE
## HmRun          FALSE      FALSE
## Runs           FALSE      FALSE
## RBI            FALSE      FALSE
## Walks          FALSE      FALSE
## Years          FALSE      FALSE
## CAtBat         FALSE      FALSE
## CHits          FALSE      FALSE
## CHmRun         FALSE      FALSE
## CRuns          FALSE      FALSE
## CRBI           FALSE      FALSE
## CWalks         FALSE      FALSE
## LeagueN        FALSE      FALSE
## DivisionW      FALSE      FALSE
## PutOuts        FALSE      FALSE
## Assists        FALSE      FALSE
## Errors         FALSE      FALSE
## NewLeagueN     FALSE      FALSE
## 1 subsets of each size up to 8
## Selection Algorithm: exhaustive
##          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI
## 1  ( 1 ) &quot; &quot;   &quot; &quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 2  ( 1 ) &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 3  ( 1 ) &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 4  ( 1 ) &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 5  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 6  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 7  ( 1 ) &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot; 
## 8  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot; 
##          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN
## 1  ( 1 ) &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 2  ( 1 ) &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 3  ( 1 ) &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 4  ( 1 ) &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 5  ( 1 ) &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 6  ( 1 ) &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 7  ( 1 ) &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 8  ( 1 ) &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;</code></pre>
<p>An asterisk indicates the variable is included in the model, so the best variables in the three-predictor model are <code>Hits</code>, <code>CRBI</code> and <code>PutOuts</code>.
Note that there is a <code>broom::tidy()</code> function available for these objects:</p>
<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb536-1"><a href="linear-model-selection-and-regularization.html#cb536-1" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">tidy</span>(regsubsets_hitters_salary) <span class="sc">%&gt;%</span></span>
<span id="cb536-2"><a href="linear-model-selection-and-regularization.html#cb536-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">glimpse</span>()</span></code></pre></div>
<pre><code>## Rows: 8
## Columns: 24
## $ `(Intercept)` &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE
## $ AtBat         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE
## $ Hits          &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE
## $ HmRun         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
## $ Runs          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
## $ RBI           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
## $ Walks         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE
## $ Years         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
## $ CAtBat        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE
## $ CHits         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE
## $ CHmRun        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE
## $ CRuns         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE
## $ CRBI          &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE
## $ CWalks        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE
## $ LeagueN       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
## $ DivisionW     &lt;lgl&gt; FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE
## $ PutOuts       &lt;lgl&gt; FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE
## $ Assists       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
## $ Errors        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
## $ NewLeagueN    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
## $ r.squared     &lt;dbl&gt; 0.3214501, 0.4252237, 0.4514294, 0.4754067, 0.4908036, 0~
## $ adj.r.squared &lt;dbl&gt; 0.3188503, 0.4208024, 0.4450753, 0.4672734, 0.4808971, 0~
## $ BIC           &lt;dbl&gt; -90.84637, -128.92622, -135.62693, -141.80892, -144.0714~
## $ mallows_cp    &lt;dbl&gt; 104.281319, 50.723090, 38.693127, 27.856220, 21.613011, ~</code></pre>
<p>Instead of a formatted table of asterisks, this gives us a <code>tibble</code> of booleans which are nice to work with for producing custom outputs like:</p>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb538-1"><a href="linear-model-selection-and-regularization.html#cb538-1" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">tidy</span>(regsubsets_hitters_salary) <span class="sc">%&gt;%</span></span>
<span id="cb538-2"><a href="linear-model-selection-and-regularization.html#cb538-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span><span class="st">`</span><span class="at">(Intercept)</span><span class="st">`</span>) <span class="sc">%&gt;%</span></span>
<span id="cb538-3"><a href="linear-model-selection-and-regularization.html#cb538-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_rownames</span>(<span class="at">var =</span> <span class="st">&quot;n_vars&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb538-4"><a href="linear-model-selection-and-regularization.html#cb538-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gt</span>(<span class="at">rowname_col =</span> <span class="st">&quot;n_vars&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb538-5"><a href="linear-model-selection-and-regularization.html#cb538-5" aria-hidden="true" tabindex="-1"></a>  gt<span class="sc">::</span><span class="fu">data_color</span>(</span>
<span id="cb538-6"><a href="linear-model-selection-and-regularization.html#cb538-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">columns =</span> AtBat<span class="sc">:</span>NewLeagueN,</span>
<span id="cb538-7"><a href="linear-model-selection-and-regularization.html#cb538-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">colors =</span> <span class="fu">col_numeric</span>(</span>
<span id="cb538-8"><a href="linear-model-selection-and-regularization.html#cb538-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">palette =</span> <span class="fu">c</span>(td_colors<span class="sc">$</span>nice<span class="sc">$</span>soft_orange, td_colors<span class="sc">$</span>nice<span class="sc">$</span>lime_green),</span>
<span id="cb538-9"><a href="linear-model-selection-and-regularization.html#cb538-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">domain =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb538-10"><a href="linear-model-selection-and-regularization.html#cb538-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb538-11"><a href="linear-model-selection-and-regularization.html#cb538-11" aria-hidden="true" tabindex="-1"></a>  gt<span class="sc">::</span><span class="fu">fmt_number</span>(r.squared<span class="sc">:</span>mallows_cp, <span class="at">n_sigfig =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## Warning: `add_rownames()` was deprecated in dplyr 1.0.0.
## Please use `tibble::rownames_to_column()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.</code></pre>
<div id="evgzjwkzar" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#evgzjwkzar .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#evgzjwkzar .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#evgzjwkzar .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#evgzjwkzar .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 6px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#evgzjwkzar .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#evgzjwkzar .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#evgzjwkzar .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#evgzjwkzar .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#evgzjwkzar .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#evgzjwkzar .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#evgzjwkzar .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#evgzjwkzar .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#evgzjwkzar .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#evgzjwkzar .gt_from_md > :first-child {
  margin-top: 0;
}

#evgzjwkzar .gt_from_md > :last-child {
  margin-bottom: 0;
}

#evgzjwkzar .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#evgzjwkzar .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#evgzjwkzar .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#evgzjwkzar .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#evgzjwkzar .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#evgzjwkzar .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#evgzjwkzar .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#evgzjwkzar .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#evgzjwkzar .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#evgzjwkzar .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#evgzjwkzar .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#evgzjwkzar .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#evgzjwkzar .gt_left {
  text-align: left;
}

#evgzjwkzar .gt_center {
  text-align: center;
}

#evgzjwkzar .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#evgzjwkzar .gt_font_normal {
  font-weight: normal;
}

#evgzjwkzar .gt_font_bold {
  font-weight: bold;
}

#evgzjwkzar .gt_font_italic {
  font-style: italic;
}

#evgzjwkzar .gt_super {
  font-size: 65%;
}

#evgzjwkzar .gt_footnote_marks {
  font-style: italic;
  font-weight: normal;
  font-size: 65%;
}
</style>
<table class="gt_table">
  
  <thead class="gt_col_headings">
    <tr>
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1"></th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">AtBat</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">Hits</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">HmRun</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">Runs</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">RBI</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">Walks</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">Years</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">CAtBat</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">CHits</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">CHmRun</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">CRuns</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">CRBI</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">CWalks</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">LeagueN</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">DivisionW</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">PutOuts</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">Assists</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">Errors</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">NewLeagueN</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">r.squared</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">adj.r.squared</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">BIC</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">mallows_cp</th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td class="gt_row gt_left gt_stub">1</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_right">0.3215</td>
<td class="gt_row gt_right">0.3189</td>
<td class="gt_row gt_right">&minus;90.85</td>
<td class="gt_row gt_right">104.3</td></tr>
    <tr><td class="gt_row gt_left gt_stub">2</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_right">0.4252</td>
<td class="gt_row gt_right">0.4208</td>
<td class="gt_row gt_right">&minus;128.9</td>
<td class="gt_row gt_right">50.72</td></tr>
    <tr><td class="gt_row gt_left gt_stub">3</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_right">0.4514</td>
<td class="gt_row gt_right">0.4451</td>
<td class="gt_row gt_right">&minus;135.6</td>
<td class="gt_row gt_right">38.69</td></tr>
    <tr><td class="gt_row gt_left gt_stub">4</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_right">0.4754</td>
<td class="gt_row gt_right">0.4673</td>
<td class="gt_row gt_right">&minus;141.8</td>
<td class="gt_row gt_right">27.86</td></tr>
    <tr><td class="gt_row gt_left gt_stub">5</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_right">0.4908</td>
<td class="gt_row gt_right">0.4809</td>
<td class="gt_row gt_right">&minus;144.1</td>
<td class="gt_row gt_right">21.61</td></tr>
    <tr><td class="gt_row gt_left gt_stub">6</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_right">0.5087</td>
<td class="gt_row gt_right">0.4972</td>
<td class="gt_row gt_right">&minus;147.9</td>
<td class="gt_row gt_right">14.02</td></tr>
    <tr><td class="gt_row gt_left gt_stub">7</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_right">0.5141</td>
<td class="gt_row gt_right">0.5008</td>
<td class="gt_row gt_right">&minus;145.3</td>
<td class="gt_row gt_right">13.13</td></tr>
    <tr><td class="gt_row gt_left gt_stub">8</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #ABDDA4; color: #000000;">TRUE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_center" style="background-color: #FDAE61; color: #000000;">FALSE</td>
<td class="gt_row gt_right">0.5286</td>
<td class="gt_row gt_right">0.5137</td>
<td class="gt_row gt_right">&minus;147.6</td>
<td class="gt_row gt_right">7.401</td></tr>
  </tbody>
  
  
</table>
</div>
<p>Note that, by default, <code>regsubsets()</code> fits models with up to 8 predictors, this can be adjusted with the <code>nvmax</code> argument:</p>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb540-1"><a href="linear-model-selection-and-regularization.html#cb540-1" aria-hidden="true" tabindex="-1"></a>regsubsets_hitters_salary <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> hitters, <span class="at">nvmax =</span> <span class="dv">19</span>)</span></code></pre></div>
<p>To help choose the best model, plot <span class="math inline">\(R^2\)</span>, adjusted <span class="math inline">\(R^2\)</span>, <span class="math inline">\(C_p\)</span> and BIC versus number of predictors for all the models:</p>
<div class="sourceCode" id="cb541"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb541-1"><a href="linear-model-selection-and-regularization.html#cb541-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tidy</span>(regsubsets_hitters_salary) <span class="sc">%&gt;%</span></span>
<span id="cb541-2"><a href="linear-model-selection-and-regularization.html#cb541-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(r.squared<span class="sc">:</span>mallows_cp) <span class="sc">%&gt;%</span></span>
<span id="cb541-3"><a href="linear-model-selection-and-regularization.html#cb541-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">n_vars =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">n</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb541-4"><a href="linear-model-selection-and-regularization.html#cb541-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="sc">-</span>n_vars, <span class="at">names_to =</span> <span class="st">&quot;metric&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb541-5"><a href="linear-model-selection-and-regularization.html#cb541-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> n_vars, <span class="at">y =</span> value)) <span class="sc">+</span></span>
<span id="cb541-6"><a href="linear-model-selection-and-regularization.html#cb541-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb541-7"><a href="linear-model-selection-and-regularization.html#cb541-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb541-8"><a href="linear-model-selection-and-regularization.html#cb541-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(</span>
<span id="cb541-9"><a href="linear-model-selection-and-regularization.html#cb541-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> . <span class="sc">%&gt;%</span></span>
<span id="cb541-10"><a href="linear-model-selection-and-regularization.html#cb541-10" aria-hidden="true" tabindex="-1"></a>      <span class="fu">group_by</span>(metric) <span class="sc">%&gt;%</span></span>
<span id="cb541-11"><a href="linear-model-selection-and-regularization.html#cb541-11" aria-hidden="true" tabindex="-1"></a>      <span class="fu">filter</span>(value <span class="sc">==</span> <span class="fu">ifelse</span>(<span class="fu">str_detect</span>(metric, <span class="st">&quot;r.squared&quot;</span>),</span>
<span id="cb541-12"><a href="linear-model-selection-and-regularization.html#cb541-12" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">max</span>(value), <span class="fu">min</span>(value))),</span>
<span id="cb541-13"><a href="linear-model-selection-and-regularization.html#cb541-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">xintercept =</span> n_vars), <span class="at">size =</span> <span class="dv">1</span></span>
<span id="cb541-14"><a href="linear-model-selection-and-regularization.html#cb541-14" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb541-15"><a href="linear-model-selection-and-regularization.html#cb541-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> metric, <span class="at">scales =</span> <span class="st">&quot;free_y&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-293-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>As expected, <span class="math inline">\(R^2\)</span> increases monotonically.
The best number of variables by adjusted <span class="math inline">\(R^2\)</span>, <span class="math inline">\(C_p\)</span> and BIC are 11, 6 and 10, respectively.
We can use <code>coef()</code> to get the coefficient estimates associated with the model with a specified number of variables:</p>
<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb542-1"><a href="linear-model-selection-and-regularization.html#cb542-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(regsubsets_hitters_salary, <span class="dv">6</span>)</span></code></pre></div>
<pre><code>##  (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW 
##   91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 
##      PutOuts 
##    0.2643076</code></pre>
</div>
<div id="forward-and-backward-stepwise-selection" class="section level4 unnumbered">
<h4>Forward and Backward Stepwise Selection</h4>
<p>Forward and backward stepwise selection can be performed with the <code>method = "forward"</code> and <code>"backward"</code> arguments:</p>
<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb544-1"><a href="linear-model-selection-and-regularization.html#cb544-1" aria-hidden="true" tabindex="-1"></a>regsubsets_hitters_salary_forward <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> hitters,</span>
<span id="cb544-2"><a href="linear-model-selection-and-regularization.html#cb544-2" aria-hidden="true" tabindex="-1"></a>                                                <span class="at">nvmax =</span> <span class="dv">19</span>, <span class="at">method =</span> <span class="st">&quot;forward&quot;</span>)</span>
<span id="cb544-3"><a href="linear-model-selection-and-regularization.html#cb544-3" aria-hidden="true" tabindex="-1"></a>regsubsets_hitters_salary_backward <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> hitters,</span>
<span id="cb544-4"><a href="linear-model-selection-and-regularization.html#cb544-4" aria-hidden="true" tabindex="-1"></a>                                                 <span class="at">nvmax =</span> <span class="dv">19</span>, <span class="at">method =</span> <span class="st">&quot;backward&quot;</span>)</span></code></pre></div>
</div>
<div id="choosing-among-models-using-the-validation-set-approach-and-cross-validation" class="section level4" number="6.5.1.1">
<h4><span class="header-section-number">6.5.1.1</span> Choosing Among Models Using the Validation-Set Approach and Cross-Validation</h4>
<p>Use <code>rsample</code> to perform a 50-50 split into training and testing data:</p>
<div class="sourceCode" id="cb545"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb545-1"><a href="linear-model-selection-and-regularization.html#cb545-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">8</span>)</span>
<span id="cb545-2"><a href="linear-model-selection-and-regularization.html#cb545-2" aria-hidden="true" tabindex="-1"></a>hitters_split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(hitters, <span class="at">prop =</span> <span class="fl">0.5</span>)</span>
<span id="cb545-3"><a href="linear-model-selection-and-regularization.html#cb545-3" aria-hidden="true" tabindex="-1"></a>hitters_train <span class="ot">&lt;-</span> <span class="fu">training</span>(hitters_split)</span>
<span id="cb545-4"><a href="linear-model-selection-and-regularization.html#cb545-4" aria-hidden="true" tabindex="-1"></a>hitters_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(hitters_split)</span>
<span id="cb545-5"><a href="linear-model-selection-and-regularization.html#cb545-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb545-6"><a href="linear-model-selection-and-regularization.html#cb545-6" aria-hidden="true" tabindex="-1"></a>hitters_train_best <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> hitters_train, <span class="at">nvmax =</span> <span class="dv">19</span>)</span></code></pre></div>
<p>I’ll use a different approach to extracting model coefficients from the <code>regsubsets</code> object and getting MSE from the testing set:</p>
<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb546-1"><a href="linear-model-selection-and-regularization.html#cb546-1" aria-hidden="true" tabindex="-1"></a>hitters_test_mse <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">n_vars =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">19</span>) <span class="sc">%&gt;%</span></span>
<span id="cb546-2"><a href="linear-model-selection-and-regularization.html#cb546-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb546-3"><a href="linear-model-selection-and-regularization.html#cb546-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">model_coefs =</span> <span class="fu">map</span>(</span>
<span id="cb546-4"><a href="linear-model-selection-and-regularization.html#cb546-4" aria-hidden="true" tabindex="-1"></a>      n_vars,</span>
<span id="cb546-5"><a href="linear-model-selection-and-regularization.html#cb546-5" aria-hidden="true" tabindex="-1"></a>      <span class="sc">~</span> <span class="fu">names</span>(<span class="fu">coef</span>(hitters_train_best, <span class="at">i =</span> .x)) <span class="sc">%&gt;%</span></span>
<span id="cb546-6"><a href="linear-model-selection-and-regularization.html#cb546-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Annoyingly, need to rename the categorical coefficients</span></span>
<span id="cb546-7"><a href="linear-model-selection-and-regularization.html#cb546-7" aria-hidden="true" tabindex="-1"></a>        <span class="fu">str_replace</span>(<span class="st">&quot;NewLeagueN&quot;</span>, <span class="st">&quot;NewLeague&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb546-8"><a href="linear-model-selection-and-regularization.html#cb546-8" aria-hidden="true" tabindex="-1"></a>        <span class="fu">str_replace</span>(<span class="st">&quot;DivisionW&quot;</span>, <span class="st">&quot;Division&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb546-9"><a href="linear-model-selection-and-regularization.html#cb546-9" aria-hidden="true" tabindex="-1"></a>        <span class="fu">str_replace</span>(<span class="st">&quot;LeagueN&quot;</span>, <span class="st">&quot;League&quot;</span>)</span>
<span id="cb546-10"><a href="linear-model-selection-and-regularization.html#cb546-10" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb546-11"><a href="linear-model-selection-and-regularization.html#cb546-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">model_formula =</span> <span class="fu">map</span>(</span>
<span id="cb546-12"><a href="linear-model-selection-and-regularization.html#cb546-12" aria-hidden="true" tabindex="-1"></a>      model_coefs,</span>
<span id="cb546-13"><a href="linear-model-selection-and-regularization.html#cb546-13" aria-hidden="true" tabindex="-1"></a>      <span class="sc">~</span> <span class="fu">formula</span>(<span class="fu">paste0</span>(<span class="st">&quot;Salary ~ &quot;</span>, <span class="fu">paste</span>(.x[<span class="sc">-</span><span class="dv">1</span>], <span class="at">collapse =</span> <span class="st">&quot;+&quot;</span>)))</span>
<span id="cb546-14"><a href="linear-model-selection-and-regularization.html#cb546-14" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb546-15"><a href="linear-model-selection-and-regularization.html#cb546-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">model_fit =</span> <span class="fu">map</span>(model_formula, <span class="sc">~</span> <span class="fu">lm</span>(.x, <span class="at">data =</span> hitters_test)),</span>
<span id="cb546-16"><a href="linear-model-selection-and-regularization.html#cb546-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">mse =</span> <span class="fu">map_dbl</span>(model_fit, <span class="sc">~</span> <span class="fu">mean</span>(.x<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb546-17"><a href="linear-model-selection-and-regularization.html#cb546-17" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb546-18"><a href="linear-model-selection-and-regularization.html#cb546-18" aria-hidden="true" tabindex="-1"></a>hitters_test_mse <span class="sc">%&gt;%</span></span>
<span id="cb546-19"><a href="linear-model-selection-and-regularization.html#cb546-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(n_vars, model_coefs, mse) <span class="sc">%&gt;%</span></span>
<span id="cb546-20"><a href="linear-model-selection-and-regularization.html#cb546-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gt</span>() <span class="sc">%&gt;%</span></span>
<span id="cb546-21"><a href="linear-model-selection-and-regularization.html#cb546-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data_color</span>(<span class="at">columns =</span> mse,</span>
<span id="cb546-22"><a href="linear-model-selection-and-regularization.html#cb546-22" aria-hidden="true" tabindex="-1"></a>             <span class="at">colors =</span> <span class="fu">c</span>(td_colors<span class="sc">$</span>nice<span class="sc">$</span>charcoal, td_colors<span class="sc">$</span>nice<span class="sc">$</span>lime_green))</span></code></pre></div>
<div id="tpgnuzerro" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#tpgnuzerro .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#tpgnuzerro .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#tpgnuzerro .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#tpgnuzerro .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 6px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#tpgnuzerro .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#tpgnuzerro .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#tpgnuzerro .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#tpgnuzerro .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#tpgnuzerro .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#tpgnuzerro .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#tpgnuzerro .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#tpgnuzerro .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#tpgnuzerro .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#tpgnuzerro .gt_from_md > :first-child {
  margin-top: 0;
}

#tpgnuzerro .gt_from_md > :last-child {
  margin-bottom: 0;
}

#tpgnuzerro .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#tpgnuzerro .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#tpgnuzerro .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#tpgnuzerro .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#tpgnuzerro .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#tpgnuzerro .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#tpgnuzerro .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#tpgnuzerro .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#tpgnuzerro .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#tpgnuzerro .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#tpgnuzerro .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#tpgnuzerro .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#tpgnuzerro .gt_left {
  text-align: left;
}

#tpgnuzerro .gt_center {
  text-align: center;
}

#tpgnuzerro .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#tpgnuzerro .gt_font_normal {
  font-weight: normal;
}

#tpgnuzerro .gt_font_bold {
  font-weight: bold;
}

#tpgnuzerro .gt_font_italic {
  font-style: italic;
}

#tpgnuzerro .gt_super {
  font-size: 65%;
}

#tpgnuzerro .gt_footnote_marks {
  font-style: italic;
  font-weight: normal;
  font-size: 65%;
}
</style>
<table class="gt_table">
  
  <thead class="gt_col_headings">
    <tr>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">n_vars</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">model_coefs</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">mse</th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td class="gt_row gt_right">1</td>
<td class="gt_row gt_center">(Intercept), CRuns</td>
<td class="gt_row gt_right" style="background-color: #ABDDA4; color: #000000;">123433.20</td></tr>
    <tr><td class="gt_row gt_right">2</td>
<td class="gt_row gt_center">(Intercept), Walks, CHits</td>
<td class="gt_row gt_right" style="background-color: #92BE95; color: #000000;">111877.57</td></tr>
    <tr><td class="gt_row gt_right">3</td>
<td class="gt_row gt_center">(Intercept), Walks, CAtBat, CHits</td>
<td class="gt_row gt_right" style="background-color: #85AD8D; color: #000000;">105330.24</td></tr>
    <tr><td class="gt_row gt_right">4</td>
<td class="gt_row gt_center">(Intercept), Walks, CAtBat, CHits, Division</td>
<td class="gt_row gt_right" style="background-color: #7BA187; color: #000000;">100607.93</td></tr>
    <tr><td class="gt_row gt_right">5</td>
<td class="gt_row gt_center">(Intercept), Walks, CAtBat, CHits, Division, PutOuts</td>
<td class="gt_row gt_right" style="background-color: #709380; color: #000000;">95384.83</td></tr>
    <tr><td class="gt_row gt_right">6</td>
<td class="gt_row gt_center">(Intercept), Walks, CAtBat, CHits, Division, PutOuts, Assists</td>
<td class="gt_row gt_right" style="background-color: #6E917F; color: #000000;">94286.58</td></tr>
    <tr><td class="gt_row gt_right">7</td>
<td class="gt_row gt_center">(Intercept), Walks, CAtBat, CHits, CRuns, Division, PutOuts, Assists</td>
<td class="gt_row gt_right" style="background-color: #6D907E; color: #000000;">94187.18</td></tr>
    <tr><td class="gt_row gt_right">8</td>
<td class="gt_row gt_center">(Intercept), Walks, CAtBat, CHits, CRuns, League, Division, PutOuts, Assists</td>
<td class="gt_row gt_right" style="background-color: #6D907E; color: #000000;">94126.34</td></tr>
    <tr><td class="gt_row gt_right">9</td>
<td class="gt_row gt_center">(Intercept), AtBat, HmRun, Walks, CAtBat, CHits, Division, PutOuts, Assists, Errors</td>
<td class="gt_row gt_right" style="background-color: #66877A; color: #FFFFFF;">90507.07</td></tr>
    <tr><td class="gt_row gt_right">10</td>
<td class="gt_row gt_center">(Intercept), AtBat, HmRun, Walks, Years, CAtBat, CHits, Division, PutOuts, Assists, Errors</td>
<td class="gt_row gt_right" style="background-color: #658679; color: #FFFFFF;">90078.74</td></tr>
    <tr><td class="gt_row gt_right">11</td>
<td class="gt_row gt_center">(Intercept), AtBat, HmRun, Walks, CAtBat, CHits, CRuns, Division, PutOuts, Assists, Errors, NewLeague</td>
<td class="gt_row gt_right" style="background-color: #658679; color: #FFFFFF;">90011.36</td></tr>
    <tr><td class="gt_row gt_right">12</td>
<td class="gt_row gt_center">(Intercept), AtBat, Hits, HmRun, Walks, CAtBat, CHits, CRuns, Division, PutOuts, Assists, Errors, NewLeague</td>
<td class="gt_row gt_right" style="background-color: #587671; color: #FFFFFF;">83725.73</td></tr>
    <tr><td class="gt_row gt_right">13</td>
<td class="gt_row gt_center">(Intercept), AtBat, Hits, HmRun, Walks, Years, CAtBat, CHits, CRuns, Division, PutOuts, Assists, Errors, NewLeague</td>
<td class="gt_row gt_right" style="background-color: #587671; color: #FFFFFF;">83645.01</td></tr>
    <tr><td class="gt_row gt_right">14</td>
<td class="gt_row gt_center">(Intercept), AtBat, Hits, HmRun, Walks, Years, CAtBat, CHits, CHmRun, CRuns, Division, PutOuts, Assists, Errors, NewLeague</td>
<td class="gt_row gt_right" style="background-color: #3E5861; color: #FFFFFF;">70781.78</td></tr>
    <tr><td class="gt_row gt_right">15</td>
<td class="gt_row gt_center">(Intercept), AtBat, Hits, HmRun, Walks, Years, CAtBat, CHits, CHmRun, CRuns, League, Division, PutOuts, Assists, Errors, NewLeague</td>
<td class="gt_row gt_right" style="background-color: #3B555F; color: #FFFFFF;">69332.68</td></tr>
    <tr><td class="gt_row gt_right">16</td>
<td class="gt_row gt_center">(Intercept), AtBat, Hits, HmRun, RBI, Walks, Years, CAtBat, CHits, CRuns, CRBI, League, Division, PutOuts, Assists, Errors, NewLeague</td>
<td class="gt_row gt_right" style="background-color: #3E5961; color: #FFFFFF;">71043.96</td></tr>
    <tr><td class="gt_row gt_right">17</td>
<td class="gt_row gt_center">(Intercept), AtBat, Hits, HmRun, RBI, Walks, Years, CAtBat, CHits, CHmRun, CRuns, CWalks, League, Division, PutOuts, Assists, Errors, NewLeague</td>
<td class="gt_row gt_right" style="background-color: #334C5A; color: #FFFFFF;">65515.02</td></tr>
    <tr><td class="gt_row gt_right">18</td>
<td class="gt_row gt_center">(Intercept), AtBat, Hits, HmRun, Runs, RBI, Walks, Years, CAtBat, CHits, CHmRun, CRuns, CWalks, League, Division, PutOuts, Assists, Errors, NewLeague</td>
<td class="gt_row gt_right" style="background-color: #304958; color: #FFFFFF;">64117.21</td></tr>
    <tr><td class="gt_row gt_right">19</td>
<td class="gt_row gt_center">(Intercept), AtBat, Hits, HmRun, Runs, RBI, Walks, Years, CAtBat, CHits, CHmRun, CRuns, CRBI, CWalks, League, Division, PutOuts, Assists, Errors, NewLeague</td>
<td class="gt_row gt_right" style="background-color: #2F4858; color: #FFFFFF;">63726.78</td></tr>
  </tbody>
  
  
</table>
</div>
</div>
</div>
<div id="ridge-regression-and-the-lasso" class="section level3" number="6.5.2">
<h3><span class="header-section-number">6.5.2</span> Ridge Regression and the Lasso</h3>
<div id="ridge-regression-1" class="section level4 unnumbered">
<h4>Ridge Regression</h4>
<p>The <code>glmnet::glmnet()</code> function uses a different syntax for fitting models:</p>
<div class="sourceCode" id="cb547"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb547-1"><a href="linear-model-selection-and-regularization.html#cb547-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary <span class="sc">~</span> ., hitters)[, <span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb547-2"><a href="linear-model-selection-and-regularization.html#cb547-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> hitters<span class="sc">$</span>Salary</span>
<span id="cb547-3"><a href="linear-model-selection-and-regularization.html#cb547-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb547-4"><a href="linear-model-selection-and-regularization.html#cb547-4" aria-hidden="true" tabindex="-1"></a>lambda_grid <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb547-5"><a href="linear-model-selection-and-regularization.html#cb547-5" aria-hidden="true" tabindex="-1"></a>ridge_salary <span class="ot">&lt;-</span> glmnet<span class="sc">::</span><span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambda_grid)</span></code></pre></div>
<p>And the results are also output in a vary particular way.
Here are the coefficients associated with the 50th <span class="math inline">\(\lambda\)</span> value (= 1.149757^{4}) and the 60th (= 705.4802311):</p>
<div class="sourceCode" id="cb548"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb548-1"><a href="linear-model-selection-and-regularization.html#cb548-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(ridge_salary)[, <span class="dv">50</span>]; <span class="fu">coef</span>(ridge_salary)[, <span class="dv">60</span>]</span></code></pre></div>
<pre><code>##   (Intercept)         AtBat          Hits         HmRun          Runs 
## 407.356050200   0.036957182   0.138180344   0.524629976   0.230701523 
##           RBI         Walks         Years        CAtBat         CHits 
##   0.239841459   0.289618741   1.107702929   0.003131815   0.011653637 
##        CHmRun         CRuns          CRBI        CWalks       LeagueN 
##   0.087545670   0.023379882   0.024138320   0.025015421   0.085028114 
##     DivisionW       PutOuts       Assists        Errors    NewLeagueN 
##  -6.215440973   0.016482577   0.002612988  -0.020502690   0.301433531</code></pre>
<pre><code>##  (Intercept)        AtBat         Hits        HmRun         Runs          RBI 
##  54.32519950   0.11211115   0.65622409   1.17980910   0.93769713   0.84718546 
##        Walks        Years       CAtBat        CHits       CHmRun        CRuns 
##   1.31987948   2.59640425   0.01083413   0.04674557   0.33777318   0.09355528 
##         CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists 
##   0.09780402   0.07189612  13.68370191 -54.65877750   0.11852289   0.01606037 
##       Errors   NewLeagueN 
##  -0.70358655   8.61181213</code></pre>
<p>The coefficients in the first model are much smaller compared to the second, as expected.
This can also be seen with the <span class="math inline">\(\mathcal{l}_2\)</span> norm:</p>
<div class="sourceCode" id="cb551"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb551-1"><a href="linear-model-selection-and-regularization.html#cb551-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>(<span class="fu">coef</span>(ridge_salary)[<span class="sc">-</span><span class="dv">1</span>, <span class="dv">50</span>]<span class="sc">^</span><span class="dv">2</span>)); <span class="fu">sqrt</span>(<span class="fu">sum</span>(<span class="fu">coef</span>(ridge_salary)[<span class="sc">-</span><span class="dv">1</span>, <span class="dv">60</span>]<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 6.360612</code></pre>
<pre><code>## [1] 57.11001</code></pre>
<p>Part of the reason I really like <code>tidymodels</code> (and <a href="https://youtu.be/kAZe9UpMx_s?t=235">one of the reasons it was created in the first place</a>) is the consistency.
It provides a unified interface for fitting models from many different packages.
Without ISLR walking me through the steps, it would take me a long time to learn the syntax of <code>glmnet</code> and how to extract the results I want.
The <code>tidymodels</code> implementation of this workflow might be longer, but it is standardized – these are lines of code I’ve written variations of hundreds of times now.</p>
<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb554-1"><a href="linear-model-selection-and-regularization.html#cb554-1" aria-hidden="true" tabindex="-1"></a>ridge_spec <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>(<span class="at">penalty =</span> <span class="dv">0</span>, <span class="at">mixture =</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span></span>
<span id="cb554-2"><a href="linear-model-selection-and-regularization.html#cb554-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>, <span class="at">path_values =</span> lambda_grid)</span>
<span id="cb554-3"><a href="linear-model-selection-and-regularization.html#cb554-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb554-4"><a href="linear-model-selection-and-regularization.html#cb554-4" aria-hidden="true" tabindex="-1"></a>hitters_ridge_fit <span class="ot">&lt;-</span> <span class="fu">fit</span>(ridge_spec, Salary <span class="sc">~</span> ., <span class="at">data =</span> hitters)</span>
<span id="cb554-5"><a href="linear-model-selection-and-regularization.html#cb554-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb554-6"><a href="linear-model-selection-and-regularization.html#cb554-6" aria-hidden="true" tabindex="-1"></a><span class="fu">tidy</span>(hitters_ridge_fit, <span class="at">penalty =</span> lambda_grid[<span class="dv">50</span>])</span></code></pre></div>
<pre><code>## # A tibble: 20 x 3
##    term         estimate penalty
##    &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;
##  1 (Intercept) 407.       11498.
##  2 AtBat         0.0370   11498.
##  3 Hits          0.138    11498.
##  4 HmRun         0.525    11498.
##  5 Runs          0.231    11498.
##  6 RBI           0.240    11498.
##  7 Walks         0.290    11498.
##  8 Years         1.11     11498.
##  9 CAtBat        0.00313  11498.
## 10 CHits         0.0117   11498.
## 11 CHmRun        0.0875   11498.
## 12 CRuns         0.0234   11498.
## 13 CRBI          0.0241   11498.
## 14 CWalks        0.0250   11498.
## 15 LeagueN       0.0850   11498.
## 16 DivisionW    -6.22     11498.
## 17 PutOuts       0.0165   11498.
## 18 Assists       0.00261  11498.
## 19 Errors       -0.0205   11498.
## 20 NewLeagueN    0.301    11498.</code></pre>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb556-1"><a href="linear-model-selection-and-regularization.html#cb556-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tidy</span>(hitters_ridge_fit, <span class="at">penalty =</span> lambda_grid[<span class="dv">60</span>])</span></code></pre></div>
<pre><code>## # A tibble: 20 x 3
##    term        estimate penalty
##    &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;
##  1 (Intercept)  54.3       705.
##  2 AtBat         0.112     705.
##  3 Hits          0.656     705.
##  4 HmRun         1.18      705.
##  5 Runs          0.938     705.
##  6 RBI           0.847     705.
##  7 Walks         1.32      705.
##  8 Years         2.60      705.
##  9 CAtBat        0.0108    705.
## 10 CHits         0.0467    705.
## 11 CHmRun        0.338     705.
## 12 CRuns         0.0936    705.
## 13 CRBI          0.0978    705.
## 14 CWalks        0.0719    705.
## 15 LeagueN      13.7       705.
## 16 DivisionW   -54.7       705.
## 17 PutOuts       0.119     705.
## 18 Assists       0.0161    705.
## 19 Errors       -0.704     705.
## 20 NewLeagueN    8.61      705.</code></pre>
<p>Visualize coefficient estimates:</p>
<div class="sourceCode" id="cb558"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb558-1"><a href="linear-model-selection-and-regularization.html#cb558-1" aria-hidden="true" tabindex="-1"></a><span class="fu">map_dfr</span>(lambda_grid,</span>
<span id="cb558-2"><a href="linear-model-selection-and-regularization.html#cb558-2" aria-hidden="true" tabindex="-1"></a>        <span class="sc">~</span> <span class="fu">tidy</span>(hitters_ridge_fit, <span class="at">penalty =</span> .x)) <span class="sc">%&gt;%</span></span>
<span id="cb558-3"><a href="linear-model-selection-and-regularization.html#cb558-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(term <span class="sc">!=</span> <span class="st">&quot;(Intercept)&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb558-4"><a href="linear-model-selection-and-regularization.html#cb558-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> penalty, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb558-5"><a href="linear-model-selection-and-regularization.html#cb558-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb558-6"><a href="linear-model-selection-and-regularization.html#cb558-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>(<span class="at">breaks =</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">4</span>),</span>
<span id="cb558-7"><a href="linear-model-selection-and-regularization.html#cb558-7" aria-hidden="true" tabindex="-1"></a>                <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;1e-2&quot;</span>, <span class="st">&quot;1e2&quot;</span>, <span class="st">&quot;1e6&quot;</span>, <span class="st">&quot;1e10&quot;</span>)) <span class="sc">+</span></span>
<span id="cb558-8"><a href="linear-model-selection-and-regularization.html#cb558-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> term, <span class="at">scales =</span> <span class="st">&quot;free_y&quot;</span>, <span class="at">ncol =</span> <span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb558-9"><a href="linear-model-selection-and-regularization.html#cb558-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_facet_borders</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-302-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The <code>predict()</code> function also takes a <code>penalty</code> argument:</p>
<div class="sourceCode" id="cb559"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb559-1"><a href="linear-model-selection-and-regularization.html#cb559-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(hitters_ridge_fit, <span class="at">penalty =</span> <span class="dv">50</span>, <span class="at">new_data =</span> hitters)</span></code></pre></div>
<pre><code>## # A tibble: 263 x 1
##     .pred
##     &lt;dbl&gt;
##  1  468. 
##  2  663. 
##  3 1023. 
##  4  506. 
##  5  550. 
##  6  200. 
##  7   79.6
##  8  105. 
##  9  836. 
## 10  864. 
## # ... with 253 more rows</code></pre>
<p>Split the data and fit to the training set:</p>
<div class="sourceCode" id="cb561"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb561-1"><a href="linear-model-selection-and-regularization.html#cb561-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">10</span>)</span>
<span id="cb561-2"><a href="linear-model-selection-and-regularization.html#cb561-2" aria-hidden="true" tabindex="-1"></a>hitters_split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(hitters, <span class="at">prop =</span> <span class="fl">0.5</span>)</span>
<span id="cb561-3"><a href="linear-model-selection-and-regularization.html#cb561-3" aria-hidden="true" tabindex="-1"></a>hitters_train <span class="ot">&lt;-</span> <span class="fu">training</span>(hitters_split)</span>
<span id="cb561-4"><a href="linear-model-selection-and-regularization.html#cb561-4" aria-hidden="true" tabindex="-1"></a>hitters_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(hitters_split)</span>
<span id="cb561-5"><a href="linear-model-selection-and-regularization.html#cb561-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb561-6"><a href="linear-model-selection-and-regularization.html#cb561-6" aria-hidden="true" tabindex="-1"></a>hitters_ridge_fit <span class="ot">&lt;-</span> <span class="fu">fit</span>(ridge_spec, Salary <span class="sc">~</span> ., <span class="at">data =</span> hitters_train)</span>
<span id="cb561-7"><a href="linear-model-selection-and-regularization.html#cb561-7" aria-hidden="true" tabindex="-1"></a><span class="fu">tidy</span>(hitters_ridge_fit, <span class="at">penalty =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## # A tibble: 20 x 3
##    term         estimate penalty
##    &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;
##  1 (Intercept)  -40.8          4
##  2 AtBat         -0.311        4
##  3 Hits           2.60         4
##  4 HmRun         -4.02         4
##  5 Runs          -1.72         4
##  6 RBI            0.609        4
##  7 Walks          7.44         4
##  8 Years         -1.61         4
##  9 CAtBat        -0.0760       4
## 10 CHits          0.239        4
## 11 CHmRun         0.102        4
## 12 CRuns          1.09         4
## 13 CRBI           0.467        4
## 14 CWalks        -0.954        4
## 15 LeagueN      -60.8          4
## 16 DivisionW   -120.           4
## 17 PutOuts        0.124        4
## 18 Assists       -0.133        4
## 19 Errors         0.240        4
## 20 NewLeagueN    55.5          4</code></pre>
<p>Now fit to the testing set and get MSE for <span class="math inline">\(\lambda\)</span> = 4:</p>
<div class="sourceCode" id="cb563"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb563-1"><a href="linear-model-selection-and-regularization.html#cb563-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that `broom::augment()` is not implemented for glmnet models, so need to</span></span>
<span id="cb563-2"><a href="linear-model-selection-and-regularization.html#cb563-2" aria-hidden="true" tabindex="-1"></a><span class="co">#  use `predict()` with `penalty` argument</span></span>
<span id="cb563-3"><a href="linear-model-selection-and-regularization.html#cb563-3" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_cols</span>(</span>
<span id="cb563-4"><a href="linear-model-selection-and-regularization.html#cb563-4" aria-hidden="true" tabindex="-1"></a>  hitters_test,</span>
<span id="cb563-5"><a href="linear-model-selection-and-regularization.html#cb563-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">predict</span>(hitters_ridge_fit, <span class="at">new_data =</span> hitters_test, <span class="at">penalty =</span> <span class="dv">4</span>)</span>
<span id="cb563-6"><a href="linear-model-selection-and-regularization.html#cb563-6" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb563-7"><a href="linear-model-selection-and-regularization.html#cb563-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">mse =</span> <span class="fu">mean</span>((Salary <span class="sc">-</span> .pred)<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##       mse
##     &lt;dbl&gt;
## 1 142801.</code></pre>
<p>Aside: here is a shortcut with <code>yardstick::rmse()</code>:</p>
<div class="sourceCode" id="cb565"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb565-1"><a href="linear-model-selection-and-regularization.html#cb565-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_cols</span>(</span>
<span id="cb565-2"><a href="linear-model-selection-and-regularization.html#cb565-2" aria-hidden="true" tabindex="-1"></a>  hitters_test,</span>
<span id="cb565-3"><a href="linear-model-selection-and-regularization.html#cb565-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">predict</span>(hitters_ridge_fit, <span class="at">new_data =</span> hitters_test, <span class="at">penalty =</span> <span class="dv">4</span>)</span>
<span id="cb565-4"><a href="linear-model-selection-and-regularization.html#cb565-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb565-5"><a href="linear-model-selection-and-regularization.html#cb565-5" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">rmse</span>(Salary, .pred) <span class="sc">%&gt;%</span></span>
<span id="cb565-6"><a href="linear-model-selection-and-regularization.html#cb565-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">mse =</span> .estimate<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 4
##   .metric .estimator .estimate     mse
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;
## 1 rmse    standard        378. 142801.</code></pre>
<p>We should find that the null model (just an intercept, so each test observation is predicted to be the mean of the training) gives the same fit as a ridge regression model with a very large <span class="math inline">\(\lambda\)</span> penalty:</p>
<div class="sourceCode" id="cb567"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb567-1"><a href="linear-model-selection-and-regularization.html#cb567-1" aria-hidden="true" tabindex="-1"></a>hitters_lm_null_fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Salary <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> hitters_train)</span>
<span id="cb567-2"><a href="linear-model-selection-and-regularization.html#cb567-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb567-3"><a href="linear-model-selection-and-regularization.html#cb567-3" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_cols</span>(</span>
<span id="cb567-4"><a href="linear-model-selection-and-regularization.html#cb567-4" aria-hidden="true" tabindex="-1"></a>  hitters_test,</span>
<span id="cb567-5"><a href="linear-model-selection-and-regularization.html#cb567-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">predict</span>(hitters_ridge_fit, <span class="at">new_data =</span> hitters_test, <span class="at">penalty =</span> <span class="fl">1e10</span>),</span>
<span id="cb567-6"><a href="linear-model-selection-and-regularization.html#cb567-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">.lm_pred =</span> <span class="fu">predict</span>(hitters_lm_null_fit, <span class="at">newdata =</span> hitters_test)</span>
<span id="cb567-7"><a href="linear-model-selection-and-regularization.html#cb567-7" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb567-8"><a href="linear-model-selection-and-regularization.html#cb567-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb567-9"><a href="linear-model-selection-and-regularization.html#cb567-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">mse_ridge =</span> <span class="fu">mean</span>((Salary <span class="sc">-</span> .pred)<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb567-10"><a href="linear-model-selection-and-regularization.html#cb567-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">mse_null =</span> <span class="fu">mean</span>((Salary <span class="sc">-</span> .lm_pred)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb567-11"><a href="linear-model-selection-and-regularization.html#cb567-11" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code>## # A tibble: 1 x 2
##   mse_ridge mse_null
##       &lt;dbl&gt;    &lt;dbl&gt;
## 1   202640.  202640.</code></pre>
<p>Likewise, compare a penalty of 0 to least squares regression with all predictors:</p>
<div class="sourceCode" id="cb569"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb569-1"><a href="linear-model-selection-and-regularization.html#cb569-1" aria-hidden="true" tabindex="-1"></a>hitters_lm_fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> hitters_train)</span>
<span id="cb569-2"><a href="linear-model-selection-and-regularization.html#cb569-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb569-3"><a href="linear-model-selection-and-regularization.html#cb569-3" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_cols</span>(</span>
<span id="cb569-4"><a href="linear-model-selection-and-regularization.html#cb569-4" aria-hidden="true" tabindex="-1"></a>  hitters_test,</span>
<span id="cb569-5"><a href="linear-model-selection-and-regularization.html#cb569-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">predict</span>(hitters_ridge_fit, <span class="at">new_data =</span> hitters_test, <span class="at">penalty =</span> <span class="dv">0</span>),</span>
<span id="cb569-6"><a href="linear-model-selection-and-regularization.html#cb569-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">.lm_pred =</span> <span class="fu">predict</span>(hitters_lm_fit, <span class="at">newdata =</span> hitters_test)</span>
<span id="cb569-7"><a href="linear-model-selection-and-regularization.html#cb569-7" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb569-8"><a href="linear-model-selection-and-regularization.html#cb569-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb569-9"><a href="linear-model-selection-and-regularization.html#cb569-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">mse_ridge =</span> <span class="fu">mean</span>((Salary <span class="sc">-</span> .pred)<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb569-10"><a href="linear-model-selection-and-regularization.html#cb569-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">mse_lm =</span> <span class="fu">mean</span>((Salary <span class="sc">-</span> .lm_pred)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb569-11"><a href="linear-model-selection-and-regularization.html#cb569-11" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code>## # A tibble: 1 x 2
##   mse_ridge  mse_lm
##       &lt;dbl&gt;   &lt;dbl&gt;
## 1   145236. 145023.</code></pre>
<p>Close, but not exactly the same MSE.
Reading the footnote in the text (page 279), it appears to be due to a numerical approximation on the part of <code>glmnet()</code>.</p>
<p>Instead of arbitrarily choosing <span class="math inline">\(\lambda\)</span> = 4, let’s use 10-fold cross-validation:</p>
<div class="sourceCode" id="cb571"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb571-1"><a href="linear-model-selection-and-regularization.html#cb571-1" aria-hidden="true" tabindex="-1"></a>hitters_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> hitters_train) <span class="sc">%&gt;%</span></span>
<span id="cb571-2"><a href="linear-model-selection-and-regularization.html#cb571-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_dummy</span>(<span class="fu">all_nominal_predictors</span>())</span>
<span id="cb571-3"><a href="linear-model-selection-and-regularization.html#cb571-3" aria-hidden="true" tabindex="-1"></a>ridge_spec <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>(<span class="at">penalty =</span> <span class="fu">tune</span>(), <span class="at">mixture =</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span></span>
<span id="cb571-4"><a href="linear-model-selection-and-regularization.html#cb571-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>)</span>
<span id="cb571-5"><a href="linear-model-selection-and-regularization.html#cb571-5" aria-hidden="true" tabindex="-1"></a>hitters_ridge_workflow <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb571-6"><a href="linear-model-selection-and-regularization.html#cb571-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(ridge_spec) <span class="sc">%&gt;%</span></span>
<span id="cb571-7"><a href="linear-model-selection-and-regularization.html#cb571-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(hitters_recipe)</span>
<span id="cb571-8"><a href="linear-model-selection-and-regularization.html#cb571-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb571-9"><a href="linear-model-selection-and-regularization.html#cb571-9" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">291</span>)</span>
<span id="cb571-10"><a href="linear-model-selection-and-regularization.html#cb571-10" aria-hidden="true" tabindex="-1"></a>hitters_resamples <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(hitters_train, <span class="at">v =</span> <span class="dv">10</span>)</span>
<span id="cb571-11"><a href="linear-model-selection-and-regularization.html#cb571-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb571-12"><a href="linear-model-selection-and-regularization.html#cb571-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Same values as previously</span></span>
<span id="cb571-13"><a href="linear-model-selection-and-regularization.html#cb571-13" aria-hidden="true" tabindex="-1"></a>lambda_grid <span class="ot">&lt;-</span> <span class="fu">grid_regular</span>(<span class="fu">penalty</span>(<span class="at">range =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">10</span>)), <span class="at">levels =</span> <span class="dv">100</span>)</span>
<span id="cb571-14"><a href="linear-model-selection-and-regularization.html#cb571-14" aria-hidden="true" tabindex="-1"></a>hitters_ridge_tune <span class="ot">&lt;-</span> <span class="fu">tune_grid</span>(</span>
<span id="cb571-15"><a href="linear-model-selection-and-regularization.html#cb571-15" aria-hidden="true" tabindex="-1"></a>  hitters_ridge_workflow, <span class="at">resamples =</span> hitters_resamples,</span>
<span id="cb571-16"><a href="linear-model-selection-and-regularization.html#cb571-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">grid =</span> lambda_grid</span>
<span id="cb571-17"><a href="linear-model-selection-and-regularization.html#cb571-17" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>The warnings here are coming from the <code>rsq</code> metric blowing up at large penalty values, which I can quickly visualize with <code>autoplot()</code>:</p>
<div class="sourceCode" id="cb572"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb572-1"><a href="linear-model-selection-and-regularization.html#cb572-1" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(hitters_ridge_tune)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-310-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The full list of metrics can be collected like this:</p>
<div class="sourceCode" id="cb573"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb573-1"><a href="linear-model-selection-and-regularization.html#cb573-1" aria-hidden="true" tabindex="-1"></a><span class="fu">collect_metrics</span>(hitters_ridge_tune)</span></code></pre></div>
<pre><code>## # A tibble: 200 x 7
##    penalty .metric .estimator    mean     n std_err .config               
##      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 
##  1  0.01   rmse    standard   320.       10 27.9    Preprocessor1_Model001
##  2  0.01   rsq     standard     0.540    10  0.0739 Preprocessor1_Model001
##  3  0.0132 rmse    standard   320.       10 27.9    Preprocessor1_Model002
##  4  0.0132 rsq     standard     0.540    10  0.0739 Preprocessor1_Model002
##  5  0.0175 rmse    standard   320.       10 27.9    Preprocessor1_Model003
##  6  0.0175 rsq     standard     0.540    10  0.0739 Preprocessor1_Model003
##  7  0.0231 rmse    standard   320.       10 27.9    Preprocessor1_Model004
##  8  0.0231 rsq     standard     0.540    10  0.0739 Preprocessor1_Model004
##  9  0.0305 rmse    standard   320.       10 27.9    Preprocessor1_Model005
## 10  0.0305 rsq     standard     0.540    10  0.0739 Preprocessor1_Model005
## # ... with 190 more rows</code></pre>
<p>Or find the best fit directly:</p>
<div class="sourceCode" id="cb575"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb575-1"><a href="linear-model-selection-and-regularization.html#cb575-1" aria-hidden="true" tabindex="-1"></a><span class="fu">select_best</span>(hitters_ridge_tune, <span class="at">metric =</span> <span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 2
##   penalty .config               
##     &lt;dbl&gt; &lt;chr&gt;                 
## 1    534. Preprocessor1_Model040</code></pre>
<p>With this penalty selected, finalize the workflow and perform the final fit:</p>
<div class="sourceCode" id="cb577"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb577-1"><a href="linear-model-selection-and-regularization.html#cb577-1" aria-hidden="true" tabindex="-1"></a>hitters_ridge_workflow_final <span class="ot">&lt;-</span> hitters_ridge_workflow <span class="sc">%&gt;%</span></span>
<span id="cb577-2"><a href="linear-model-selection-and-regularization.html#cb577-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize_workflow</span>(<span class="fu">select_best</span>(hitters_ridge_tune, <span class="at">metric =</span> <span class="st">&quot;rmse&quot;</span>))</span>
<span id="cb577-3"><a href="linear-model-selection-and-regularization.html#cb577-3" aria-hidden="true" tabindex="-1"></a>hitters_ridge_fit_final <span class="ot">&lt;-</span> <span class="fu">fit</span>(hitters_ridge_workflow_final,</span>
<span id="cb577-4"><a href="linear-model-selection-and-regularization.html#cb577-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">data =</span> hitters_train)</span>
<span id="cb577-5"><a href="linear-model-selection-and-regularization.html#cb577-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb577-6"><a href="linear-model-selection-and-regularization.html#cb577-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that `augment()` works here because the fit is finalized, with a single</span></span>
<span id="cb577-7"><a href="linear-model-selection-and-regularization.html#cb577-7" aria-hidden="true" tabindex="-1"></a><span class="co">#  `penalty` value -- previously I had to use `bind_cols(predict(...))`</span></span>
<span id="cb577-8"><a href="linear-model-selection-and-regularization.html#cb577-8" aria-hidden="true" tabindex="-1"></a><span class="fu">augment</span>(hitters_ridge_fit_final, <span class="at">new_data =</span> hitters_test) <span class="sc">%&gt;%</span></span>
<span id="cb577-9"><a href="linear-model-selection-and-regularization.html#cb577-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rmse</span>(Salary, .pred) <span class="sc">%&gt;%</span></span>
<span id="cb577-10"><a href="linear-model-selection-and-regularization.html#cb577-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transmute</span>(<span class="at">rmse =</span> .estimate, <span class="at">mse =</span> rmse<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 2
##    rmse     mse
##   &lt;dbl&gt;   &lt;dbl&gt;
## 1  376. 141380.</code></pre>
<p>It’s pretty marginal, but this is an improvement over the MSE we got using <span class="math inline">\(\lambda\)</span> = 4.
Lastly, here are the coefficient estimates of the final fit, none of which are exactly zero, as expected:</p>
<div class="sourceCode" id="cb579"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb579-1"><a href="linear-model-selection-and-regularization.html#cb579-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tidy</span>(hitters_ridge_fit_final)</span></code></pre></div>
<pre><code>## # A tibble: 20 x 3
##    term        estimate penalty
##    &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;
##  1 (Intercept) -56.0       534.
##  2 AtBat         0.177     534.
##  3 Hits          0.689     534.
##  4 HmRun         0.129     534.
##  5 Runs          1.07      534.
##  6 RBI           0.794     534.
##  7 Walks         1.89      534.
##  8 Years         4.50      534.
##  9 CAtBat        0.0160    534.
## 10 CHits         0.0691    534.
## 11 CHmRun        0.417     534.
## 12 CRuns         0.137     534.
## 13 CRBI          0.133     534.
## 14 CWalks        0.100     534.
## 15 PutOuts       0.0964    534.
## 16 Assists      -0.0226    534.
## 17 Errors       -1.41      534.
## 18 League_N     -2.39      534.
## 19 Division_W  -54.9       534.
## 20 NewLeague_N   8.59      534.</code></pre>
</div>
<div id="the-lasso-1" class="section level4 unnumbered">
<h4>The Lasso</h4>
<p>Before performing cross-validation, plot coefficient estimates for a range of <span class="math inline">\(\lambda\)</span> values:</p>
<div class="sourceCode" id="cb581"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb581-1"><a href="linear-model-selection-and-regularization.html#cb581-1" aria-hidden="true" tabindex="-1"></a>lasso_spec <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>(<span class="at">penalty =</span> <span class="dv">0</span>, <span class="at">mixture =</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span></span>
<span id="cb581-2"><a href="linear-model-selection-and-regularization.html#cb581-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>, <span class="at">path_values =</span> lambda_grid<span class="sc">$</span>penalty)</span>
<span id="cb581-3"><a href="linear-model-selection-and-regularization.html#cb581-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb581-4"><a href="linear-model-selection-and-regularization.html#cb581-4" aria-hidden="true" tabindex="-1"></a>hitters_lasso_fit <span class="ot">&lt;-</span> <span class="fu">fit</span>(lasso_spec, Salary <span class="sc">~</span> ., <span class="at">data =</span> hitters)</span>
<span id="cb581-5"><a href="linear-model-selection-and-regularization.html#cb581-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb581-6"><a href="linear-model-selection-and-regularization.html#cb581-6" aria-hidden="true" tabindex="-1"></a><span class="fu">map_dfr</span>(lambda_grid<span class="sc">$</span>penalty,</span>
<span id="cb581-7"><a href="linear-model-selection-and-regularization.html#cb581-7" aria-hidden="true" tabindex="-1"></a>        <span class="sc">~</span> <span class="fu">tidy</span>(hitters_lasso_fit, <span class="at">penalty =</span> .x)) <span class="sc">%&gt;%</span></span>
<span id="cb581-8"><a href="linear-model-selection-and-regularization.html#cb581-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(term <span class="sc">!=</span> <span class="st">&quot;(Intercept)&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb581-9"><a href="linear-model-selection-and-regularization.html#cb581-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> penalty, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb581-10"><a href="linear-model-selection-and-regularization.html#cb581-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb581-11"><a href="linear-model-selection-and-regularization.html#cb581-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>(<span class="at">breaks =</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">4</span>),</span>
<span id="cb581-12"><a href="linear-model-selection-and-regularization.html#cb581-12" aria-hidden="true" tabindex="-1"></a>                <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;1e-2&quot;</span>, <span class="st">&quot;1e2&quot;</span>, <span class="st">&quot;1e6&quot;</span>, <span class="st">&quot;1e10&quot;</span>)) <span class="sc">+</span></span>
<span id="cb581-13"><a href="linear-model-selection-and-regularization.html#cb581-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> term, <span class="at">scales =</span> <span class="st">&quot;free_y&quot;</span>, <span class="at">ncol =</span> <span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb581-14"><a href="linear-model-selection-and-regularization.html#cb581-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_facet_borders</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-315-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>All of the coefficients quickly go to zero.
Knowing this, I’m going to constrain <code>penalty_grid</code> to a smaller range.</p>
<p>Now follow the same procedure as the ridge regression model to find the best choice of <span class="math inline">\(\lambda\)</span>:</p>
<div class="sourceCode" id="cb582"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb582-1"><a href="linear-model-selection-and-regularization.html#cb582-1" aria-hidden="true" tabindex="-1"></a>lasso_spec <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>(<span class="at">penalty =</span> <span class="fu">tune</span>(), <span class="at">mixture =</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span></span>
<span id="cb582-2"><a href="linear-model-selection-and-regularization.html#cb582-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>)</span>
<span id="cb582-3"><a href="linear-model-selection-and-regularization.html#cb582-3" aria-hidden="true" tabindex="-1"></a>hitters_lasso_workflow <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb582-4"><a href="linear-model-selection-and-regularization.html#cb582-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(lasso_spec) <span class="sc">%&gt;%</span></span>
<span id="cb582-5"><a href="linear-model-selection-and-regularization.html#cb582-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(hitters_recipe)</span>
<span id="cb582-6"><a href="linear-model-selection-and-regularization.html#cb582-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb582-7"><a href="linear-model-selection-and-regularization.html#cb582-7" aria-hidden="true" tabindex="-1"></a>lambda_grid <span class="ot">&lt;-</span> <span class="fu">grid_regular</span>(<span class="fu">penalty</span>(<span class="at">range =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>)), <span class="at">levels =</span> <span class="dv">50</span>)</span>
<span id="cb582-8"><a href="linear-model-selection-and-regularization.html#cb582-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb582-9"><a href="linear-model-selection-and-regularization.html#cb582-9" aria-hidden="true" tabindex="-1"></a>hitters_lasso_tune <span class="ot">&lt;-</span> <span class="fu">tune_grid</span>(</span>
<span id="cb582-10"><a href="linear-model-selection-and-regularization.html#cb582-10" aria-hidden="true" tabindex="-1"></a>  hitters_lasso_workflow, <span class="at">resamples =</span> hitters_resamples,</span>
<span id="cb582-11"><a href="linear-model-selection-and-regularization.html#cb582-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">grid =</span> lambda_grid</span>
<span id="cb582-12"><a href="linear-model-selection-and-regularization.html#cb582-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb582-13"><a href="linear-model-selection-and-regularization.html#cb582-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb582-14"><a href="linear-model-selection-and-regularization.html#cb582-14" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(hitters_lasso_tune)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-316-1.png" width="576" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb583"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb583-1"><a href="linear-model-selection-and-regularization.html#cb583-1" aria-hidden="true" tabindex="-1"></a><span class="fu">select_best</span>(hitters_lasso_tune, <span class="at">metric =</span> <span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 2
##   penalty .config              
##     &lt;dbl&gt; &lt;chr&gt;                
## 1    22.2 Preprocessor1_Model42</code></pre>
<div class="sourceCode" id="cb585"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb585-1"><a href="linear-model-selection-and-regularization.html#cb585-1" aria-hidden="true" tabindex="-1"></a>hitters_lasso_workflow_final <span class="ot">&lt;-</span> hitters_lasso_workflow <span class="sc">%&gt;%</span></span>
<span id="cb585-2"><a href="linear-model-selection-and-regularization.html#cb585-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize_workflow</span>(<span class="fu">select_best</span>(hitters_lasso_tune, <span class="at">metric =</span> <span class="st">&quot;rmse&quot;</span>))</span>
<span id="cb585-3"><a href="linear-model-selection-and-regularization.html#cb585-3" aria-hidden="true" tabindex="-1"></a>hitters_lasso_fit_final <span class="ot">&lt;-</span> <span class="fu">fit</span>(hitters_lasso_workflow_final,</span>
<span id="cb585-4"><a href="linear-model-selection-and-regularization.html#cb585-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">data =</span> hitters_train)</span>
<span id="cb585-5"><a href="linear-model-selection-and-regularization.html#cb585-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb585-6"><a href="linear-model-selection-and-regularization.html#cb585-6" aria-hidden="true" tabindex="-1"></a><span class="fu">augment</span>(hitters_lasso_fit_final, <span class="at">new_data =</span> hitters_test) <span class="sc">%&gt;%</span></span>
<span id="cb585-7"><a href="linear-model-selection-and-regularization.html#cb585-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rmse</span>(Salary, .pred) <span class="sc">%&gt;%</span></span>
<span id="cb585-8"><a href="linear-model-selection-and-regularization.html#cb585-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transmute</span>(<span class="at">rmse =</span> .estimate, <span class="at">mse =</span> rmse<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 2
##    rmse     mse
##   &lt;dbl&gt;   &lt;dbl&gt;
## 1  377. 142129.</code></pre>
<p>This is essentially the same performance as the ridge regression model.
The advantage is that the lasso has performed variable selection for us bty setting some to exactly zero:</p>
<div class="sourceCode" id="cb587"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb587-1"><a href="linear-model-selection-and-regularization.html#cb587-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tidy</span>(hitters_lasso_fit_final) <span class="sc">%&gt;%</span></span>
<span id="cb587-2"><a href="linear-model-selection-and-regularization.html#cb587-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(estimate)</span></code></pre></div>
<pre><code>## # A tibble: 20 x 3
##    term        estimate penalty
##    &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;
##  1 Division_W  -73.0       22.2
##  2 (Intercept) -59.9       22.2
##  3 AtBat         0         22.2
##  4 HmRun         0         22.2
##  5 Runs          0         22.2
##  6 RBI           0         22.2
##  7 Years         0         22.2
##  8 CAtBat        0         22.2
##  9 CHits         0         22.2
## 10 CHmRun        0         22.2
## 11 CWalks        0         22.2
## 12 Assists       0         22.2
## 13 Errors        0         22.2
## 14 League_N      0         22.2
## 15 NewLeague_N   0         22.2
## 16 PutOuts       0.0979    22.2
## 17 CRBI          0.407     22.2
## 18 CRuns         0.416     22.2
## 19 Hits          1.68      22.2
## 20 Walks         3.36      22.2</code></pre>
</div>
</div>
<div id="pcr-and-pls-regression" class="section level3" number="6.5.3">
<h3><span class="header-section-number">6.5.3</span> PCR and PLS Regression</h3>
</div>
</div>
<div id="exercises-3" class="section level2" number="6.6">
<h2><span class="header-section-number">6.6</span> Exercises</h2>
</div>
<div id="reproducibility-3" class="section level2 unnumbered">
<h2>Reproducibility</h2>
<div class="sourceCode" id="cb589"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb589-1"><a href="linear-model-selection-and-regularization.html#cb589-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Sys.time</span>()</span></code></pre></div>
<pre><code>## [1] &quot;2022-04-04 10:10:51 AST&quot;</code></pre>
<div class="sourceCode" id="cb591"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb591-1"><a href="linear-model-selection-and-regularization.html#cb591-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="st">&quot;git2r&quot;</span> <span class="sc">%in%</span> <span class="fu">installed.packages</span>()) {</span>
<span id="cb591-2"><a href="linear-model-selection-and-regularization.html#cb591-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (git2r<span class="sc">::</span><span class="fu">in_repository</span>()) {</span>
<span id="cb591-3"><a href="linear-model-selection-and-regularization.html#cb591-3" aria-hidden="true" tabindex="-1"></a>    git2r<span class="sc">::</span><span class="fu">repository</span>()</span>
<span id="cb591-4"><a href="linear-model-selection-and-regularization.html#cb591-4" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb591-5"><a href="linear-model-selection-and-regularization.html#cb591-5" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## Local:    main C:/Users/tdunn/Documents/learning/islr-tidy
## Remote:   main @ origin (https://github.com/taylordunn/islr-tidy)
## Head:     [43905e7] 2022-04-04: Finished section 6.5.2</code></pre>
<div class="sourceCode" id="cb593"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb593-1"><a href="linear-model-selection-and-regularization.html#cb593-1" aria-hidden="true" tabindex="-1"></a>sessioninfo<span class="sc">::</span><span class="fu">session_info</span>()</span></code></pre></div>
<pre><code>## - Session info ---------------------------------------------------------------
##  setting  value                       
##  version  R version 4.1.3 (2022-03-10)
##  os       Windows 10 x64              
##  system   x86_64, mingw32             
##  ui       RTerm                       
##  language (EN)                        
##  collate  English_Canada.1252         
##  ctype    English_Canada.1252         
##  tz       America/Curacao             
##  date     2022-04-04                  
## 
## - Packages -------------------------------------------------------------------
##  package        * version    date       lib source                           
##  abind            1.4-5      2016-07-21 [1] CRAN (R 4.1.1)                   
##  assertthat       0.2.1      2019-03-21 [1] CRAN (R 4.1.0)                   
##  backports        1.2.1      2020-12-09 [1] CRAN (R 4.1.0)                   
##  base64enc        0.1-3      2015-07-28 [1] CRAN (R 4.1.0)                   
##  bayestestR       0.10.5     2021-07-26 [1] CRAN (R 4.1.0)                   
##  bit              4.0.4      2020-08-04 [1] CRAN (R 4.1.2)                   
##  bit64            4.0.5      2020-08-30 [1] CRAN (R 4.1.2)                   
##  bookdown         0.24       2021-09-02 [1] CRAN (R 4.1.1)                   
##  boot             1.3-28     2021-05-03 [2] CRAN (R 4.1.3)                   
##  broom          * 0.7.10     2021-10-31 [1] CRAN (R 4.1.2)                   
##  bslib            0.2.5.1    2021-05-18 [1] CRAN (R 4.1.0)                   
##  cachem           1.0.6      2021-08-19 [1] CRAN (R 4.1.1)                   
##  car              3.0-12     2021-11-06 [1] CRAN (R 4.1.2)                   
##  carData          3.0-4      2020-05-22 [1] CRAN (R 4.1.1)                   
##  cellranger       1.1.0      2016-07-27 [1] CRAN (R 4.1.0)                   
##  checkmate        2.0.0      2020-02-06 [1] CRAN (R 4.1.0)                   
##  class            7.3-20     2022-01-16 [2] CRAN (R 4.1.3)                   
##  cli              3.1.1      2022-01-20 [1] CRAN (R 4.1.2)                   
##  coda             0.19-4     2020-09-30 [1] CRAN (R 4.1.0)                   
##  codetools        0.2-18     2020-11-04 [2] CRAN (R 4.1.3)                   
##  colorspace       2.0-2      2021-06-24 [1] CRAN (R 4.1.0)                   
##  combinat         0.0-8      2012-10-29 [1] CRAN (R 4.1.1)                   
##  corrr          * 0.4.3      2020-11-24 [1] CRAN (R 4.1.0)                   
##  crayon           1.4.2      2021-10-29 [1] CRAN (R 4.1.1)                   
##  datawizard       0.1.0      2021-06-18 [1] CRAN (R 4.1.0)                   
##  DBI              1.1.2      2021-12-20 [1] CRAN (R 4.1.2)                   
##  dbplyr           2.1.1      2021-04-06 [1] CRAN (R 4.1.0)                   
##  DEoptimR         1.0-9      2021-05-24 [1] CRAN (R 4.1.0)                   
##  dials          * 0.0.10     2021-09-10 [1] CRAN (R 4.1.1)                   
##  DiceDesign       1.9        2021-02-13 [1] CRAN (R 4.1.0)                   
##  digest           0.6.29     2021-12-01 [1] CRAN (R 4.1.2)                   
##  discrim        * 0.1.3      2021-07-21 [1] CRAN (R 4.1.2)                   
##  distill          1.3        2021-10-13 [1] CRAN (R 4.1.2)                   
##  distributional   0.2.2      2021-02-02 [1] CRAN (R 4.1.2)                   
##  doParallel     * 1.0.16     2020-10-16 [1] CRAN (R 4.1.1)                   
##  downlit          0.4.0      2021-10-29 [1] CRAN (R 4.1.1)                   
##  dplyr          * 1.0.7      2021-06-18 [1] CRAN (R 4.1.0)                   
##  dunnr          * 0.2.5      2022-01-15 [1] Github (taylordunn/dunnr@c83b30e)
##  effectsize       0.4.5      2021-05-25 [1] CRAN (R 4.1.0)                   
##  ellipsis         0.3.2      2021-04-29 [1] CRAN (R 4.1.0)                   
##  emmeans          1.7.0      2021-09-29 [1] CRAN (R 4.1.2)                   
##  equatiomatic     0.2.0      2021-01-30 [1] CRAN (R 4.1.0)                   
##  estimability     1.3        2018-02-11 [1] CRAN (R 4.1.1)                   
##  evaluate         0.14       2019-05-28 [1] CRAN (R 4.1.0)                   
##  extrafont        0.17       2014-12-08 [1] CRAN (R 4.1.0)                   
##  extrafontdb      1.0        2012-06-11 [1] CRAN (R 4.1.0)                   
##  fansi            1.0.2      2022-01-14 [1] CRAN (R 4.1.2)                   
##  farver           2.1.0      2021-02-28 [1] CRAN (R 4.1.0)                   
##  fastmap          1.1.0      2021-01-25 [1] CRAN (R 4.1.0)                   
##  forcats        * 0.5.1      2021-01-27 [1] CRAN (R 4.1.0)                   
##  foreach        * 1.5.1      2020-10-15 [1] CRAN (R 4.1.0)                   
##  fs               1.5.2      2021-12-08 [1] CRAN (R 4.1.2)                   
##  furrr            0.2.3      2021-06-25 [1] CRAN (R 4.1.2)                   
##  future           1.23.0     2021-10-31 [1] CRAN (R 4.1.2)                   
##  generics         0.1.1      2021-10-25 [1] CRAN (R 4.1.1)                   
##  GGally           2.1.2      2021-06-21 [1] CRAN (R 4.1.0)                   
##  ggdist         * 3.0.0      2021-07-19 [1] CRAN (R 4.1.2)                   
##  ggplot2        * 3.3.5      2021-06-25 [1] CRAN (R 4.1.0)                   
##  ggrepel          0.9.1      2021-01-15 [1] CRAN (R 4.1.0)                   
##  ggridges         0.5.3      2021-01-08 [1] CRAN (R 4.1.0)                   
##  git2r            0.28.0     2021-01-10 [1] CRAN (R 4.1.0)                   
##  glmnet         * 4.1-3      2021-11-02 [1] CRAN (R 4.1.1)                   
##  globals          0.14.0     2020-11-22 [1] CRAN (R 4.1.0)                   
##  glue             1.6.0      2021-12-17 [1] CRAN (R 4.1.2)                   
##  gower            0.2.2      2020-06-23 [1] CRAN (R 4.1.0)                   
##  GPfit            1.0-8      2019-02-08 [1] CRAN (R 4.1.0)                   
##  gridExtra        2.3        2017-09-09 [1] CRAN (R 4.1.0)                   
##  gt             * 0.3.1      2021-08-07 [1] CRAN (R 4.1.2)                   
##  gtable           0.3.0      2019-03-25 [1] CRAN (R 4.1.0)                   
##  hardhat          0.1.6      2021-07-14 [1] CRAN (R 4.1.0)                   
##  haven            2.4.1      2021-04-23 [1] CRAN (R 4.1.0)                   
##  here           * 1.0.1      2020-12-13 [1] CRAN (R 4.1.0)                   
##  highr            0.9        2021-04-16 [1] CRAN (R 4.1.0)                   
##  hms              1.1.1      2021-09-26 [1] CRAN (R 4.1.2)                   
##  htmltools        0.5.2      2021-08-25 [1] CRAN (R 4.1.1)                   
##  httpuv           1.6.5      2022-01-05 [1] CRAN (R 4.1.2)                   
##  httr             1.4.2      2020-07-20 [1] CRAN (R 4.1.0)                   
##  igraph           1.2.6      2020-10-06 [1] CRAN (R 4.1.0)                   
##  infer          * 1.0.0      2021-08-13 [1] CRAN (R 4.1.1)                   
##  insight          0.14.2     2021-06-22 [1] CRAN (R 4.1.0)                   
##  ipred            0.9-12     2021-09-15 [1] CRAN (R 4.1.1)                   
##  ISLR2          * 1.3-1      2022-01-10 [1] CRAN (R 4.1.2)                   
##  iterators      * 1.0.13     2020-10-15 [1] CRAN (R 4.1.0)                   
##  jquerylib        0.1.4      2021-04-26 [1] CRAN (R 4.1.0)                   
##  jsonlite         1.7.3      2022-01-17 [1] CRAN (R 4.1.2)                   
##  kknn             1.3.1      2016-03-26 [1] CRAN (R 4.1.2)                   
##  klaR             0.6-15     2020-02-19 [1] CRAN (R 4.1.2)                   
##  knitr            1.37       2021-12-16 [1] CRAN (R 4.1.2)                   
##  labeling         0.4.2      2020-10-20 [1] CRAN (R 4.1.0)                   
##  labelled         2.8.0      2021-03-08 [1] CRAN (R 4.1.0)                   
##  later            1.3.0      2021-08-18 [1] CRAN (R 4.1.2)                   
##  lattice          0.20-45    2021-09-22 [2] CRAN (R 4.1.3)                   
##  lava             1.6.9      2021-03-11 [1] CRAN (R 4.1.0)                   
##  leaps          * 3.1        2020-01-16 [1] CRAN (R 4.1.3)                   
##  lhs              1.1.1      2020-10-05 [1] CRAN (R 4.1.0)                   
##  lifecycle        1.0.1      2021-09-24 [1] CRAN (R 4.1.1)                   
##  listenv          0.8.0      2019-12-05 [1] CRAN (R 4.1.0)                   
##  lubridate        1.8.0      2021-10-07 [1] CRAN (R 4.1.1)                   
##  magrittr         2.0.1      2020-11-17 [1] CRAN (R 4.1.0)                   
##  MASS             7.3-55     2022-01-16 [2] CRAN (R 4.1.3)                   
##  Matrix         * 1.4-0      2021-12-08 [2] CRAN (R 4.1.3)                   
##  memoise          2.0.1      2021-11-26 [1] CRAN (R 4.1.2)                   
##  MetBrewer        0.1.0      2022-01-05 [1] CRAN (R 4.1.2)                   
##  mgcv             1.8-39     2022-02-24 [2] CRAN (R 4.1.3)                   
##  mime             0.12       2021-09-28 [1] CRAN (R 4.1.1)                   
##  miniUI           0.1.1.1    2018-05-18 [1] CRAN (R 4.1.1)                   
##  modeldata      * 0.1.1      2021-07-14 [1] CRAN (R 4.1.0)                   
##  modelr           0.1.8      2020-05-19 [1] CRAN (R 4.1.0)                   
##  munsell          0.5.0      2018-06-12 [1] CRAN (R 4.1.0)                   
##  mvtnorm        * 1.1-3      2021-10-08 [1] CRAN (R 4.1.1)                   
##  nlme             3.1-155    2022-01-16 [2] CRAN (R 4.1.3)                   
##  nnet             7.3-17     2022-01-16 [2] CRAN (R 4.1.3)                   
##  parallelly       1.27.0     2021-07-19 [1] CRAN (R 4.1.0)                   
##  parameters       0.14.0     2021-05-29 [1] CRAN (R 4.1.0)                   
##  parsnip        * 0.1.7      2021-07-21 [1] CRAN (R 4.1.0)                   
##  patchwork      * 1.1.1      2020-12-17 [1] CRAN (R 4.1.0)                   
##  performance      0.7.3      2021-07-21 [1] CRAN (R 4.1.1)                   
##  pillar           1.7.0      2022-02-01 [1] CRAN (R 4.1.2)                   
##  pkgconfig        2.0.3      2019-09-22 [1] CRAN (R 4.1.0)                   
##  plyr             1.8.6      2020-03-03 [1] CRAN (R 4.1.0)                   
##  poissonreg     * 0.1.1      2021-08-07 [1] CRAN (R 4.1.2)                   
##  prettyunits      1.1.1      2020-01-24 [1] CRAN (R 4.1.0)                   
##  pROC             1.17.0.1   2021-01-13 [1] CRAN (R 4.1.0)                   
##  prodlim          2019.11.13 2019-11-17 [1] CRAN (R 4.1.0)                   
##  promises         1.2.0.1    2021-02-11 [1] CRAN (R 4.1.0)                   
##  purrr          * 0.3.4      2020-04-17 [1] CRAN (R 4.1.2)                   
##  qqplotr          0.0.5      2021-04-23 [1] CRAN (R 4.1.0)                   
##  questionr        0.7.5      2021-10-06 [1] CRAN (R 4.1.2)                   
##  R6               2.5.1      2021-08-19 [1] CRAN (R 4.1.1)                   
##  RColorBrewer     1.1-2      2014-12-07 [1] CRAN (R 4.1.0)                   
##  Rcpp             1.0.8      2022-01-13 [1] CRAN (R 4.1.2)                   
##  readr          * 2.1.1      2021-11-30 [1] CRAN (R 4.1.2)                   
##  readxl           1.3.1      2019-03-13 [1] CRAN (R 4.1.0)                   
##  recipes        * 0.1.17     2021-09-27 [1] CRAN (R 4.1.1)                   
##  repr             1.1.3      2021-01-21 [1] CRAN (R 4.1.1)                   
##  reprex           2.0.0      2021-04-02 [1] CRAN (R 4.1.0)                   
##  reshape          0.8.8      2018-10-23 [1] CRAN (R 4.1.0)                   
##  rlang          * 0.4.12     2021-10-18 [1] CRAN (R 4.1.1)                   
##  rmarkdown        2.11       2021-09-14 [1] CRAN (R 4.1.1)                   
##  robustbase       0.93-8     2021-06-02 [1] CRAN (R 4.1.0)                   
##  rpart            4.1.16     2022-01-24 [2] CRAN (R 4.1.3)                   
##  rprojroot        2.0.2      2020-11-15 [1] CRAN (R 4.1.0)                   
##  rsample        * 0.1.0      2021-05-08 [1] CRAN (R 4.1.0)                   
##  rstudioapi       0.13       2020-11-12 [1] CRAN (R 4.1.0)                   
##  Rttf2pt1         1.3.8      2020-01-10 [1] CRAN (R 4.1.1)                   
##  rvest            1.0.0      2021-03-09 [1] CRAN (R 4.1.0)                   
##  sass             0.4.0      2021-05-12 [1] CRAN (R 4.1.0)                   
##  scales         * 1.1.1      2020-05-11 [1] CRAN (R 4.1.0)                   
##  see              0.6.4      2021-05-29 [1] CRAN (R 4.1.0)                   
##  sessioninfo      1.1.1      2018-11-05 [1] CRAN (R 4.1.0)                   
##  shape            1.4.6      2021-05-19 [1] CRAN (R 4.1.1)                   
##  shiny            1.6.0      2021-01-25 [1] CRAN (R 4.1.0)                   
##  skimr            2.1.3      2021-03-07 [1] CRAN (R 4.1.1)                   
##  stringi          1.7.6      2021-11-29 [1] CRAN (R 4.1.2)                   
##  stringr        * 1.4.0      2019-02-10 [1] CRAN (R 4.1.0)                   
##  survival         3.2-13     2021-08-24 [2] CRAN (R 4.1.3)                   
##  tibble         * 3.1.6      2021-11-07 [1] CRAN (R 4.1.1)                   
##  tictoc         * 1.0.1      2021-04-19 [1] CRAN (R 4.1.1)                   
##  tidymodels     * 0.1.4      2021-10-01 [1] CRAN (R 4.1.1)                   
##  tidyr          * 1.1.4      2021-09-27 [1] CRAN (R 4.1.1)                   
##  tidyselect       1.1.1      2021-04-30 [1] CRAN (R 4.1.0)                   
##  tidyverse      * 1.3.1      2021-04-15 [1] CRAN (R 4.1.2)                   
##  timeDate         3043.102   2018-02-21 [1] CRAN (R 4.1.0)                   
##  tune           * 0.1.6      2021-07-21 [1] CRAN (R 4.1.0)                   
##  tzdb             0.2.0      2021-10-27 [1] CRAN (R 4.1.2)                   
##  usethis          2.1.5      2021-12-09 [1] CRAN (R 4.1.2)                   
##  utf8             1.2.2      2021-07-24 [1] CRAN (R 4.1.0)                   
##  vctrs          * 0.3.8      2021-04-29 [1] CRAN (R 4.1.0)                   
##  vroom            1.5.7      2021-11-30 [1] CRAN (R 4.1.2)                   
##  withr            2.4.3      2021-11-30 [1] CRAN (R 4.1.2)                   
##  workflows      * 0.2.3      2021-07-16 [1] CRAN (R 4.1.0)                   
##  workflowsets   * 0.1.0      2021-07-22 [1] CRAN (R 4.1.0)                   
##  xfun             0.29       2021-12-14 [1] CRAN (R 4.1.2)                   
##  xml2             1.3.3      2021-11-30 [1] CRAN (R 4.1.2)                   
##  xtable           1.8-4      2019-04-21 [1] CRAN (R 4.1.0)                   
##  yaml             2.2.1      2020-02-01 [1] CRAN (R 4.1.0)                   
##  yardstick      * 0.0.8      2021-03-28 [1] CRAN (R 4.1.0)                   
## 
## [1] C:/Users/tdunn/Documents/R/win-library/4.1
## [2] C:/Program Files/R/R-4.1.3/library</code></pre>

<div id="refs" class="references csl-bib-body hanging-indent">
<div class="csl-entry">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em><span class="nocase">The Elements of Statistical Learning</span></em>. Springer Series in Statistics. New York, NY: Springer New York. <a href="https://doi.org/10.1007/978-0-387-84858-7">https://doi.org/10.1007/978-0-387-84858-7</a>.
</div>
<div class="csl-entry">
Smith, Gary. 2018. <span>“<span class="nocase">Step away from stepwise</span>.”</span> <em>Journal of Big Data</em> 5 (1): 32. <a href="https://doi.org/10.1186/s40537-018-0143-6">https://doi.org/10.1186/s40537-018-0143-6</a>.
</div>
</div>
</div>
</div>








<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Smith2018" class="csl-entry">
Smith, Gary. 2018. <span>“<span class="nocase">Step away from stepwise</span>.”</span> <em>Journal of Big Data</em> 5 (1): 32. <a href="https://doi.org/10.1186/s40537-018-0143-6">https://doi.org/10.1186/s40537-018-0143-6</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="resampling-methods.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
