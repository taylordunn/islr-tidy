```{r echo=FALSE}
knitr::opts_chunk$set(fig.retina = 2.5, fig.align = "center",
                      fig.width = 6, fig.height = 3,
                      cache.path = "cache/")
```

# Moving Beyond Linearity

Load the usual packages:

```{r message=FALSE}
library(tidyverse)
library(tidymodels)
library(broom)
library(gt)
library(patchwork)
library(tictoc)

# Load my R package and set the ggplot theme
library(dunnr)
extrafont::loadfonts(device = "win", quiet = TRUE)
theme_set(theme_td())
set_geom_fonts()
set_palette()
```

In the previous chapter, we saw how to improve upon standard least squares linear regression using ridge regression, the lasso, PCA, and other techniques.
In that setting, the complexity of the linear model is reduced to reduce the variance of the estimates.
In this chapter, we relax the linearity assumption while still trying to maintain some interpretability, with these methods:

* *Polynomial regression* extends the linear model by adding extra predictors by raising original predictors to a power.
* *Step functions* cut the range of a variable into $K$ distinct regions to produce a qualitative variable.
* *Regression splines* are a flexible combination of polynomials and step functions that involve polynomial functions fit to data in $K$ distinct regions.
* *Smoothing splines* are similar to regression splines but involve a smoothness penalty.
* *Local regression* is similar to splines but allows smooth overlaps across regions.
* *Generalized additive models* allows us to extend the above methods to deal with multiple predictors.

## Polynomial Regression {#polynomial-regression}

Polynomial regression involves raising one or more predictors to a degree $d$, each with its own coefficient:

$$
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \dots + \beta_d x_i^d + \epsilon_i.
$$

For a large enough degree $d$, polynomial regression can produce an extremely non-linear curve, though it is unusual to use $d$ greater than 3 or 4 because the curve can become overly flexible and produce some strange shapes.

Re-create Figure 7.1 with the `wage` data:

```{r figure7-1, warning=FALSE}
wage <- ISLR2::Wage

wage_poly_4_linear_fit <- lm(wage ~ poly(age, 4), data = wage)
wage_poly_4_logistic_fit <- glm(I(wage > 250) ~ poly(age, 4),
                                data = wage, family = binomial)

# Grid of age values for predictions
age_grid <- seq(18, 80, length.out = 63)

p1 <- wage_poly_4_linear_fit %>%
  augment(newdata = tibble(age = age_grid), interval = "confidence") %>%
  ggplot(aes(x = age)) +
  geom_point(data = wage, aes(y = wage),
             shape = 21, color = "grey50", alpha = 0.5) +
  geom_line(aes(y = .fitted), color = "darkblue", size = 1.5) +
  geom_line(aes(y = .lower),
            lty = 2, color = "darkblue", size = 1) +
  geom_line(aes(y = .upper),
            lty = 2, color = "darkblue", size = 1) +
  labs(y = "wage")

p2 <- wage_poly_4_logistic_fit %>%
  augment(newdata = tibble(age = age_grid),
          type.predict = "link", se_fit = TRUE) %>%
  mutate(
    # Have to compute CIs manually for logistic regression
    .lower = .fitted - 1.96 * .se.fit,
    .upper = .fitted + 1.96 * .se.fit,
    # Convert from log-odds to probability scales
    across(c(.fitted, .lower, .upper), ~ exp(.x) / (1 + exp(.x)))
  ) %>%
  ggplot(aes(x = age)) +
  geom_rug(data = wage %>% filter(wage > 250),
           sides = "t", color = "grey50", alpha = 0.5) +
  geom_rug(data = wage %>% filter(wage <= 250),
           sides = "b", color = "grey50", alpha = 0.5) +
  geom_line(aes(y = .fitted), color = "darkblue", size = 1.5) +
  geom_line(aes(y = .lower), lty = 2, color = "darkblue", size = 1) +
  geom_line(aes(y = .upper), lty = 2, color = "darkblue", size = 1) +
  coord_cartesian(ylim = c(0, 0.2)) +
  labs(y = "Pr(wage > 250 | age)")

p1 + p2
```

## Step Functions {#step-functions}

>
Using polynomial functions of the features as predictors in a linear model
imposes a global structure on the non-linear function of $X$. We can instead
use step functions in order to avoid imposing such a global structure. Here
we break the range of $X$ into *bins*, and fit a different constant in each bin.
This amounts to converting a continuous variable into an *ordered categorical variable*.

This involves using a set of $K$ cutpoints $c_k$ which corresponds to dummy variables $C_k(X)$:

$$
\begin{align}
C_0(X) &= I(X < c_1), \\
C_1(X) &= I(c_1 \leq X < c_2), \\
C_2(X) &= I(c_2 \leq X < c_3), \\
&\vdots \\
C_{K-1}(X) &= I(c_{K-1} \leq X < c_K), \\
C_K(X) &= I(c_K \leq X),
\end{align}
$$

where $I()$ is an *indicator* function that returns a 1 or 0 if the condition is true or false.
The least squares linear model is then:

$$
y_i = \beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2 (x_i) + \dots + \beta_K C_K (x_i) + \epsilon_i.
$$

To re-create Figure 7.2, I'll use `cut` with 4 breaks to separate the `age` predictor:

```{r figure7-2, warning=FALSE}
wage_step_linear_fit <- lm(wage ~ cut(age, breaks = 4), data = wage)
wage_step_logistic_fit <- glm(I(wage > 250) ~ cut(age, breaks = 4),
                                data = wage, family = binomial)
p1 <- wage_step_linear_fit %>%
  augment(newdata = tibble(age = age_grid),
          interval = "confidence", level = 0.50) %>%
  ggplot(aes(x = age)) +
  geom_point(data = wage, aes(y = wage),
             shape = 21, color = "grey50", alpha = 0.5) +
  geom_line(aes(y = .fitted), color = "darkgreen", size = 1.5) +
  geom_line(aes(y = .lower),
            lty = 2, color = "darkgreen", size = 1) +
  geom_line(aes(y = .upper),
            lty = 2, color = "darkgreen", size = 1) +
  labs(y = "wage")

p2 <- wage_step_logistic_fit %>%
  augment(newdata = tibble(age = age_grid),
          type.predict = "link", se_fit = TRUE) %>%
  mutate(
    .lower = .fitted - 1.96 * .se.fit,
    .upper = .fitted + 1.96 * .se.fit,
    across(c(.fitted, .lower, .upper), ~ exp(.x) / (1 + exp(.x)))
  ) %>%
  ggplot(aes(x = age)) +
  geom_rug(data = wage %>% filter(wage > 250),
           sides = "t", color = "grey50", alpha = 0.5) +
  geom_rug(data = wage %>% filter(wage <= 250),
           sides = "b", color = "grey50", alpha = 0.5) +
  geom_line(aes(y = .fitted), color = "darkgreen", size = 1.5) +
  geom_line(aes(y = .lower), lty = 2, color = "darkgreen", size = 1) +
  geom_line(aes(y = .upper), lty = 2, color = "darkgreen", size = 1) +
  coord_cartesian(ylim = c(0, 0.2)) +
  labs(y = "Pr(wage > 250 | age)")

p1 + p2
```

>
Unfortunately, unless there are natural breakpoints in the predictors,
piecewise-constant functions can miss the action. For example, in the left- hand
panel of Figure 7.2, the first bin clearly misses the increasing trend
of `wage` with `age`. Nevertheless, step function approaches are very popular
in biostatistics and epidemiology, among other disciplines. For example,
5-year age groups are often used to define the bins.

## Basis Functions

>
Polynomial and piecewise-constant regression models are in fact special
cases of a basis function approach. The idea is to have at hand a fam ily of
functions or transformations that can be applied to a variable $X$:
$b_1(X), b_2(X), \dots, b_K(X)$. Instead of fitting a linear model in $X$, we fit the
model

$$
y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2 (x_i) + \dots + \beta_K b_K (x_i) + \epsilon_i.
$$

In polynomial regression, these basis functions were $b_j(x_i) = x_i^j$.
In piecewise-constant regression, they were $b_j(x_i) = I(c_j \leq x_i < c_{j+1})$.
Despite the increased complexity, this still amounts to estimating the unknown regression coefficients $\beta$, for which all the least squares tools and models apply.

## Regression Splines

### Piecewise Polynomials

>
Instead of fitting a high-degree polynomial over the entire range of $X$, piecewise
polynomial regression involves fitting separate low-degree polynomials
over different regions of $X$. For example, a piecewise cubic polynomial works
by fitting a cubic regression model of the form

$$
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i.
$$

>
where the coefficients $\beta_0, \beta_1, \beta_2,$ and  $\beta_3$ differ in parts of the range of $X$.
The points where the coefficients change are called *knots*.

With 0 knots, we have the standard cubic polynomial with $d = 3$ as described in section 7.1.
With a single knot at point $c$, this takes the form:

$$
\begin{align}
y_i &= \beta_{01} + \beta_{11} x_i + \beta_{21} x_i^2 + \beta_{31} x_i^3 + \epsilon_i \ \ \ \text{if} \ \  x_i < c \\
&= \beta_{02} + \beta_{22} x_i + \beta_{22} x_i^2 + \beta_{32} x_i^3 + \epsilon_i \ \ \ \text{if} \ \  x_i \geq c.
\end{align}
$$

Functionally, this is essentially fitting two separate regression equations on subsets of $X$, with 8 degrees of freedom for the eight regression coefficients.

### Constraints and Splines

The problem with piecewise polynomials is that the resulting fit can be discontinuous, like in the top left panel of Figure 7.3.
To remedy this, we can fit a piecewise polynomial under the *constraint* that the fitted curve must be continuous, like in the top right panel.
The bottom left panel shows the result of two additional constraints: that the first and second derivative are continuous at `age = 50` -- this is called a *cubic spline*, which generally has $K + 4$ degrees of freedom (=5 in this example).
The lower right panel shows a *linear spline*.

### The Spline Basis Representation {#spline-basis}

In order to implement the continuity constraints for regression splines, we can use the basis model.
A cubic spline with $K$ knots can be modeled as

$$
y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2 (x_i) + \dots + \beta_{K+3} b_{K+3} (x_i) + \epsilon_i,
$$

for an appropriate choice of basis functions $b_1, b_2, \dots, b_{K+3}$.
The model can then be fit with least squares.

There are many equivalent representations of cubic splines using different basis functions.
The most direct is to start with the cubic polynomial and then add one *truncated power basis function* per knot:

$$
\begin{align}
h(x, \xi) = (x - \xi)^2_+ &= (x - \xi)^3 \ \ \ \text{if} \ \ \ x > \xi \\ 
&= 0 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{otherwise},
\end{align}
$$

where $\xi$ is the knot.
One can show that adding the term $\beta_4 h(x, \xi)$ to the cubic spline model above will lead to a discontinuity in only the third derivative at $\xi$, but remain continuous in the first and second derivatives.

Unfortunately, these splines have high variance at boundaries of the predictors.
This can be reduced with *natural splines* which have additional constraints at the boundaries to produce more stable estimates.
We can show this with confidence intervals of the models fit with cubic and natural cubic splines, as in Figure 7.4:

```{r figure7-4, warning=FALSE}
library(splines)

# Use just a subset of the data to mimic the figure
set.seed(20)
d <- wage %>%
  filter(wage < 300) %>%
  slice_sample(n = 500)

wage_bs_linear_fit <- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = d)
wage_ns_linear_fit <- lm(wage ~ ns(age, knots = c(25, 40, 60)), data = d)

bind_rows(
  wage_bs_linear_fit %>%
    augment(newdata = tibble(age = age_grid), interval = "confidence") %>%
    mutate(model = "Cubic spline"),
  wage_ns_linear_fit %>%
    augment(newdata = tibble(age = age_grid), interval = "confidence") %>%
    mutate(model = "Natural cubic spline")
) %>%
  mutate(model = fct_rev(model)) %>%
  ggplot(aes(x = age)) +
  geom_point(data = d, aes(y = wage),
             shape = 21, color = "grey50", alpha = 0.5) +
  geom_line(aes(y = .fitted, color = model), size = 1.5) +
  geom_line(aes(y = .lower, color = model), lty = 2, size = 1) +
  geom_line(aes(y = .upper, color = model), lty = 2, size = 1) +
  geom_vline(xintercept = c(25, 40, 60), lty = 2) +
  coord_cartesian(ylim = c(40, 300)) +
  theme(legend.position = c(0.7, 0.9)) +
  labs(color = NULL)
```

### Choosing the Number and Locations of the Knots {#choosing-knots}

>
When we fit a spline, where should we place the knots? The regression
spline is most flexible in regions that contain a lot of knots, because in
those regions the polynomial coefficients can change rapidly. Hence, one
option is to place more knots in places where we feel the function might
vary most rapidly, and to place fewer knots where it seems more stable.
While this option can work well, in practice it is common to place knots in
a uniform fashion. One way to do this is to specify the desired degrees of
freedom, and then have the software automatically place the corresponding
number of knots at uniform quantiles of the data.

The `splines::ns()` function, when provided the `df` argument, computes knot locations based on percentiles.
For the `age` data and `df = 4` (3 knots):

```{r}
age_ns <- ns(wage$age, df = 4)
attr(age_ns, "knots")
```

Fit the model and re-create Figure 7.5:

```{r figure7-5, warning=FALSE}
wage_ns_linear_fit <- lm(wage ~ ns(age, df = 4), data = wage)
wage_ns_logistic_fit <- glm(I(wage > 250) ~ ns(age, df = 4),
                            data = wage, family = binomial)

p1 <- wage_ns_linear_fit %>%
  augment(newdata = tibble(age = age_grid), interval = "confidence") %>%
  ggplot(aes(x = age)) +
  geom_point(data = wage, aes(y = wage),
             shape = 21, color = "grey50", alpha = 0.5) +
  geom_line(aes(y = .fitted), color = "red", size = 1.5) +
  geom_line(aes(y = .lower), color = "red", lty = 2, size = 1) +
  geom_line(aes(y = .upper), color = "red", lty = 2, size = 1) +
  geom_vline(xintercept = attr(age_ns, "knots"), lty = 2)
p2 <- wage_ns_logistic_fit %>%
  augment(newdata = tibble(age = age_grid),
          type.predict = "link", se_fit = TRUE) %>%
  mutate(
    .lower = .fitted - 1.96 * .se.fit,
    .upper = .fitted + 1.96 * .se.fit,
    across(c(.fitted, .lower, .upper), ~ exp(.x) / (1 + exp(.x)))
  ) %>%
  ggplot(aes(x = age)) +
  geom_rug(data = wage %>% filter(wage > 250),
           sides = "t", color = "grey50", alpha = 0.5) +
  geom_rug(data = wage %>% filter(wage <= 250),
           sides = "b", color = "grey50", alpha = 0.5) +
  geom_line(aes(y = .fitted), color = "red", size = 1.5) +
  geom_line(aes(y = .lower), lty = 2, color = "red", size = 1) +
  geom_line(aes(y = .upper), lty = 2, color = "red", size = 1) +
  geom_vline(xintercept = attr(age_ns, "knots"), lty = 2) +
  coord_cartesian(ylim = c(0, 0.2)) +
  labs(y = "Pr(wage > 250 | age)")

p1 + p2
```

The automatic choice of knot location is usually sufficient, but how do we decide on the number of knots $K$?
The same way we usually choose model hyperparameters: resampling.
For this, we turn to `tidymodels`, which has `step_bs()` and `step_ns()` functions for specifying basis splines and natural basis splines.

```{r}
set.seed(93)
wage_resamples <- vfold_cv(wage, v = 10)

wage_lm_bs_rec <- recipe(wage ~ age, data = wage) %>%
  step_bs(age, deg_free = tune(), degree = tune())
wage_lm_ns_rec <- recipe(wage ~ age, data = wage) %>%
  step_ns(age, deg_free = tune())
lm_spec <- linear_reg()

wage_lm_bs_workflow <- workflow() %>%
  add_model(lm_spec) %>%
  add_recipe(wage_lm_bs_rec)
wage_lm_ns_workflow <- workflow() %>%
  add_model(lm_spec) %>%
  add_recipe(wage_lm_ns_rec)
```

If I understand Figure 7.6 correctly, the results being displayed in the right panel aren't all cubic splines because those models can't have <3 degrees of freedom.
For example, if I set degrees of freedom to `df = 2` and the polynomial degree of freedom to `degree = 3`, then `bs()` will calculate a negative number of knots.
It will automatically adjust `df` so the number of knots equals 0 and return a warning:

```{r}
lm(wage ~ bs(age, df = 2, degree = 3), data = wage)
```

So in choosing the hyperparameters for these models, I will vary both `deg_free` and `degree` so that it fits a linear spline, quadratic spline, and 7 cubic splines (with varying numbers of knots) as follows:

```{r}
bs_df_grid <- bind_rows(
  tibble(deg_free = 1:3, degree = 1:3),
  tibble(deg_free = 4:10, degree = 3)
)
bs_df_grid
```

```{r}
wage_lm_bs_tune <- tune_grid(
  wage_lm_bs_workflow, resamples = wage_resamples,
  grid = bs_df_grid
)
```

The way degrees of freedom work in natural splines doesn't require this adjustment, I can just provide `deg_free = 1:10`:

```{r}
wage_lm_ns_tune <- tune_grid(
  wage_lm_ns_workflow, resamples = wage_resamples,
  grid = bs_df_grid %>% select(deg_free)
)
```

Finally, re-create Figure 7.6:

```{r figure7-6}
p1 <- collect_metrics(wage_lm_ns_tune) %>%
  filter(.metric == "rmse") %>%
  mutate(mse = mean^2) %>%
  ggplot(aes(x = deg_free, y = mse)) +
  geom_line(color = "red") +
  geom_point(fill = "red", color = "white", shape = 21, size = 3) +
  scale_x_continuous("Degrees of freedom of natural spline",
                     breaks = seq(2, 10, 2))  +
  labs(y = "Mean squared error")
p2 <- collect_metrics(wage_lm_bs_tune) %>%
  filter(.metric == "rmse") %>%
  mutate(mse = mean^2) %>%
  ggplot(aes(x = deg_free, y = mse)) +
  geom_line(color = "blue") +
  geom_point(fill = "blue", color = "white", shape = 21, size = 3) +
  scale_x_continuous("Degrees of freedom of cubic spline",
                     breaks = seq(2, 10, 2))  +
  labs(y = NULL)
p1 + p2
```

```{r include=FALSE}
# Proving to myself that the linear model `wage ~ age` is equivalent to...
lm_linear <- lm(wage ~ age, data = wage)
# ... the model with a 1-degree spline (0 knots)
lm_11 <- lm(wage ~ bs(age, df = 1, degree = 1), data = wage)

# Also that quadratic regression is equivalent to...
lm_quad <- lm(wage ~ poly(age, 2), data = wage)
# ... the model with a 2-degree spline (0 knots)
lm_12 <- lm(wage ~ bs(age, df = 1, degree = 2), data = wage)

# And cubic
lm_cubic <- lm(wage ~ poly(age, 3), data = wage)
lm_13 <- lm(wage ~ bs(age, df = 1, degree = 3), data = wage)

lm_21 <- lm(wage ~ bs(age, df = 2, degree = 1), data = wage)
lm_31 <- lm(wage ~ bs(age, df = 3, degree = 1), data = wage)
lm_22 <- lm(wage ~ bs(age, df = 2, degree = 2), data = wage)
lm_32 <- lm(wage ~ bs(age, df = 3, degree = 2), data = wage)

bind_rows(
  augment(lm_linear, newdata = tibble(age = age_grid)) %>%
    mutate(model = "lm_linear"),
  augment(lm_11, newdata = tibble(age = age_grid)) %>%
    mutate(model = "lm_11"),
  augment(lm_12, newdata = tibble(age = age_grid)) %>%
    mutate(model = "lm_12"),
  augment(lm_quad, newdata = tibble(age = age_grid)) %>%
    mutate(model = "lm_quad"),
  augment(lm_13, newdata = tibble(age = age_grid)) %>%
    mutate(model = "lm_13"),
  augment(lm_cubic, newdata = tibble(age = age_grid)) %>%
    mutate(model = "lm_cubic")
) %>%
  mutate(model = fct_inorder(model)) %>%
  ggplot(aes(x = age, y = .fitted)) +
  geom_line() +
  facet_wrap(~ model, ncol = 2)
```


```{r include=FALSE}
# Playing around with df some more

# Linear spline with a single knot
lm_21 <- lm(wage ~ bs(age, df = 2, degree = 1), data = wage)
# Linear spline with two knots
lm_31 <- lm(wage ~ bs(age, df = 3, degree = 1), data = wage)
# Quadratic with one knot
lm_22 <- lm(wage ~ bs(age, df = 2, degree = 2), data = wage)
# Quadratic with two knot
lm_32 <- lm(wage ~ bs(age, df = 3, degree = 2), data = wage)

bind_rows(
  augment(lm_21, newdata = tibble(age = age_grid)) %>%
    mutate(model = "lm_21"),
  augment(lm_31, newdata = tibble(age = age_grid)) %>%
    mutate(model = "lm_31"),
  augment(lm_22, newdata = tibble(age = age_grid)) %>%
    mutate(model = "lm_22"),
  augment(lm_32, newdata = tibble(age = age_grid)) %>%
    mutate(model = "lm_32")
) %>%
  mutate(model = fct_inorder(model)) %>%
  ggplot(aes(x = age, y = .fitted)) +
  geom_line() +
  facet_wrap(~ model, ncol = 2)
```

### Comparison to Polynomial Regression {#comparison-polynomial}

Figure 7.7 compares a natural cubic spline with 15 degrees of freedom to a 15-degree polynomial regression on the `wage` data:

```{r figure7-7, warning=FALSE}
wage_ns_15_fit <- lm(wage ~ ns(age, df = 15), data = wage)
wage_poly_15_fit <- lm(wage ~ poly(age, degree = 15), data = wage)

bind_rows(
  wage_ns_15_fit %>%
    augment(newdata = tibble(age = age_grid)) %>%
    mutate(model = "Natural cubic spline"),
  wage_poly_15_fit %>%
    # Extend the age range a bit to see more of the fit
    augment(newdata = tibble(age = c(age_grid, 80.5))) %>%
    mutate(model = "Polynomial")
) %>%
  ggplot(aes(x = age)) +
  geom_point(data = wage, aes(y = wage),
             shape = 21, color = "grey50", alpha = 0.5) +
  geom_line(aes(y = .fitted, color = model), size = 1.5) +
  theme(legend.position = c(0.8, 0.8)) +
  labs(color = NULL)
```

>
The extra flexibility in the polynomial produces undesirable results at the boundaries, while the natural
cubic spline still provides a reasonable fit to the data. Regression splines
often give superior results to polynomial regression. This is because unlike
polynomials, which must use a high degree (exponent in the highest monomial term,
e.g. $X^{15}$) to produce flexible fits, splines introduce flexibility
by increasing the number of knots but keeping the degree fixed. Generally,
this approach produces more stable estimates. Splines also allow us to place
more knots, and hence flexibility, over regions where the function $f$ seems
to be changing rapidly, and fewer knots where $f$ appears more stable.

## Smoothing Splines

### An Overview of Smoothing Splines

To fit a curve $g(x)$ to a set of data, we want $\text{RSS} = \sum_{i=1}^n (y_i - g(x_i))^2$ to be small.
If there are no constraints on $g(x)$, however, then it will simply overfit the data completely for $\text{RSS} = 0$.

There are a number of ways to ensure that $g$ is smooth.
One way is to find $g$ that minimizes

$$
\text{RSS} + \lambda \int g'' (t)^2 dt
$$

where $\lambda$ is a non-negative penalty parameter (that will control the bias-variance trade-off as we've seen before).
The function $g$ that minimizes the above is known as a *smoothing spline*.
By placing the constraint on the second derivative $g''$, we encourage $g$ to be smooth.

It turns out that the optimal $g(x)$ is a natural cubic polynomial with knots at unique values of $x$ (and continuous first and second derivatives at each knot).
However, it is not the same one we would get with the basis function approach in Section 7.4.3 with knots at each value of $x$ -- rather, it is a *shrunken* version of that spline, where $\lambda$ controls the level of shrinkage

### Choosing the Smoothing Parameter $\lambda$ {#smoothing-parameter}

It may seem like a smoothing spline, with knots at every unique value of $x_i$, will have far too many degrees of freedom, but the tuning parameter $\lambda$ controls the *effective degrees of freedom* $df_{\lambda}$.
The higher $df_{\lambda}$, the more flexible (lower bias, higher variance) the smoothing spline.

Unlike the other spline methods, we do not need to select the number or location of the knots.
We instead have to choose the value of $\lambda$, usually via cross-validation.
It turns out that the LOOCV error can be computed very efficiently for smoothing splines, with essentially the same cost as computing a single fit, with this formula:

$$
\text{RSS}_{cv} (\lambda) = \sum_{i=1}^n (y_i - \hat{g}_{\lambda}^{(-i)} (x_i))^2
= \sum_{i=1}^n \left[\frac{y_i - \hat{g}_{\lambda}(x_i)}{1 - \{\textbf{S}_{\lambda}\}_{ii}}\right].
$$

where $\hat{g}_{\lambda}^{(-i)}$ is the fitted value for this smoothing spline evaluated at $x_i$, with the fit using all training observations except for $(x_i, y_i)$.

>
In contrast, $\hat{g}_{\lambda}(x_i)$ indicates the smoothing
spline function fit to all of the training observations and evaluated at $x_i$.
This remarkable formula says that we can compute each of these leave- one-out
fits using only $\hat{g}_{\lambda}$, the original fit to all of the data!
We have
a very similar formula (5.2) on page 202 in Chapter 5 for least squares
linear regression. Using (5.2), we can very quickly perform LOOCV for
the regression splines discussed earlier in this chapter, as well as for least
squares regression using arbitrary basis functions.

The `smooth.spline()` function fits a smoothing spline to data as follows:

```{r figure7-8, warning=FALSE}
wage_smooth_df_16 <- smooth.spline(x = wage$age, y = wage$wage, df = 16)
wage_smooth_df_16

wage_smooth_cv <- smooth.spline(x = wage$age, y = wage$wage, cv = TRUE)
wage_smooth_cv

bind_rows(
  as_tibble(predict(wage_smooth_df_16, age_grid)) %>%
    mutate(model = "16 degrees of freedom"),
  as_tibble(predict(wage_smooth_cv, age_grid)) %>%
    mutate(model = "6.8 degrees of freedom (LOOCV)")
) %>%
  ggplot() +
  geom_point(data = wage, aes(x = age, y = wage),
             shape = 21, color = "grey50", alpha = 0.5) +
  geom_line(aes(x = x, y = y, color = model), size = 1.5) +
  theme(legend.position = "top") +
  labs(color = NULL)
```

>
The red curve indicates the fit obtained from pre-specifying that we
would like a smoothing spline with 16 effective degrees of freedom. The blue
curve is the smoothing spline obtained when $\lambda$ is chosen using LOOCV; in
this case, the value of $\lambda$ chosen results in 6.8 effective degrees of freedom
(computed using (7.13)). For this data, there is little discernible difference
between the two smoothing splines, beyond the fact that the one with 16
degrees of freedom seems slightly wigglier. Since there is little difference
between the two fits, the smoothing spline fit with 6.8 degrees of freedom
is preferable, since in general simpler models are better unless the data
provides evidence in support of a more complex model.

## Local Regression {#local-regression}

*Local regression* involves computing the fit at a target point $x_0$ using just the nearby (local) training observations.
It involves a number of choices, such the weighting function $K$, and whether to fit a linear, constant or quadratic regression.
The most important choice is the span $s$ which is the proportion of points used to compute the local regression at $x_0$.
It is analogous to $\lambda$ in smoothing splines: the smaller the value $s$, the more *local* and flexible the fit; the larger the value, the less flexible.

```{r figure7-10, warning=FALSE}
wage_local_s_0.2 <- loess(wage ~ age, data = wage, span = 0.2)
wage_local_s_0.7 <- loess(wage ~ age, data = wage, span = 0.7)

wage_local_s_0.2; wage_local_s_0.7

bind_rows(
  augment(wage_local_s_0.2, newdata = tibble(age = age_grid)) %>%
    mutate(model = "Span is 0.2 (16.4 degres of freedom)"),
  augment(wage_local_s_0.7, newdata = tibble(age = age_grid)) %>%
    mutate(model = "Span is 0.7 (5.3 degres of freedom)")
) %>%
  ggplot(aes(x = age)) +
  geom_point(data = wage, aes(y = wage),
             shape = 21, color = "grey50", alpha = 0.5) +
  geom_line(aes(y = .fitted, color = model), size = 1.5) +
  theme(legend.position = "top") +
  labs(color = NULL)
```

## Generalized Additive Models

In previous sections, we have presented approaches for flexibly predicting a response $Y$ with a single predictor $X$.
*Generalized additive models* (GAMs) provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining *additivity*.

### GAMs for Regression Problems {#gam-regression}

To extend the multiple linear regression model, we replace the coefficient components $\beta_j x_{ij}$ with smooth non-linear functions $f_j (x_{ij})$:

$$
y_i = \beta_0 + f_1 (x_{i1}) + f_2 (x_{i2}) + \dots + f_p (x_{ip}) + \epsilon_i.
$$

Consider the task of fitting the model:

$$
\texttt{wage} = \beta_0 + f_1(\texttt{year}) + f_2 (\texttt{age}) + f_3 (\texttt{education}) + \epsilon.
$$

In the example in Figure 7.11, the quantitative variables `year` and `age` are fit using natural splines, and the qualitative variable (with 5 levels) `education` is fit via the usual dummy variable approach.

```{r figure7-11, fig.width=6}
wage_gam_ns_fit <- lm(wage ~ ns(year, 4) + ns(age, 5) + education, data = wage)
# Though the model wasn't fit with `gam::gam()`, we can still use the
#  `gam::plot.Gam()` function on the `lm` object to retrieve the smooth functions
# Grab the data from the plot object so I can use it in `ggplot2`
d <- invisible(gam::plot.Gam(wage_gam_ns_fit))

d <- map(
  d$preplot,
  ~ tibble(x = .$x, y = .$y, y_se = .$se.y, xlab = .$xlab, ylab = .$ylab) %>%
    mutate(y_lower = y - y_se, y_upper = y + y_se)
)

p1 <- d$`ns(year, 4)` %>%
  ggplot(aes(x)) +
  geom_line(aes(y = y), color = "red") +
  geom_line(aes(y = y_lower), lty = 2, color = "red") +
  geom_line(aes(y = y_upper), lty = 2, color = "red") +
  labs(x = NULL, title = d$`ns(year, 4)`$ylab, y = NULL) +
  ylim(c(-20, 20))
p2 <- d$`ns(age, 5)` %>%
  ggplot(aes(x)) +
  geom_line(aes(y = y), color = "red") +
  geom_line(aes(y = y_lower), lty = 2, color = "red") +
  geom_line(aes(y = y_upper), lty = 2, color = "red") +
  labs(x = NULL, title = d$`ns(age, 5)`$ylab, y = NULL)
p3 <- d$education %>%
  ggplot(aes(x)) +
  geom_errorbar(aes(y = y, ymin = y_lower, ymax = y_upper)) +
  labs(x = NULL, title = d$education$ylab, y = NULL) +
  theme(axis.text.x = element_text(angle = 45, size = 8, vjust = 0.7))
p1 + p2 + p3
```

For Figure 7.12, we use the `gam` package and specify smoothing splines:

```{r results='hide'}
library(gam)
wage_gam_smooth_fit <-
  gam(wage ~ s(year, 4) + s(age, 5) + education,
      data = wage)

d <- plot(wage_gam_smooth_fit, se = TRUE, col = "blue")
```


```{r figure7-12, message=FALSE}
d <- map(
  d$preplot,
  ~ tibble(x = .$x, y = .$y, y_se = .$se.y, xlab = .$xlab, ylab = .$ylab) %>%
    mutate(y_lower = y - y_se, y_upper = y + y_se)
)

p1 <- d$`s(year, 4)` %>%
  ggplot(aes(x)) +
  geom_line(aes(y = y), color = "blue") +
  geom_line(aes(y = y_lower), lty = 2, color = "blue") +
  geom_line(aes(y = y_upper), lty = 2, color = "blue") +
  labs(x = NULL, title = d$`s(year, 4)`$ylab, y = NULL) +
  ylim(c(-20, 20))
p2 <- d$`s(age, 5)` %>%
  ggplot(aes(x)) +
  geom_line(aes(y = y), color = "blue") +
  geom_line(aes(y = y_lower), lty = 2, color = "blue") +
  geom_line(aes(y = y_upper), lty = 2, color = "blue") +
  labs(x = NULL, title = d$`s(age, 5)`$ylab, y = NULL)
p3 <- d$education %>%
  ggplot(aes(x)) +
  geom_errorbar(aes(y = y, ymin = y_lower, ymax = y_upper)) +
  labs(x = NULL, title = d$education$ylab, y = NULL) +
  theme(axis.text.x = element_text(angle = 45, size = 8, vjust = 0.7))
p1 + p2 + p3
```


```{r message=FALSE, include=FALSE, eval=FALSE}
# Trying to get the exact same fit with `mgcv`
detach("package:gam", unload = TRUE)
library(mgcv)
wage_mgcv_smooth_fit <-
  gam(
    # With `mgcv::s()`, I need to specify degrees of freedom with `k`, and
    #  turn off penalization with `fx`
    # wage ~ s(year, k = 5, fx = FALSE, bs = "cr") + s(age, k = 6, fx = FALSE, bs = "cr") +
    #wage ~ s(year, k = 5, fx = FALSE) + s(age, k = 6, fx = FALSE) +
    # wage ~ s(year, k = 4, fx = TRUE) + s(age, k = 5, fx = TRUE) +
    #   education,
    # wage ~ s(year, k = 4, fx = FALSE) + s(age, k = 5, fx = FALSE) +
    #   education,
    # wage ~ s(year, k = 4, fx = FALSE, bs = "cr") + s(age, k = 5, fx = FALSE, bs = "cr") +
    #   education,
    wage ~ s(year, k = 4, fx = TRUE, bs = "cr") + s(age, k = 5, fx = TRUE, bs = "cr") +
      education,
    data = wage
  )

d_mgcv <- plot(wage_mgcv_smooth_fit, se = TRUE, all.terms = TRUE)

bind_rows(
  gam = tibble(x = d$`s(age, 5)`$x,
               y = d$`s(age, 5)`$y),
  mgcv = tibble(x = d_mgcv[[2]]$x, y = d_mgcv[[2]]$fit),
  .id = "func"
) %>%
  ggplot(aes(x, y)) +
  geom_line(aes(color = func))
```

The fitted functions are essentially equivalent between the natural and smoothing splines.

>
Fitting a GAM with a smoothing spline is not quite as simple as fitting a GAM
with a natural spline, since in the case of smoothing splines, least squares
cannot be used. However, standard software such as the `gam()` function in R
can be used to fit GAMs using smoothing splines, via an approach known
as backfitting. This method fits a model involving multiple predictors by backfitting repeatedly updating the fit for each predictor in turn, holding the others
fixed. The beauty of this approach is that each time we update a function,
we simply apply the fitting method for that variable to a partial residual.

>
We do not have to use splines as the building blocks for GAMs: we can
just as well use local regression, polynomial regression, or any combination
of the approaches seen earlier in this chapter in order to create a GAM.

#### Pros and Cons of GAMs {-}

Pros:

* GAMs allow $f_j$ fits to each $X_j$, which is a way to flexibly and (nearly) automatically capture non-linearity.
* Non-linear fits can improve prediction accuracy.
* Because the models are additive, we can examine the effect of each $X_j$ on $Y$ separately, while holding others fixed.
* Smoothness of each $f_j$ can be summarized via degrees of freedom.

Cons:

* The main limitation is the additive restriction. We cannot add interaction terms like $X_j \times X_k$ to GAMs.

GAMs provide as useful compromise between linear and fully non-parametric models.

### GAMs for Classification Problems {#gam-classification}

GAMs are easily extended to other response distributions via the `family` argument, such as `binomial` for logistic regression classification.

```{r results='hide'}
wage_gam_fit_binom <- gam(
  I(wage > 250) ~ year + s(age, 5) + education,
  family = binomial, data = wage
)
d <- plot(wage_gam_fit_binom)
```


```{r figure7-13}
d <- map(
  d$preplot,
  ~ tibble(x = .$x, y = .$y, y_se = .$se.y, xlab = .$xlab, ylab = .$ylab) %>%
    mutate(y_lower = y - y_se, y_upper = y + y_se)
)

p1 <- d$year %>%
  ggplot(aes(x)) +
  geom_line(aes(y = y), color = "green") +
  geom_line(aes(y = y_lower), lty = 2, color = "green") +
  geom_line(aes(y = y_upper), lty = 2, color = "green") +
  labs(x = NULL, title = d$`s(year, 4)`$ylab, y = NULL) +
  ylim(c(-4, 4))
p2 <- d$`s(age, 5)` %>%
  ggplot(aes(x)) +
  geom_line(aes(y = y), color = "green") +
  geom_line(aes(y = y_lower), lty = 2, color = "green") +
  geom_line(aes(y = y_upper), lty = 2, color = "green") +
  labs(x = NULL, title = d$`s(age, 5)`$ylab, y = NULL)
p3 <- d$education %>%
  ggplot(aes(x)) +
  geom_errorbar(aes(y = y, ymin = y_lower, ymax = y_upper)) +
  labs(x = NULL, title = d$education$ylab, y = NULL) +
  theme(axis.text.x = element_text(angle = 45, size = 8, vjust = 0.7))
p1 + p2 + p3
```

There are 0 occurrences of `wage > 250` for `education = "1. < HS Grad"`, which is causing very wide SE range.
Re-fit the model excluding that value of `education`:

```{r results='hide'}
wage_gam_fit_binom <- gam(
  I(wage > 250) ~ year + s(age, 5) + education,
  family = binomial, data = filter(wage, education != "1. < HS Grad")
)
d <- plot(wage_gam_fit_binom)
```


```{r figure7-14}
d <- map(
  d$preplot,
  ~ tibble(x = .$x, y = .$y, y_se = .$se.y, xlab = .$xlab, ylab = .$ylab) %>%
    mutate(y_lower = y - y_se, y_upper = y + y_se)
)

p1 <- d$year %>%
  ggplot(aes(x)) +
  geom_line(aes(y = y), color = "green") +
  geom_line(aes(y = y_lower), lty = 2, color = "green") +
  geom_line(aes(y = y_upper), lty = 2, color = "green") +
  labs(x = NULL, title = d$year$ylab, y = NULL) +
  ylim(c(-4, 4))
p2 <- d$`s(age, 5)` %>%
  ggplot(aes(x)) +
  geom_line(aes(y = y), color = "green") +
  geom_line(aes(y = y_lower), lty = 2, color = "green") +
  geom_line(aes(y = y_upper), lty = 2, color = "green") +
  labs(x = NULL, title = d$`s(age, 5)`$ylab, y = NULL)
p3 <- d$education %>%
  ggplot(aes(x)) +
  geom_errorbar(aes(y = y, ymin = y_lower, ymax = y_upper)) +
  labs(x = NULL, title = d$education$ylab, y = NULL) +
  theme(axis.text.x = element_text(angle = 45, size = 8, vjust = 0.7))
p1 + p2 + p3
```

## Lab: Non-linear Modeling

### Polynomial Regression and Step Functions

The figure was re-produced in section \@ref(polynomial-regression).

To determine the simplest sufficient polynomial model, we can perform $F$-tests with `anova()`.

```{r}
wage_poly_1_linear_fit <- lm(wage ~ age, data = wage)
wage_poly_2_linear_fit <- lm(wage ~ poly(age, 2), data = wage)
wage_poly_3_linear_fit <- lm(wage ~ poly(age, 3), data = wage)
wage_poly_5_linear_fit <- lm(wage ~ poly(age, 5), data = wage)

anova(wage_poly_1_linear_fit, wage_poly_2_linear_fit, wage_poly_3_linear_fit,
      wage_poly_4_linear_fit, wage_poly_5_linear_fit)
```

The cubic fit is probably the best choice here.

Equivalently, because of the way `poly()` created orthogonalized polynomials, we can get the same $p$-values and $F$-statistics (= square of the $t$-statistics) like this:

```{r}
tidy(wage_poly_5_linear_fit) %>%
  transmute(term, t = statistic, `F` = t^2, p = scales::pvalue(p.value)) %>%
  gt() %>%
  fmt_number(columns = c(t, `F`), n_sigfig = 4)
```

However, this only works because the polynomials are orthogonal and there is just the one predictor.
Otherwise, `anova()` must be used for these model comparisons.

The step function was fit and visualized in section \@ref(step-functions).

### Splines

The cubic spline and natural spline fits are shown in sections \@ref(spline-basis) and \@ref(choosing-knots).
The smoothing spline fits are shown in section \@ref(smoothing-parameter), and the local regression fits in section \@ref(local-regression).

### GAMs {#gams-lab}

Figure 7.11 and 7.12 are re-produced in section \@ref(gam-regression).

Use `anova()` with different variations of the `year` variable:

```{r}
wage_gam_no_year <- 
  gam(wage ~ s(age, 5) + education, data = wage)
wage_gam_linear_year <- 
  gam(wage ~ year + s(age, 5) + education, data = wage)

anova(wage_gam_no_year, wage_gam_linear_year, wage_gam_smooth_fit, test = "F")
```

We find evidence that the linear function of `year` is preferred, which can also be seen in the model `summary()`:

```{r}
summary(wage_gam_smooth_fit)
```

We see under `Anova for Parametric Effects` that all of the terms are statistically significant, for even a linear relationship.
Under `Anova for Nonparametric Effects`, we see that only the `age` term has a statistically significant improvement over the linear.

The logistic regression example is in section \@ref(gam-classification).

## Exercises

### Applied {-}

#### 6. Polynomial and step function regression on `wage` {-}

(a) Find the optimal degree $d$ for polynomial regression.

```{r exercise-6}
set.seed(201)
wage_resamples <- vfold_cv(wage, v = 10)

lm_spec <- linear_reg()
wage_poly_spec <- recipe(wage ~ age, data = wage) %>%
  step_poly(age, degree = tune())

wage_poly_workflow <- workflow() %>%
  add_model(lm_spec) %>%
  add_recipe(wage_poly_spec)

degree_grid <- tibble(degree = 1:10)
wage_poly_tune <- tune_grid(wage_poly_workflow, resamples = wage_resamples,
                            grid = degree_grid)
autoplot(wage_poly_tune) +
  scale_x_continuous(breaks = 1:10)
```

The best `degree` values:

```{r}
show_best(wage_poly_tune, metric = "rmse")
```

But it is clear from the above figure that there is little difference past `degree` 2 or 3.
We can find the least complex model within one standard error of the best model with this function:

```{r}
(best_degree <- select_by_one_std_err(wage_poly_tune, metric = "rmse", degree))
```

And here is how the fit looks overlaid on the data:

```{r}
age_grid <- tibble(age = 18:80)
finalize_workflow(wage_poly_workflow, best_degree) %>%
  fit(data = wage) %>%
  augment(new_data = age_grid) %>%
  ggplot(aes(x = age)) +
  geom_point(data = wage, aes(y = wage),
             shape = 21, color = "grey50", alpha = 0.5) +
  geom_line(aes(y = .pred), color = td_colors$nice$soft_orange, size = 2)
```

(b) Find the optimal number of cuts $K$ for step function regression.

```{r}
wage_step_spec <- recipe(wage ~ age, data = wage) %>%
  step_discretize(age, num_breaks = tune(),
                  # Need to adjust this from the default (=10)
                  min_unique = 2)

wage_step_workflow <- workflow() %>%
  add_model(lm_spec) %>%
  add_recipe(wage_step_spec)

breaks_grid <- tibble(num_breaks = 2:20)
wage_step_tune <- tune_grid(wage_step_workflow, resamples = wage_resamples,
                            grid = breaks_grid)
autoplot(wage_step_tune)
```

```{r}
show_best(wage_step_tune, metric = "rmse")
```

The best model by RMSE has the most `num_breaks`, but within one `std_error` gives:

```{r}
(best_num_breaks <-
  select_by_one_std_err(wage_step_tune, metric = "rmse", num_breaks))
```

For posterity, here is the best model by RMSE, and best model within one standard error:

```{r}
bind_rows(
  finalize_workflow(wage_step_workflow,
                    select_best(wage_step_tune, metric = "rmse")) %>%
    fit(data = wage) %>%
    augment(new_data = age_grid) %>%
    mutate(model = "best by RMSE"),
  finalize_workflow(wage_step_workflow, best_num_breaks) %>%
    fit(data = wage) %>%
    augment(new_data = age_grid) %>%
    mutate(model = "best within 1 SE")
) %>%
  ggplot(aes(x = age)) +
  geom_point(data = wage, aes(y = wage),
             shape = 21, color = "grey50", alpha = 0.5) +
  geom_line(aes(y = .pred, color = model), size = 2) +
  theme(legend.position = "top") +
  labs(color = NULL)
```


#### 7. Explore other predictors in `wage` {-}

In `tidymodels`, the implementation of smooth splines and GAMs uses the `mgcv` package as the default (and currently only) engine, so I'll use that for this example.

```{r message=FALSE}
detach("package:gam", unload = TRUE)
library(mgcv)
glimpse(wage)
```

Every other feature in this data set is data set is categorical.
Here is the `wage` distribution by the different categories:

```{r fig.height=5}
wage %>%
  select(wage, where(is.factor), -education) %>%
  rownames_to_column() %>%
  pivot_longer(cols = -c(wage, rowname),
               names_to = "var", values_to = "val") %>%
  ggplot(aes(y = val, x = wage)) +
  geom_boxplot() +
  facet_wrap(~ var, scales = "free_y", ncol = 2)
```

Some general observations:

* People with better `health` have slightly higher wages.
* People with `health_ins` have higher wages.
* People with `jobclass = "Information"` have slightly higher wages.
* People with `maritl = "Married"` have have higher wages than `"Never Married"`.
* People with `race = "Other"` has lower wages.
* The `region` variable has a single value.

Since these are all categorical, there aren't any new smoothing/spline/polynomial/etc. functions to try out.
Instead I will add the variables and determine if they improve the fit:

```{r}
wage_gam_1 <- gam(wage ~ s(age, k = 5) + s(year, k = 4) + education, data = wage)
summary(wage_gam_1)
```

Like we saw in the lab (section \@ref(gams-lab)), the `year` variable can be modeled as linear.

```{r}
wage_gam_2 <- gam(wage ~ s(age, k = 5) + year + education, data = wage)
summary(wage_gam_2)
```

Now add the new categorical variables:

```{r}
wage_gam_3 <- gam(
  wage ~ s(age, k = 5) + year + education + health + health_ins + jobclass +
    maritl + race,
  data = wage
)
summary(wage_gam_3)

par(mar = c(2, 2, 2, 2))
plot(wage_gam_3, pages = 1, all.terms = TRUE, shade = TRUE)
```

It does provide a statistically singificant improvement:

```{r}
anova(wage_gam_2, wage_gam_3, test = "F")
```
#### 8. Non-linear models of `auto` {-}

```{r}
auto <- ISLR2::Auto %>%
  # Origin is a factor coded as numeric
  mutate(
    origin = factor(origin, levels = c(1, 2, 3),
                    labels = c("American", "European", "Japanese"))
  )
glimpse(auto)
```

The variables (excluding `name`) vs `mpg`:

```{r message=FALSE, fig.height=5}
p1 <- auto %>%
  select(mpg, acceleration, displacement, year, horsepower) %>%
  rownames_to_column() %>%
  pivot_longer(cols = -c(rowname, mpg), names_to = "var") %>%
  ggplot(aes(x = value, y = mpg)) +
  geom_point() +
  geom_smooth(method = "gam") +
  facet_wrap(~ var, scales = "free_x")
p2 <- auto %>%
  transmute(mpg, cylinders = factor(cylinders), origin) %>%
  rownames_to_column() %>%
  pivot_longer(cols = -c(rowname, mpg), names_to = "var") %>%
  ggplot(aes(y = factor(value), x = mpg)) +
  geom_boxplot() +
  facet_wrap(~ var, scales = "free_y")
p1 / p2 +
  plot_layout(heights = c(2, 1))
```

Fit a GAM with all of the variables:

```{r}
auto_gam <- gam(
  mpg ~ s(acceleration, k = 5) + s(displacement, k = 5) + s(horsepower, k = 5) +
    s(year, k = 5) + s(cylinders, k = 3) + origin,
  data = auto
)
summary(auto_gam)
```

This suggests that there are non-linear relationships in `acceleration`, `displacement`, `horsepower`, and `year`.
Visualize the smooth functions:

```{r}
plot(auto_gam, pages = 1, shade = TRUE)
```


#### 9. Predicting nitrogen oxide concentration in `boston` {-}

```{r}
boston <- ISLR2::Boston
glimpse(boston)
```

Visualize the `nox` and `dis` variables:

```{r fig.height=5, message=FALSE}
p1 <- boston %>%
  select(nox, dis) %>%
  pivot_longer(cols = c(nox, dis)) %>%
  ggplot(aes(x = value)) +
  geom_boxplot(aes(y = 0, fill = name), outlier.shape = NA) +
  geom_jitter(aes(y = 1, color = name), alpha = 0.5) +
  facet_wrap(~ name, scales = "free_x") +
  dunnr::remove_axis("y") +
  theme(legend.position = "none")
p2 <- boston %>%
  ggplot(aes(x = dis, y = nox)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "gam")
p1 / p2
```


Looks like higher values of `dis` (the distance from Boston employment centers) are associated with lower `nox` (nitrogen oxide concentration).

(a) Cubic polynomial regression.

```{r}
lm_spec <- linear_reg()
boston_poly_3_recipe <- recipe(nox ~ dis, data = boston) %>%
  step_poly(dis, degree = 3)
boston_poly_3_fit <- workflow() %>%
  add_recipe(boston_poly_3_recipe) %>%
  add_model(lm_spec) %>%
  fit(data = boston)
summary(boston_poly_3_fit %>% extract_fit_engine())
```

Visualize the cubic fit:

```{r}
dis_grid <- tibble(dis = seq(min(boston$dis), max(boston$dis),
                             length.out = 100))
boston_poly_3_pred <- bind_cols(
  dis_grid,
  # I'll use `predict` instead of `augment` because I don't think I can get
  #  SEs/CIs easily with the latter
  predict(boston_poly_3_fit, new_data = dis_grid),
  predict(boston_poly_3_fit, new_data = dis_grid, type = "conf_int")
)

p_base <- ggplot(boston, aes(x = dis)) +
  geom_point(aes(y = nox), color = "grey50", alpha = 0.5) +
  scale_x_continuous(breaks = seq(2, 12, 2))

p_base +
  geom_line(data = boston_poly_3_pred, aes(y = .pred),
            color = td_colors$nice$spanish_blue, size = 2) +
  geom_ribbon(data = boston_poly_3_pred,
              aes(ymin = .pred_lower, ymax = .pred_upper),
              fill = td_colors$nice$spanish_blue, alpha = 0.5)
```

(b) Polynomial fits from 1 to 10 degrees.

Since we're not `tune()`ing the `degree` parameter on resamples here (just fitting a model for each `degree` to the full data set) I can't use the typical `tune_grid()/fit_resamples()` approach.

```{r}
boston_recipe <- recipe(nox ~ dis, data = boston)

boston_poly_fits <-
  tibble(
    degree = 1:10,
    poly_fit = map(
      degree,
      ~ workflow() %>%
        add_recipe(boston_recipe %>% step_poly(dis, degree = .x)) %>%
        add_model(lm_spec) %>%
        fit(data = boston)
    )
  ) %>%
  mutate(
    poly_pred = map(
      poly_fit,
      ~ bind_cols(
        dis_grid,
        predict(.x, new_data = dis_grid),
        predict(.x, new_data = dis_grid, type = "conf_int")
      )
    ),
    mod_rss = map_dbl(poly_fit, ~ glance(.x)$deviance)
  )

p_base +
  geom_line(
    data = boston_poly_fits %>%
      mutate(
        degree_label = fct_inorder(paste0(degree, " (", round(mod_rss, 3), ")"),
                                   ordered = TRUE)
      ) %>%
      unnest(poly_pred),
    aes(y = .pred, color = degree_label), size = 1
  ) +
  scale_color_viridis_d(end = 0.8) +
  labs(color = "Degree (RSS)") +
  coord_cartesian(ylim = c(0.35, 0.9))
```

(c) Choose the optimal degree by cross-validation.

Now I can `tune()`:

```{r}
boston_poly_recipe <- recipe(nox ~ dis, data = boston) %>%
  step_poly(dis, degree = tune())
boston_poly_workflow <- workflow() %>%
  add_recipe(boston_poly_recipe) %>%
  add_model(lm_spec)

degree_grid <- tibble(degree = 1:10)

set.seed(4928)
boston_resamples <- vfold_cv(boston, v = 10)

boston_poly_tune <-
  tune_grid(boston_poly_workflow, resamples = boston_resamples,
            grid = degree_grid)

autoplot(boston_poly_tune) +
  scale_x_continuous(breaks = 1:10)
```

Both the 3-degree and 4-degree fits appear best.
I would choose the 3-degree since it is simpler.
You could also make a case for the quadratic fit by the one SE rule:

```{r}
select_by_one_std_err(boston_poly_tune, metric = "rmse", degree)
```


(d) Regression splines.

With 4 degrees of freedom, a cubic spline has a single knot.
By default, it will be placed at the median of `dis`:

```{r}
attr(bs(boston$dis, df = 4), "knots")
```


```{r}
boston_spline_4_recipe <- recipe(nox ~ dis, data = boston) %>%
  step_bs(dis, deg_free = 4)
boston_spline_4_fit <- workflow() %>%
  add_model(lm_spec) %>%
  add_recipe(boston_spline_4_recipe) %>%
  fit(data = boston)

extract_fit_engine(boston_spline_4_fit) %>%
  summary()
```


```{r}
boston_spline_4_pred <- bind_cols(
  dis_grid,
  predict(boston_spline_4_fit, new_data = dis_grid),
  predict(boston_spline_4_fit, new_data = dis_grid, type = "conf_int")
)

p_base +
  geom_line(data = boston_spline_4_pred, aes(y = .pred),
            color = td_colors$nice$ruby_red, size = 2) +
  geom_ribbon(data = boston_spline_4_pred,
              aes(ymin = .pred_lower, ymax = .pred_upper),
              fill = td_colors$nice$ruby_red, alpha = 0.5)

```

(e) Regression splines for a range of degrees of freedom.

I'll range `deg_free` from 3 (no knots) to 10 (7 knots):

```{r}
boston_spline_fits <-
  tibble(
    deg_free = 3:10,
    spline_fit = map(
      deg_free,
      ~ workflow() %>%
        add_recipe(boston_recipe %>% step_bs(dis, deg_free = .x)) %>%
        add_model(lm_spec) %>%
        fit(data = boston)
    )
  ) %>%
  mutate(
    spline_pred = map(
      spline_fit,
      ~ bind_cols(
        dis_grid,
        predict(.x, new_data = dis_grid),
        predict(.x, new_data = dis_grid, type = "conf_int")
      )
    ),
    mod_rss = map_dbl(spline_fit, ~ glance(.x)$deviance)
  )

p_base +
  geom_line(
    data = boston_spline_fits %>%
      mutate(
        df_label = fct_inorder(paste0(deg_free, " (", round(mod_rss, 3), ")"),
                                   ordered = TRUE)
      ) %>%
      unnest(spline_pred),
    aes(y = .pred, color = df_label), size = 1
  ) +
  scale_color_viridis_d(end = 0.8) +
  labs(color = "df (RSS)") +
  coord_cartesian(ylim = c(0.35, 0.9))
```


(f) Choose the optimal degrees of freedom by cross-validation

```{r}
boston_spline_recipe <- recipe(nox ~ dis, data = boston) %>%
  step_bs(dis, deg_free = tune())
boston_spline_workflow <- workflow() %>%
  add_recipe(boston_spline_recipe) %>%
  add_model(lm_spec)
deg_free_grid <- tibble(deg_free = 3:15)
```


```{r boston-spline-tune, cache=TRUE}
boston_spline_tune <-
  tune_grid(boston_spline_workflow, resamples = boston_resamples,
            grid = deg_free_grid)
```

```{r}
autoplot(boston_spline_tune) +
  scale_x_continuous(breaks = 3:15)
```

The RMSE estimates are a bit noisier than I would like.
The best model within one SE:

```{r}
select_by_one_std_err(boston_spline_tune, metric = "rmse", deg_free)
```

From the plot, it might be surprising that the best model has 0 knots (`deg_free = 3`), but relative to the standard errors, there isn't a huge improvement for larger values.

```{r include=FALSE}
# Grab metrics for the below text
spline_metrics <- collect_metrics(boston_spline_tune) %>%
  filter(.metric == "rmse") %>%
  pull(mean, name = deg_free)
```


An alternative way of choosing the model is by percent loss via `tune::select_by_pct_loss()`.
The best model here has an RMSE of
`r round(min(spline_metrics), 4)`.
The `deg_free = 3` model has an RMSE of
`r round(spline_metrics[['3']], 4)`.
This corresponds to a percentage loss of 
(`r round(spline_metrics[['3']], 4)` - `r round(min(spline_metrics), 4)`) /
`r round(min(spline_metrics), 4)` * 100 = 
`r scales::percent((spline_metrics[['3']] - min(spline_metrics)) / min(spline_metrics), accuracy = 0.1)`.
We can choose an upper `limit` of acceptable percent loss in `tune::select_by_pct_loss()` -- the default is `limit = 2` which eliminates `deg_free = 3` as an option:

```{r}
select_by_pct_loss(boston_spline_tune, metric = "rmse", deg_free)
```

By percent loss, `deg_free = 9` is chosen.

#### 10. Predicting tuition with `college` {-}

```{r}
college <- ISLR2::College %>% janitor::clean_names()
glimpse(college)
```

Some quick EDA:

```{r message=FALSE, fig.height=7, fig.width=8}
p1 <- college %>%
  ggplot(aes(x = outstate)) +
  geom_density(fill = td_colors$nice$mellow_yellow, alpha = 0.5) +
  geom_rug() +
  dunnr::remove_axis("y")
p2 <- college %>%
  ggplot(aes(y = private, x = outstate)) +
  geom_boxplot(fill = td_colors$nice$mellow_yellow, alpha = 0.5)
p3 <- college %>%
  select(where(is.double)) %>%
  pivot_longer(cols = -outstate) %>%
  ggplot(aes(x = value, y = outstate)) +
  geom_point(color = "grey50", alpha = 0.3) +
  geom_smooth(method = "gam", color = td_colors$nice$mellow_yellow) +
  facet_wrap(~ name, scales = "free_x", ncol = 4)
(p1 + p2) / p3 +
  plot_layout(heights = c(1, 5))
```


(a) Split the data and select a subset of predictors.

```{r}
set.seed(10291)
college_splits <- initial_split(college, prop = 0.7, strata = outstate)
college_train <- training(college_splits)
college_test <- testing(college_splits)
college_resamples <- vfold_cv(college_train, v = 10)
```

```{r include=FALSE}
# Here is how to choose subset automatically with lasso
lasso_spec <- linear_reg(mixture = 1, penalty = tune()) %>%
  set_engine("glmnet")
college_lasso_recipe <- recipe(outstate ~ ., data = college_train) %>%
  step_novel(all_nominal()) %>%
  step_dummy(all_nominal()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors())
college_lasso_workflow <- workflow() %>%
  add_recipe(college_lasso_recipe) %>%
  add_model(lasso_spec)

penalty_grid <- grid_regular(penalty(range = c(-10, 3)), levels = 30)

college_lasso_tune <- tune_grid(
  college_lasso_workflow, resamples = college_resamples,
  grid = penalty_grid
)

college_lasso_train_fit <- finalize_workflow(
  college_lasso_workflow, select_best(college_lasso_tune, metric = "rmse")
) %>%
  fit(data = college_train)
college_lasso_train_fit %>%
  tidy() %>%
  filter(term != "(Intercept)") %>%
  arrange(abs(estimate))
# The only variable removed (coefficient shrunk to zero) was `p_undergrad`
```

```{r}
college_regsubsets <-
  leaps::regsubsets(outstate ~ ., data = college_train,
                    method = "forward", nvmax = 17) %>%
  tidy() %>%
  select(-`(Intercept)`) %>%
  rownames_to_column(var = "n_vars") %>%
  pivot_longer(cols = privateYes:grad_rate, names_to = "var") %>%
  filter(value) %>%
  select(-value) %>%
  group_by(across(-var)) %>%
  summarise(var = paste0(sort(var), collapse = ", "), .groups = "drop") %>%
  mutate(n_vars = as.integer(n_vars)) %>%
  arrange(n_vars)
college_regsubsets %>%
  gt() %>%
  fmt_number(columns = c(r.squared, adj.r.squared, BIC, mallows_cp),
             n_sigfig = 4) %>%
  data_color(columns = r.squared, colors = c("red", "green"), alpha = 0.5) %>%
  data_color(columns = adj.r.squared, colors = c("red", "green"), alpha = 0.5) %>%
  data_color(columns = BIC, colors = c("green", "red"), alpha = 0.5) %>%
  data_color(columns = mallows_cp, colors = c("green", "red"), alpha = 0.5)
```

By forward stepwise selection, the best model has 12, 7, or 11 variables by highest adjusted $R^2$, lowest BIC, and lowest $C_p$ respectively.
I'll go with the 7 variable model to keep it simpler:

```{r}
college_regsubsets %>%
  filter(BIC == min(BIC)) %>%
  select(n_vars, BIC, var)
```

(b) Fit a GAM and evaluate on the test set.

Here is the GAM spec in `tidymodels` with `mgcv` as the engine and `tune()`-able degrees of freedom:

```{r}
gam_spec <- gen_additive_mod(adjust_deg_free = tune()) %>%
  # This needs to be specificied (can be regression or classification)
  set_mode("regression") %>%
  # This doesn't, as it is the default engine
  set_engine("mgcv")
translate(gam_spec)
```

Note that the `adjust_deg_free` parameter corresponds to `gamma` in `mgcv`, which controls model smoothness.

The `workflow` for GAMs doesn't use the regular `recipe` approach -- see [this example](https://community.rstudio.com/t/tune-grid-and-gam-model-incl-gam-formula/127206).
Instead, smoothing terms are specified with the `formula` argument:

```{r}
college_gam_workflow <- workflow() %>%
  add_model(
    gam_spec,
    formula = outstate ~ s(expend) + s(grad_rate) + s(perc_alumni) +
      s(personal) + s(ph_d) + s(room_board) + private
  ) %>%
  add_variables(predictors = everything(), outcomes = outstate)
```

An explanation from the `?add_model()` documentation:

>
Typically, the terms are extracted from the formula or recipe preprocessing methods. However, some models (like survival and bayesian models) use the formula not to preprocess, but to specify the structure of the model. In those cases, a formula specifying the model structure must be passed unchanged into the model call itself.

Now tune the model for a range of `adjust_deg_free`:

```{r}
adjust_deg_free_grid <-
  grid_regular(adjust_deg_free(range = c(0.25, 5)), levels = 30)
college_gam_tune <-
  tune_grid(college_gam_workflow, resamples = college_resamples,
            grid = adjust_deg_free_grid)
autoplot(college_gam_tune)
```

Take the best model, fit to the full training set, and print the model output:

```{r}
college_gam_final_workflow <-
  finalize_workflow(college_gam_workflow,
                    select_best(college_gam_tune, metric = "rmse"))
college_gam_fit_training <- college_gam_final_workflow %>%
  fit(data = college_train)

extract_fit_engine(college_gam_fit_training) %>%
  summary()
```

(c) Evaluate the model obtained on the test set.

```{r}
college_test_pred <- college_gam_fit_training %>%
  augment(new_data = college_test)
college_test_pred %>%
  rmse(truth = outstate, estimate = .pred)
```

As a comparison, here is the test RMSE for a purely linear model:

```{r}
linear_reg() %>%
  fit(
    outstate ~ expend + grad_rate + perc_alumni +
      personal + ph_d + room_board + private,
    data = college_train
  ) %>%
  augment(new_data = college_test) %>%
  rmse(truth = outstate, estimate = .pred)
```

Plot the actual vs predicted from the GAM fit:

```{r fig.height=3, fig.width=3}
college_test_pred %>%
  ggplot(aes(x = outstate, y = .pred)) +
  geom_point(alpha = 0.5) +
  geom_abline(lty = 2)
```


(d) For which variables, if any, is there evidence of a non-linear relationship?

Visualize the smooth functions:

```{r}
extract_fit_engine(college_gam_fit_training) %>%
  plot(pages = 1, shade = TRUE)
```

The `expend`, `grad_rate`, and `ph_d` exhibit non-linear relationships

#### 11. Backfitting with simulated data {-}

(a) Generate response $Y$ and predictors $X_1$ and $X_2$ for $n = 100$.

Generate some $\beta$s:

```{r}
set.seed(2939)
(betas <- runif(3, min = -10, max = 10))
```

Then the data-generating model will be:

$$
\begin{align}
Y &= -0.785 + 9.199 X_1 - 9.655 X_2 + \epsilon \\
X_1 &\sim N(0, 2) \\
X_2 &\sim N(0, 2) \\
\epsilon &\sim N(0, 5)
\end{align}
$$

```{r}
d <- tibble(
  x1 = rnorm(100, 0, 2),
  x2 = rnorm(100, 0, 2),
  epsilon = rnorm(100, 0, 5)
) %>%
  mutate(y = betas[1] + betas[2] * x1 + betas[3] * x2 + epsilon)
glimpse(d)
```
(b) Initialize $\hat{\beta}_1$.

```{r}
beta_hat_1 <- -3.14
```

(c) Keeping $\hat{\beta}_1$ fixed, fit the model

$$
Y - \hat{\beta}_1 X_1 = \beta_0 + \beta_2 X_2 + \epsilon
$$

```{r}
d <- d %>% mutate(y_diff = y - beta_hat_1 * x1)
(beta_hat_2 <- lm(y_diff ~ x2, data = d)$coefficients[2])
```
 
(d) Keeping $\hat{\beta}_2$ fixed, fit the model
 
$$
Y - \hat{\beta}_2 X_2 = \beta_0 + \beta_1 X_1 + \epsilon
$$

```{r}
d <- d %>% mutate(y_diff = y - beta_hat_2 * x2)
(beta_hat_1 <- lm(y_diff ~ x1, data = d)$coefficients[2])
```

(e) Repeat 1000 times and plot the coefficient estimates.

```{r}
beta_hat_0 <- numeric(0)
beta_hat_1 <- c(-3.14)
beta_hat_2 <- numeric(0)
for (i in 1:1000) {
  d <- d %>% mutate(y_diff = y - beta_hat_1[i] * x1)
  m <- lm(y_diff ~ x2, data = d)
  beta_hat_2 <- c(beta_hat_2, m$coefficients[2])
  
  d <- d %>% mutate(y_diff = y - beta_hat_2[i] * x2)
  m <- lm(y_diff ~ x1, data = d)
  beta_hat_0 <- c(beta_hat_0, m$coefficients[1])
  beta_hat_1 <- c(beta_hat_1, m$coefficients[2])
}

d_estimates <- tibble(
  beta_0 = c(NA_real_, beta_hat_0),
  beta_1 = beta_hat_1,
  beta_2 = c(NA_real_, beta_hat_2)
) %>%
  mutate(i = 0:1000) %>%
  pivot_longer(cols = -i, names_to = "coef", values_to = "estimate") %>%
  filter(!is.na(estimate))
p <- ggplot(d_estimates, aes(x = i, y = estimate, color = coef)) +
  geom_line(size = 1.5)
p
```

The convergence happens immediately.

(f) Compare to a multiple linear regression.

```{r}
d_estimates_mult <- lm(y ~ x1 + x2, data = d) %>%
  tidy() %>%
  mutate(coef = factor(term, levels = c("(Intercept)", "x1", "x2"),
                       labels = c("beta_0", "beta_1", "beta_2")))
p <- p +
  geom_hline(data = d_estimates_mult,
             aes(yintercept = estimate, color = coef), lty = 2, size = 1)
p
```

Basically the exact same estimates.

(g) How many backfitting iterations were required.

Just a single iteration was needed.

```{r}
d_estimates %>%
  filter(i < 5) %>%
  pivot_wider(names_from = coef, values_from = estimate)
```

```{r}
p + 
  coord_cartesian(xlim = c(0, 5)) +
  facet_wrap(~ coef, scales = "free_y", ncol = 1)
```

#### 12. Backfitting with simulated data (continued) {-}

I'll use `matrix` to initialize such a large data set:

```{r}
set.seed(1009)
betas <- runif(100, min = -5, max = 5)
# Intercept
beta_0 <- runif(1, min = -5, max = 5)

# Increase number of observations to account for the increase in predictors
n <- 1000

# A matrix of 100 predictors times 100 observations
x <- matrix(rnorm(n * 100, mean = 0, sd = 1), ncol = 100)

# Matrix multiplication to get the response
y <- beta_0 + x %*% betas +
  # Using a smaller random noise term
  rnorm(n, mean = 0, sd = 1)
```

I'll use a vector to keep track of the $\hat{\beta}_j$s, and a data frame to track estimates over time:

```{r}
# Initial coefficient guesses
beta_hat <- runif(100, min = -10, max = 10)
# A tibble for keeping track of beta hats
beta_hat_estimates <- tibble(
  iter = 0,
  coef = paste0("beta_", 0:100),
  # Including intercept estimate here (beta_0)
  estimate = c(NA_real_, beta_hat)
)
```

I'll try 100 iterations of backfitting:

```{r}
for (i in 1:100) {
  for (j in 1:100) {
    y_resid <- y - x[, -j] %*% beta_hat[-j]
    m <- lm(y_resid ~ x[, j])
    beta_hat[j] <- m$coefficients[2]
  }
  
  beta_hat_estimates <- beta_hat_estimates %>%
    bind_rows(
      tibble(
        iter = i,
        coef = paste0("beta_", 0:100),
        estimate = c(m$coefficients[1], beta_hat)
      )
    )
}
```

And here is how each coefficient estimate evolves:

```{r}
beta_hat_estimates %>%
  filter(!is.na(estimate)) %>%
  ggplot(aes(x = iter, y = estimate, group = coef)) +
  geom_line(alpha = 0.5)
```

Fit the full multiple regression model.

```{r}
m_mult <- lm(y ~ x)
tidy(m_mult)
```

Plot the absolute difference between the multiple regression coefficients and the backfitting coefficient estimates:

```{r}
tidy(m_mult) %>%
  transmute(coef = paste0("beta_", 0:100), estimate_mult = estimate) %>%
  left_join(
    beta_hat_estimates %>%
      filter(!is.na(estimate)),
    by = "coef"
  ) %>%
  mutate(abs_diff = abs(estimate_mult - estimate)) %>%
  ggplot(aes(x = iter, y = abs_diff, group = coef)) +
  geom_line() +
  coord_cartesian(xlim = c(0, 10)) +
  scale_x_continuous(breaks = 1:10)
```

Looks like it only takes a few iterations to obtain a good approximation.

## Reproducibility {-}

```{r}
Sys.time()
```

```{r}
if ("git2r" %in% installed.packages()) {
  if (git2r::in_repository()) {
    git2r::repository()
  }
}
```

```{r}
sessioninfo::session_info()
```
