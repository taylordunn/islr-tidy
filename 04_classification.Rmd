```{r echo=FALSE}
knitr::opts_chunk$set(fig.retina = 2.5, fig.align = "center",
                      fig.width = 6, fig.height = 3)
```

# Classification

>But in many situations, the response
variable is instead qualitative. For example, eye color is qualitative. Often qualitative variables are referred to as categorical; we will use these
terms interchangeably. In this chapter, we study approaches for predicting
qualitative responses, a process that is known as classification.

The methods covered in this chapter include logistic regression (and Poisson regression), linear discriminant analysis, quadratic discriminant analysis, naive Bayes, and $K$-nearest neighbors.

## An Overview of Classification

>In this chapter, we will illustrate the concept of classification using the
simulated `Default` data set. We are interested in predicting whether an
individual will default on his or her credit card payment, on the basis of
annual income and monthly credit card balance.

Load the go-to packages and the `default` data set:

```{r message=FALSE}
library(tidyverse)
library(broom)
library(gt)
library(patchwork) # for composing plots
# Load my R package and set the ggplot theme
library(dunnr)
extrafont::loadfonts(device = "win", quiet = TRUE)
theme_set(theme_td())
set_geom_fonts()
set_palette()

default <- ISLR2::Default
glimpse(default)
```

Randomly choose a subset of the `r nrow(default)` observations and re-create Figure 4.1:

```{r figure4.1, message=FALSE}
d <- default %>%
  add_count(default, name = "n_group") %>%
  slice_sample(
    n = 1000,
    # Inversely weight by group size to get more even distribution
    weight_by = n() - n_group
  )
p1 <- d %>%
  ggplot(aes(x = balance, y = income)) +
  geom_point(aes(color = default, shape = default),
             alpha = 0.5, show.legend = FALSE)
p2 <- d %>%
  ggplot(aes(x = default, y = balance)) +
  geom_boxplot(aes(fill = default), show.legend = FALSE)
p3 <- d %>%
  ggplot(aes(x = default, y = income)) +
  geom_boxplot(aes(fill = default), show.legend = FALSE)
p1 | (p2 | p3)
```

## Why Not Linear Regression?

Linear regression cannot predict un-ordered qualitative responses with more than two levels.

>Unfortunately, in general there is no natural way to
convert a qualitative response variable with more than two levels into a
quantitative response that is ready for linear regression.

It is possible to use linear regression to predict a binary (two level) response.
For example, if we code stroke and drug overdose as dummy variables:

$$
Y =
\begin{cases}
0 & \text{if stroke;} \\
1 & \text{if drug overdose}.
\end{cases}
$$

Then we predict stroke if $\hat{Y} <= 0.5$ and overdose if $\hat{Y} > 0.5$.
It turns out that these probability estimates are not unreasonble, but there can be issues:

>However, if we use linear regression, some of our estimates might be outside the [0, 1] interval (see Figure 4.2), making them
hard to interpret as probabilities! Nevertheless, the predictions provide an ordering and can be interpreted as crude probability estimates.

## Logistic Regression

In logistic regression, we model the probability of $Y$ belonging to a class, rather than the response $Y$ itself.
The probability of `default` given `balance` can be written:

$$
\text{Pr}(\text{default = Yes}|\text{balance}) = p(\text{balance}).
$$

One might predict a default for an individual with $p(\text{balance}) > 0.5$.
Or they may alter the threshold to be conservative, e.g. $p(\text{balance}) > 0.1$

### The Logistic Model

As previously discussed, we could model the probability as linear:

$$
p(X) = \beta_0 + \beta_1 X
$$

but this could give probabilities outside of the range 0-1.
We must instead model $p(X)$ using a function that gives outputs 0-1.
Many functions meet this description, but logistic regression uses the logistic function:

$$
p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1 X}}.
$$

Fit the linear and logistic probability models and re-create Figure 4.2:

```{r figure4.2}
lm_default_balance <-
  lm(
    default ~ balance,
     # Turn the factor levels into 0 and 1
    data = default %>% mutate(default = as.numeric(default) - 1)
  )
glm_default_balance <-
  glm(default ~ balance, data = default,
      family = binomial(link = "logit"))
# Plot the data
p <- default %>%
  ggplot(aes(x = balance)) +
  geom_point(aes(y = as.numeric(default) - 1), color = td_colors$nice$soft_orange, alpha = 0.5)
# Plot the linear model
p1 <- p +
  geom_abline(slope = coef(lm_default_balance)["balance"],
              intercept = coef(lm_default_balance)["(Intercept)"],
              size = 1.5, color = td_colors$nice$strong_blue) +
  labs(y = "Probability of default")
# Plot the logistic model
p2 <- p +
  geom_line(
    aes(y = pred_default),
    data = tibble(balance = seq(0, 2700, 1)) %>%
      mutate(
        sum_beta = coef(glm_default_balance)["(Intercept)"] +
                       balance * coef(glm_default_balance)["balance"],
        pred_default = plogis(sum_beta)
      ),
    size = 1.5, color = td_colors$nice$strong_blue
  ) +
  labs(y = NULL)
p1 | p2
```


A very clear improvement.
The mean of the fitted probabilities in both models return the overall proportion of defaulters in the data set:

```{r}
predict(lm_default_balance, newdata = default) %>%
  mean()
predict(glm_default_balance, newdata = default) %>%
  plogis() %>%
  mean()
```

The odds is found by re-arranging the logistic function:

$$
\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1 X}.
$$


This can take any value between 0 ($p(X) = 0$) and $\infty$ ($p(X) = 1$).
Basic interpretation:

* A probability of 0.2 gives 1:4 odds.
* A probability of 0.9 gives 9:1 odds.

Taking the logarithm of both sides gives us the log odds or logit which is linear in $X$:

$$
\log \left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1 X.
$$
A one unit change in $X$ increases the log odds by $\beta_1$.
Equivalently, it multiplies the odds by $e^{\beta_1}$.

### Estimating the Regression Coefficients

We fit logistic regression models with maximum likelihood, which seeks estimates for $\beta_0$ and $\beta_1$ such that the predicted probabilities $\hat{p}(x_i)$ corresponds as closely as possible to the values $y_i$.
This idea is formalized using a likelihood function:

$$
\ell (\beta_0, \beta_1) = \prod_{i: y_i = 1} p(x_i) \prod_{i': y_{i'} = 0} (1 - p(x_{i'})).
$$

We find the estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ by maximizing this likelihood function.
Note that the least squares approach to linear regression is a special case of maximum likelihood.

Re-produce Table 4.1 using the fitted model:

```{r table4.1}
# Since I will be reproducing this table often, write a function
tidy_custom <- function(mod, coef_round = 4, se_round = 4, z_round = 2) {
  tidy(mod) %>%
    transmute(
      term,
      coefficient = round(estimate, coef_round),
      std.error = round(std.error, se_round),
      `z-statistic` = round(statistic, z_round),
      `p-value` = scales::pvalue(p.value)
    )
}
tidy_custom(glm_default_balance) %>% gt() 
```

The $z$-statistic plays the same role at the $t$-statistic from linear regression.
It equals $\hat{\beta}_1 / \text{SE}(\hat{\beta}_1)$ and large (absolute) values indiciate evidence against the null hypothesis $H_0: \beta_1 = 0$.
The small $p$-value associated with `balance` in the above table is small, so we reject the null hypothesis.

### Making Predictions

With the estimates, we can compute `default` probabilities for an individual with a `balance` of \$1,000 and \$2.000.

```{r}
example_balance <- c(1000, 2000)
# For convenience, add together the linear terms to get the log-odds
sum_beta <- coef(glm_default_balance)["(Intercept)"] +
  example_balance * coef(glm_default_balance)["balance"]

exp(sum_beta) / (1 + exp(sum_beta))
```

Instead of manually writing out the full equation, here are some alternatives:

This logistic distribution function `stats::plogis` (sometimes called the inverse logit) returns probabilities from the given log-odds values:

```{r}
stats::plogis(sum_beta)
```

Calling the generic `predict` on a `glm` uses `predict.glm()`:

```{r}
# By default, predict.glm() returns log-odds
predict(glm_default_balance,
        newdata = tibble(balance = example_balance)) %>%
  # So use the inverse logit
  plogis()
```

There is an argument to `predict.glm()` called `type` that specifies the scale of the returned predictions.
By default, `type` = "link" which refers to the link function which means log-odds are returned.
Setting `type` = "response" returns probabilities:

```{r}
predict(glm_default_balance, newdata = tibble(balance = example_balance),
        type = "response")
```

Fit the model with `student` as the predictor and re-create Table 4.2:

```{r table4.2}
glm_default_student <-
  glm(default ~ student, data = default,
      # Note: don't need to specify binomial(link = "logit") because it is the
      #  default link
      family = binomial)
tidy_custom(glm_default_student) %>% gt()
```

The probabilities for student and non-students:

```{r}
predict(glm_default_student, newdata = tibble(student = c("Yes", "No")),
        type = "response")
```

### Multiple Logistic Regression

The extension to multiple predictors $p$ is straightfoward:

$$
\log \left( \frac{p(X)}{1 - p(X)} \right) = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p.
$$

Fit the model with all three predictors (`income` in thousands of dollars):

```{r table4.3}
glm_default_all <-
  glm(default ~ .,
      data = default %>% mutate(income = income / 1000),
      family = binomial)
tidy_custom(glm_default_all) %>% gt()
```

The coefficient for `student` is statistically significant and negative, whereas it was positive in the univariable model.
To understand this apparent paradox, re-create Figure 4.3:

```{r figure4.3}
balance_breaks <- seq(0, 2700, by = 270)
balance_midpoints <-
  (balance_breaks[1:(length(balance_breaks) - 1)] +
     balance_breaks[2:length(balance_breaks)]) / 2
p1 <- default %>%
  mutate(
    balance_binned = cut(balance, breaks = balance_breaks,
                         include.lowest = TRUE, labels = balance_midpoints),
    balance_binned = as.numeric(as.character(balance_binned))
  ) %>%
  group_by(student, balance_binned) %>%
  summarise(p_default = mean(default == "Yes"), .groups = "drop") %>%
  ggplot(aes(x = balance_binned, y = p_default, color = student)) +
  geom_line(size = 1.5) +
  geom_hline(
    data = default %>%
      group_by(student) %>%
      summarise(p_mean_default = mean(default == "Yes"),
                .groups = "drop"),
    aes(yintercept = p_mean_default, color = student), lty = 2, size = 1
  ) +
  scale_color_manual(values = c(td_colors$nice$strong_blue,
                                td_colors$nice$strong_red)) +
  theme(legend.position = c(0.2, 0.7))
p2 <- default %>%
  ggplot(aes(x = student, y = balance)) +
  geom_boxplot(aes(fill = student)) +
  scale_fill_manual(values = c(td_colors$nice$strong_blue,
                                td_colors$nice$strong_red)) +
  theme(legend.position = "none")
p1 | p2
```

In the left panel, we see that students have a higher overall default rate
(`r scales::percent(mean(filter(default, student == "Yes")$default == "Yes"), accuracy = 0.1)`) than non-students
(`r scales::percent(mean(filter(default, student == "No")$default == "Yes"), accuracy = 0.1)`) as shown by the dashed lines.
This is why, in the univariable regression, `student` was associated with an increase in probability of default.
But by the solid lines, we see that for most values of `balance`, students have lower default rates.
And that is what the multiple logistic regression model tells us: for fixed values of `balance` and `income`, a `student` is less likely to `default`.

This is explained by the right panel above: `student` and `balance` are correlated in that students tend to hold higher levels of debt, which is then associated with higher probability of default.

Taken altogether, we can conclude that a student is *less likely to default* than a non-student with the same credit card balance.
Without any information about their balance, however, a student is more likely to default because they are also more likely to carry a higher balance.

>This simple example illustrates the dangers and subtleties associated
with performing regressions involving only a single predictor when other
predictors may also be relevant. As in the linear regression setting, the
results obtained using one predictor may be quite different from those obtained using multiple predictors, especially when there is correlation among
the predictors. In general, the phenomenon seen in Figure 4.3 is known as
confounding.

Make predictions for a student and non-student:

```{r}
d <- tibble(
  student = c("Yes", "No"), balance = 1500,
  # Income in thousands
  income = 40000 / 1000
)
predict(glm_default_all, newdata = d, type = "response")
```

### Multinomial Logistic Regression

For predicting $K > 2$ classes, we can extend logistic regression in a method called multinomial logistic regression.
To do this, we choose a single class $K$ to serve as the baseline.
Then the probability of another class $k$ is:

$$
\text{Pr}(Y = k|X = x) = \frac{e^{\beta_{k0} + \beta_{k1} x_1 + \beta_{kp} x_p}}{1 + \sum_{l=1}^{K-1} e^{\beta_{l0} + \beta_{l1} x_1 + \beta_{lp} x_p}}
$$

for $k = 1, \dots, K - 1$.
Then for the baseline class $K$:

$$
\text{Pr}(Y = K|X = x) = \frac{1}{1 + \sum_{l=1}^{K-1} e^{\beta_{l0} + \beta_{l1} x_1 + \beta_{lp} x_p}}.
$$

The log-odds of a class $k$ is then linear in the predictors:

$$
\log \left( \frac{\text{Pr} (Y = k| X = x)}{\text{Pr} (Y = K| X = x)}\right) = \beta_{k0} + \beta_{k1} x_1 + \dots + \beta_{kp} x_p.
$$

Note that in the case of $K = 2$, the numerator becomes $p(X)$ and the denominator $1 - p(X)$, which is exactly the same the two-class logistic regression formula (Equation 4.6).

The choice of class $K$ as baseline was arbitrary.
The only thing that will change by choosing a different baseline will be the coefficient estimates, but the predictions (fitted values), and model metrics will be the same.

When performing multinomial logistic regression, we will sometimes use an alternative to dummy coding called softmax coding.

>The softmax coding is equivalent to the coding just described in the sense that the fitted values, log odds between any pair of classes, and other key model outputs will remain the
same, regardless of coding. But the softmax coding is used extensively in
some areas of the machine learning literature (and will appear again in
Chapter 10), so it is worth being aware of it. In the softmax coding, rather
than selecting a baseline class, we treat all $K$ classes symmetrically, and
assume that for $k = 1,...,K$,

$$
\text{Pr}(Y = k|X = x) = \frac{e^{\beta_{k0} + \beta_{k1} x_1 + \beta_{kp} x_p}}{ \sum_{l=1}^{K} e^{\beta_{l0} + \beta_{l1} x_1 + \beta_{lp} x_p}}.
$$

>Thus, rather than estimating coefficients for $K − 1$ classes, we actually
estimate coefficients for all $K$ classes. It is not hard to see that as a result
of (4.13), the log odds ratio between the $k$th and $k′$th classes equals

$$
\frac{\log \text{Pr} (Y = k| X = x)}{\log \text{Pr} (Y = k'| X = x)} = (\beta_{k0} - \beta_{k'0}) + (\beta_{k1} - \beta_{k'1}) x_1 + \dots + (\beta_{kp} - \beta_{k'p}) x_p.
$$

## Generative Models for Classification

>Logistic regression involves directly modeling $\text{Pr} (Y = k|X = x)$ using the
logistic function, given by (4.7) for the case of two response classes. In
statistical jargon, we model the conditional distribution of the response $Y$,
given the predictor(s) $X$. We now consider an alternative and less direct
approach to estimating these probabilities. In this new approach, we model
the distribution of the predictors $X$ separately in each of the response
classes (i.e. for each value of $Y$). We then use Bayes’ theorem to flip these
around into estimates for $\text{Pr} (Y = k|X = x)$. When the distribution of $X$
within each class is assumed to be normal, it turns out that the model is
very similar in form to logistic regression.

There are several reasons to choose this method over logistic regression:

>* When there is substantial separation between the two classes, the
parameter estimates for the logistic regression model are surprisingly
unstable. The methods that we consider in this section do not suffer
from this problem.
* If the distribution of the predictors $X$ is approximately normal in
each of the classes and the sample size is small, then the approaches
in this section may be more accurate than logistic regression.
* The methods in this section can be naturally extended to the case
of more than two response classes. (In the case of more than two
response classes, we can also use multinomial logistic regression from
Section 4.3.5.)

Consider a classification problem with $K \geq 2$ unordered classes.
Let $\pi_k$ represent the prior probability that a random observation is class $k$.
Let $f_k(X) \equiv \text{Pr}(X | Y = k)$ denote the density function of $X$ for an observation in the $k$th class.
Then Bayes' theorem states that the posterior probability than observation $X = x$ belongs to the $k$th class is

$$
\text{Pr} (Y = k|X = x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l (x)} = p_k(x).
$$

Aside: Bayes' theorem in the most simplistic form is

$$
P(Y | X) = \frac{P(X | Y) P (Y)}{P(X)}.
$$

So the probability of $X$ given class $Y$/$k$ is $P(X|Y) = f_k (x)$, the independent probability of a class $Y$/$k$ is $P(Y) = \pi_k$, and the denominator is a normalizing factor which sums over all possible values $Y$/$k$ to give the independent probability $P(X) = \sum \pi_l f_l (x)$.

Estimating $\pi_k$ is easy if we have a random sample from the population -- just take the fraction of the training observations belonging to class $k$.
Estimating the density function $f_k (x)$ is much more challenging.

>We know from Chapter 2 that the Bayes classifier, which classifies an
observation $x$ to the class for which $p_k(x)$ is largest, has the lowest possible
error rate out of all classifiers. (Of course, this is only true if all of the
terms in (4.15) are correctly specified.) Therefore, if we can find a way to
estimate $f_k(x)$, then we can plug it into (4.15) in order to approximate the
Bayes classifier.

We now discuss three classifiers that use different estimates of $f_k (x)$.

### Linear Discriminant Analysis for $p = 1$

For the case of one predictor, we start by assuming that $f_k (x)$ is normal or Gaussian, which has the following density in one dimension:

$$
f_k (x) = \frac{1}{\sqrt{2 \pi} \sigma_k} \exp \left( - \frac{1}{2\sigma_k^2} (x - \mu_k)^2\right)
$$

where $\mu_k$ and $\sigma_k^2$ are the mean and variance of the $k$th class.
For now, assume all classes have the same variance $\sigma^2$.
Plugging the above into Bayes' theorem, we have:

$$
p_k (x) =
\frac{\pi_k \frac{1}{\sqrt{2 \pi} \sigma} \exp \left( - \frac{1}{2\sigma^2} (x - \mu_k)^2\right)}
{\sum_{l=1}^K \pi_l \frac{1}{\sqrt{2 \pi} \sigma} \exp \left( - \frac{1}{2\sigma^2} (x - \mu_l)^2\right)}.
$$
The Bayes classifier assigns an observation $X = x$ to the class for which the above is largest.
Taking the log and rearranging, this is equivalent to choosing the class for which:

$$
\delta_k (x) = x \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2 \sigma^2} + \log(\pi_k)
$$

is largest.

> For instance, if $K = 2$ and $\pi_1 = \pi_2$, then the Bayes classifier
assigns an observation to class 1 if $2x (\mu_1 − \mu_2) > \mu_1^2 - \mu_2^2$, and to class 2 otherwise.
The Bayes decision boundary is the point for which $\delta_1 (x) = \delta_2 (x)$; one can show that this amounts to

$$
x = \frac{\mu_1^2 - \mu_2^2}{2 (\mu_1 - \mu_2)} = \frac{\mu_1 + \mu_2}{2}.
$$

Re-create the example in Figure 4.4:

```{r figure4.4}
bayes_boundary <- 
ggplot(data = tibble(x = seq(-4, 4, 0.1)), aes(x)) +
  stat_function(fun = dnorm, geom = "line",
                args = list(mean = -1.25, sd = 1)) +
  stat_function(fun = dnorm, geom = "line",
                args = list(mean = 1.25, sd = 1))
```

